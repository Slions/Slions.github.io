<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Hexo</title>
  
  
  <link href="https://slions.github.io/atom.xml" rel="self"/>
  
  <link href="https://slions.github.io/"/>
  <updated>2021-08-15T13:05:36.981Z</updated>
  <id>https://slions.github.io/</id>
  
  <author>
    <name>Jingyu Shi</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>走近分布式一致性协议（下篇）</title>
    <link href="https://slions.github.io/2021/08/14/%E8%B5%B0%E8%BF%91%E5%88%86%E5%B8%83%E5%BC%8F%E4%B8%80%E8%87%B4%E6%80%A7%E5%8D%8F%E8%AE%AE%EF%BC%88%E4%B8%8B%E7%AF%87%EF%BC%89/"/>
    <id>https://slions.github.io/2021/08/14/%E8%B5%B0%E8%BF%91%E5%88%86%E5%B8%83%E5%BC%8F%E4%B8%80%E8%87%B4%E6%80%A7%E5%8D%8F%E8%AE%AE%EF%BC%88%E4%B8%8B%E7%AF%87%EF%BC%89/</id>
    <published>2021-08-14T07:42:00.000Z</published>
    <updated>2021-08-15T13:05:36.981Z</updated>
    
    <content type="html"><![CDATA[<a href="/2021/08/14/%E8%B5%B0%E8%BF%91%E5%88%86%E5%B8%83%E5%BC%8F%E4%B8%80%E8%87%B4%E6%80%A7%E5%8D%8F%E8%AE%AE%EF%BC%88%E4%B8%8A%E7%AF%87%EF%BC%89/" title="上篇">上篇</a>简述了几种一致性协议（二阶段提交、三阶段提交、paxos和zab）的实现原理与优缺点，这篇了解下raft与复制状态机。<h2 id="3-5-Raft协议"><a href="#3-5-Raft协议" class="headerlink" title="3.5    Raft协议"></a>3.5    Raft协议</h2><p>类似于zookeeper的zab协议（Paxos算法），Raft也是用于保证分布式环境下多节点数据的一致性，但更易于理解。</p><p>在Raft体系中，有一个强leader，由它全权负责接收客户端的请求命令，并将命令作为日志条目赋值给其他服务器，在确认安全的时候，将日志命令提交执行。当leader故障时，会选举产生一个新的leader。在强leader的帮助下，Raft将一致性问题分解为了三个子问题：</p><ol><li>leader选举：当已有的leader故障时必须选出一个新的leader。</li><li>日志复制：leader接受来自客户端的命令，记录为日志，并复制给集群中的其他服务器，并强制其他节点的日志与leader保持一致。</li><li>安全措施：通过一些措施确保系统的安全性，如确保所有状态机按照相同顺序执行相同命令的措施。</li></ol><h3 id="3-5-1-Raft基本流程"><a href="#3-5-1-Raft基本流程" class="headerlink" title="3.5.1    Raft基本流程"></a>3.5.1    Raft基本流程</h3><p>一个Raft集群拥有多个奇数台服务器，我们一般是三台，这样可以容忍一台服务器出现故障。服务器可能会处于如下三种角色：领导者（leader）、候选人（candidate）、跟随者（follower），正常运行的情况下，会有一个leader，其他全为follower，<strong>follower只会响应leader和candidate的请求，而客户端的请求则全部由leader处理</strong>，即使有客户端请求了一个follower也会将请求重定向到leader。candidate代表候选人，出现在选举leader阶段，选举成功后candidate将会成为新的leader。可能出现的状态转换关系如下图：</p><p><img src="https://gitee.com/Slions/picbed/raw/master/img/raft7.png"></p><p>从图中可以看出，集群刚启动时，所有节点都是follower，之后在time out信号的驱使下，follower会转变成candidate去拉取选票，获得大多数选票后就会成为leader，这时候如果其他候选人发现了新的leader已经诞生，就会自动转变为follower；而如果另一个time out信号发出时，还没有选举出leader，将会重新开始一次新的选举。可见，time out信号是促使角色转换得关键因素，类似于操作系统中得中断信号。</p><p><strong>term</strong></p><p>在Raft协议中，将时间分成了一些任意长度的时间片，称为term，term使用连续递增的编号的进行识别。</p><p>每一个term都从新的选举开始，candidate们会努力争取称为leader。一旦获胜，它就会在剩余的term时间内保持leader状态。</p><p>term也起到了系统中逻辑时钟的作用，每一个server都存储了当前term编号，在server之间进行交流的时候就会带有该编号，如果一个server的编号小于另一个的，那么它会将自己的编号更新为较大的那一个；如果leader或者candidate发现自己的编号不是最新的了，就会自动转变为follower；如果接收到的请求的term编号小于自己的当前term将会拒绝执行。</p><p><strong>rpc</strong></p><p>server之间的交流是通过RPC进行的。只需要实现两种RPC就能构建一个基本的Raft集群：</p><ul><li>RequestVote RPC：它由选举过程中的candidate发起，用于拉取选票</li><li>AppendEntries RPC：它由leader发起，用于复制日志或者发送心跳信号。</li></ul><p>每个Raft节点将会根据自己节点的状态数据来对这两种RPC请求进行处理，我们先看一下每个Raft节点保存那些状态数据:</p><p><img src="https://gitee.com/Slions/picbed/raw/master/img/raft01.png"></p><p><img src="https://gitee.com/Slions/picbed/raw/master/img/raft02.png"></p><p><img src="https://gitee.com/Slions/picbed/raw/master/img/raft03.png"></p><h3 id="3-5-2-Leader选举"><a href="#3-5-2-Leader选举" class="headerlink" title="3.5.2    Leader选举"></a>3.5.2    Leader选举</h3><p>Raft通过<strong>心跳机制</strong>发起leader选举。节点都是从follower状态开始的，如果收到了来自leader或candidate的RPC，那它就保持follower状态，避免争抢成为candidate。Leader会发送空的AppendEntries RPC作为心跳信号来确立自己的地位，（心跳间隔时间）如果follower一段时间(选举超时时间)没有收到心跳，它就会认为leader已经挂了，发起新的一轮选举。</p><p>选举发起后，一个follower会增加自己的当前term编号并转变为candidate。它会首先投自己一票，然后向其他所有节点并行发起RequestVote RPC，之后candidate状态将可能发生如下三种变化:</p><ul><li>赢得选举称为leader: 如果它在一个term内收到了大多数的选票，将会在接下的剩余term时间内称为leader，然后就可以通过发送心跳确立自己的地位。(每一个server在一个term内只能投一张选票，并且按照先到先得的原则投出)</li><li>其他server称为leader：在等待投票时，可能会收到其他server发出AppendEntries RPC心跳信号，说明其他leader已经产生了。这时通过比较自己的term编号和RPC过来的term编号，如果比对方大，说明leader的term过期了，就会拒绝该RPC，并继续保持候选人身份; 如果对方编号不比自己小,则承认对方的地位,转为follower。</li><li>选票被瓜分,选举失败：如果没有candidate获取大多数选票, 则没有leader产生, candidate们等待超时后发起另一轮选举。为了防止下一次选票还被瓜分,必须采取一些额外的措施, <strong>raft采用随机election timeout的机制防止选票被持续瓜分</strong>。通过将timeout随机设为一段区间上的某个值, 因此很大概率会有某个candidate率先超时然后赢得大部分选票.（随机重试机制）</li></ul><p><img src="https://gitee.com/Slions/picbed/raw/master/img/raft05.png"></p><h3 id="3-5-3-日志复制"><a href="#3-5-3-日志复制" class="headerlink" title="3.5.3    日志复制"></a>3.5.3    日志复制</h3><p>一旦leader被选举成功，就可以对客户端提供服务了。客户端提交每一条命令都会被按<strong>顺序</strong>记录到leader的日志中，每一条命令都包含term编号和顺序索引，然后向其他节点并行发送<strong>AppendEntries RPC</strong>用以复制命令(如果命令丢失会<strong>不断重发</strong>)，当复制成功也就是大多数节点成功复制后，leader就会提交命令，即执行该命令并且将执行结果返回客户端，raft保证已经提交的命令最终也会被其他节点成功执行。leader会保存有当前已经提交的<strong>最高日志编号</strong>。顺序性确保了相同日志索引处的命令是相同的，而且之前的命令也是相同的。当发送AppendEntries RPC时，会<strong>包含leader上一条刚处理过的命令</strong>，接收节点如果发现上一条命令不匹配，就会拒绝执行。</p><h3 id="3-5-4-安全措施"><a href="#3-5-4-安全措施" class="headerlink" title="3.5.4    安全措施"></a>3.5.4    安全措施</h3><ul><li><p>日志复制原则</p><p>在Raft中，leader通过<strong>强制follower复制自己的日志</strong>来解决日志不一致的情形，那么冲突的日志将会被重写。为了让日志一致，先找到最新的一致的那条日志，然后把follower之后的日志全部删除，leader再把自己在那之后的日志一股脑推送给follower，这样就实现了一致。而寻找该条日志，可以通过AppendEntries RPC，该RPC中包含着<strong>下一次要执行的命令索引</strong>（nextIndex），如果能和follower的当前索引对上，那就执行，否则拒绝，然后leader将会逐次递减索引，直到找到相同的那条日志。</p></li><li><p>选举约束</p><p>比如某个follower在leader提交时宕机了，也就是少了几条命令，然后它又经过选举成了新的leader，这样它就会强制其他follower跟自己一样，使得其他节点上刚刚提交的命令被删除，导致客户端提交的一些命令被丢失了，raft的解决办法：Raft通过投票过程<strong>确保只有拥有全部已提交日志的candidate能成为leader</strong>。由于candidate为了拉选票需要通过RequestVote RPC联系其他节点，而之前提交的命令至少会存在于其中某一个节点上,因此只要candidate的日志至少和其他大部分节点的一样新就可以了, follower如果收到了不如自己新的candidate的RPC,就会将其丢弃。</p></li><li><p>如果命令已经被复制到了大部分节点上,但是还没来的及提交就崩溃了,这样后来的leader应该完成之前term未完成的提交. Raft通过让leader统计当前term内还未提交的命令已经被复制的数量是否<strong>半数以上</strong>, 然后进行提交。</p></li></ul><h3 id="3-5-5-日志压缩"><a href="#3-5-5-日志压缩" class="headerlink" title="3.5.5    日志压缩"></a>3.5.5    日志压缩</h3><p>随着日志大小的增长，会占用更多的内存空间，处理起来也会耗费更多的时间，对系统的可用性造成影响，因此必须想办法压缩日志大小。Snapshotting是最简单的压缩方法，系统的全部状态会写入一个snapshot保存起来，然后丢弃截止到snapshot时间点之前的所有日志。Raft中的snapshot内容如下图所示：</p><p><img src="https://gitee.com/Slions/picbed/raw/master/img/raft06.png"></p><p>每一个server都有自己的snapshot，它只保存当前状态，如上图中的当前状态为x=0,y=9，而last included index和last included term代表snapshot之前最新的命令，用于AppendEntries的状态检查。</p><p>虽然每一个server都保存有自己的snapshot，但是当follower严重落后于leader时，leader需要把自己的snapshot发送给follower加快同步，此时用到了一个新的RPC：<strong>InstallSnapshot RPC</strong>。follower收到snapshot时，需要决定如何处理自己的日志，如果收到的snapshot包含有更新的信息，它将<strong>丢弃自己已有的日志，按snapshot更新自己的状态</strong>，RPC的定义如下：</p><p><img src="https://gitee.com/Slions/picbed/raw/master/img/raft07.png"></p><p><img src="https://gitee.com/Slions/picbed/raw/master/img/raft08.png"></p><h1 id="4-复制状态机模型"><a href="#4-复制状态机模型" class="headerlink" title="4.    复制状态机模型"></a>4.    复制状态机模型</h1><p>当同一份数据存在多个副本的时候，怎么管理他们就成了重点。</p><p>复制状态机（Repilcated State Machine，RSM）的基本思想是一个分布式的复制状态机系统由多个复制单元组成，每个复制单元均是一个状态机，它的状态保存在一组状态变量中，状态机的状态能够并且只能通过外部命令来改变。（比paxos提出的时间都早，可以算是一致性协议的方法论了）</p><p>“一组状态变量”通常是基于操作日志来实现的，每一个复制单元存储一个包含一系列指令的日志，并且严格按照顺序逐条执行日志上的指令。</p><p><img src="https://gitee.com/Slions/picbed/raw/master/img/rsm01.png"></p><p>上图是一个复制状态机的实现，每个RSM都有一个replicated log，存储的是来自客户端的commands。每个RSM中replicate log中commads的顺序都是相同的，状态机按顺序处理replicate log中的command,并将处理的结果返回给客户端。由于状态机具有确定性，因此每个状态机的输出和状态都是相同的。</p><p>一致性模块（Consensus Module）用于保证每个server上Log的一致性！</p><blockquote><p>如果不做任何保障，直接将commad暴力写入，一旦服务器宕机或者出现什么其他故障，就会导致这个Log丢失，并且无法恢复。而出现故障的可能性是很高的，这就导致系统不可用。</p></blockquote><p>复制状态机它有一个很重要的性质——<strong>确定性</strong></p><blockquote><p>如果两个相同的、确定性的状态从同一状态开始，并且以相同的顺序获得相同的输入，那么这两个状态机将会生成相同的输出，并且结束在相同的状态</p></blockquote><p>GFS、HDFS、zookeeper和etcd等分布式系统都是基于复制状态机模型实现的。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;a href=&quot;/2021/08/14/%E8%B5%B0%E8%BF%91%E5%88%86%E5%B8%83%E5%BC%8F%E4%B8%80%E8%87%B4%E6%80%A7%E5%8D%8F%E8%AE%AE%EF%BC%88%E4%B8%8A%E7%AF%87%E</summary>
      
    
    
    
    <category term="分布式一致性协议" scheme="https://slions.github.io/categories/%E5%88%86%E5%B8%83%E5%BC%8F%E4%B8%80%E8%87%B4%E6%80%A7%E5%8D%8F%E8%AE%AE/"/>
    
    
    <category term="Distributed protocol" scheme="https://slions.github.io/tags/Distributed-protocol/"/>
    
  </entry>
  
  <entry>
    <title>走近分布式一致性协议（上篇）</title>
    <link href="https://slions.github.io/2021/08/14/%E8%B5%B0%E8%BF%91%E5%88%86%E5%B8%83%E5%BC%8F%E4%B8%80%E8%87%B4%E6%80%A7%E5%8D%8F%E8%AE%AE%EF%BC%88%E4%B8%8A%E7%AF%87%EF%BC%89/"/>
    <id>https://slions.github.io/2021/08/14/%E8%B5%B0%E8%BF%91%E5%88%86%E5%B8%83%E5%BC%8F%E4%B8%80%E8%87%B4%E6%80%A7%E5%8D%8F%E8%AE%AE%EF%BC%88%E4%B8%8A%E7%AF%87%EF%BC%89/</id>
    <published>2021-08-14T04:21:50.000Z</published>
    <updated>2021-08-15T12:59:50.100Z</updated>
    
    <content type="html"><![CDATA[<p>从互联网的发展可以看出由单机高耦合高资源逐步变成了现在的集群低耦合易扩展的架构，原先的做法都存在一台机器上，保证资源充足，网络稳定的情况下是最好的方案，但随着规模的不断扩大，网络的脆弱性，还有纵向扩展的局限性导致了分布式的出现。当分布式的概念出现后最重要的一点就是如何保证数据的一致性。</p><p>我们身边有很多类似的场景，比如火车票的售票系统，比如银行的转账，网上购物等等。</p><h1 id="1-一致性的级别"><a href="#1-一致性的级别" class="headerlink" title="1.    一致性的级别"></a>1.    一致性的级别</h1><ul><li><p>强一致性</p><p>始终一致，体验性最好但对系统的性能影响比较大。</p></li><li><p>弱一致性</p><p>不会承诺在系统写入之后什么时候能正确的读到该值，但会在某个时间级别后能达到数据的一致性。细分还能分成会话一致性与用户一致性。</p></li><li><p>最终一致性</p><p>是弱一致性的一个特例，会保证在一个时间内，达到数据一致性。</p></li></ul><h1 id="2-事务和分布式事务"><a href="#2-事务和分布式事务" class="headerlink" title="2.     事务和分布式事务"></a>2.     事务和分布式事务</h1><p>我们对于事务这个词听着比较熟悉，他在狭义上讲的是数据库事务，书上的解释是一系列对系统中数据进入访问与更新的操作所组成的一个程序执行逻辑单元。</p><p>事务有四个特性ACID，<code>原子性</code>，<code>一致性</code>，<code>隔离性</code>，<code>持久性</code>。</p><p>在单机的时代我们可以很容易的实现一套满足ACID的事务处理系统，但分布式数据库中，数据散落在不同的机器上，怎么对这些数据进行分布式的事务就成为了挑战。</p><p>CAP理论和BASE理论的提出：</p><p>CAP指的是一个分布式系统不可能同时满足一致性，可用性和分区容错性，最多只能满足其中的两项。</p><blockquote><p>可用性：有限的时间内返回结果</p><p>分区一致性：在遇到任何网络分区的情况下还能满足一致性和可用性，除非网络通信全断了。</p></blockquote><p>BASE理论是基于CAP演变来的，基本可用，弱状态和最终一致性。</p><h1 id="3-一致性协议"><a href="#3-一致性协议" class="headerlink" title="3.    一致性协议"></a>3.    一致性协议</h1><h2 id="3-1-二阶段提交协议"><a href="#3-1-二阶段提交协议" class="headerlink" title="3.1    二阶段提交协议"></a>3.1    二阶段提交协议</h2><p>分布式系统中，每个节点虽然可以知晓自己的操作时成功或者失败，却无法知道其他节点的操作的成功或失败。当一个事务跨越多个节点时，为了保持事务的ACID特性，需要引入一个作为<strong>协调者</strong>的组件来统一掌控所有节点(称作<strong>参与者</strong>)的操作结果并最终指示这些节点是否要把操作结果进行真正的提交(比如将更新后的数据写入磁盘等等)。</p><p>因此，二阶段提交的算法思路可以概括为：<strong>参与者将操作成败通知协调者，再由协调者根据所有参与者的反馈情报决定各参与者是否要提交操作还是中止操作。</strong></p><p><img src="https://gitee.com/Slions/picbed/raw/master/img/%E4%BA%8C%E9%98%B6%E6%AE%B5%E6%8F%90%E4%BA%A4.png"></p><h3 id="3-1-1-过程介绍"><a href="#3-1-1-过程介绍" class="headerlink" title="3.1.1    过程介绍"></a>3.1.1    过程介绍</h3><p>所谓的两个阶段是指：第一阶段：准备阶段(投票阶段)和第二阶段：提交阶段（执行阶段）。</p><ol><li><p>准备阶段</p><p>1.由协调者发起并传递带有事务信息的请求给各个参与者，询问是否可以提交事务，并等待返回结果。</p><p>2.各参与者执行事务操作，将Undo和Redo放入事务日志中（但是不提交）</p><p>3.如果参与者执行成功就返回YES（可以提交事务），失败NO(不能提交事务)</p></li><li><p>提交阶段</p><p>此阶段分两种情况：所有参与者均返回YES，有任何一个参与者返回NO</p><p>1.所有参与者均反馈YES时，即提交事务。</p><p>2.任何一个参与者反馈NO时，即中断事务。</p></li></ol><blockquote><p>提交事务：（所有参与者均反馈YES）</p><p>(1) 协调者向所有参与者发出正式提交事务的请求（即Commit请求）。</p><p>(2) 参与者执行Commit请求，并释放整个事务期间占用的资源。</p><p>(3) 各参与者向协调者反馈Ack完成的消息。</p><p>(4) 协调者收到所有参与者反馈的Ack消息后，即完成事务提交。</p><p>中断事务：（任何一个参与者反馈NO）</p><p>(1) 协调者向所有参与者发出回滚请求（即Rollback请求）。</p><p>(2) 参与者使用阶段1中的Undo信息执行回滚操作，并释放整个事务期间占用的资源。</p><p>(3) 各参与者向协调者反馈Ack完成的消息。</p><p>(4) 协调者收到所有参与者反馈的Ack消息后，即完成事务中断。</p></blockquote><h3 id="3-1-2-优缺点"><a href="#3-1-2-优缺点" class="headerlink" title="3.1.2    优缺点"></a>3.1.2    优缺点</h3><p>优点：原理简单，实现方便</p><p>缺点：同步阻塞，单点问题，脑裂问题</p><blockquote><p>在阶段2中，如果只有部分参与者接收并执行了Commit请求，会导致节点数据不一致。</p></blockquote><h2 id="3-2-三阶段提交协议"><a href="#3-2-三阶段提交协议" class="headerlink" title="3.2    三阶段提交协议"></a>3.2    三阶段提交协议</h2><h3 id="3-2-1-过程介绍"><a href="#3-2-1-过程介绍" class="headerlink" title="3.2.1    过程介绍"></a>3.2.1    过程介绍</h3><p>三阶段提交是在二基础提交上的改进，即将事务的提交过程分为CanCommit、PreCommit、do Commit三个阶段来进行处理。</p><ol><li><p>CanCommit</p><p> 1.协调者向所有参与者发出包含事务内容的CanCommit请求，询问是否可以提交事务，并等待所有参与者答复。</p><p> 2.参与者收到CanCommit请求后，如果认为可以执行事务操作，则反馈YES并进入预备状态，否则反馈NO。</p></li><li><p>PreCommit</p><p> 此阶段分为两种情况：</p><p> 1.所有参与者均受到请求并返回YES。</p><p> 2.有任何一个参与者返回NO，或者有任何一个参与者超时，协调者无法收到反馈，则事务中断。</p></li></ol><blockquote><p>事务预提交：（所有参与者均反馈YES时）</p><p>(1) 协调者向所有参与者发出PreCommit请求，进入准备阶段。</p><p>(2) 参与者收到PreCommit请求后，执行事务操作，将Undo和Redo信息记入事务日志中（但不提交）</p><p>(3) 各参与者向协调者反馈Ack响应或No响应，并等待最终指令。</p><p>中断事务：（任何一个参与者反馈NO，或者等待超时后协调者尚无法收到所有参与者的反馈时）</p><p>(1) 协调者向所有参与者发出abort请求。</p><p>(2) 无论收到协调者发出的abort请求，或者在等待协调者请求过程中出现超时，参与者均会中断事务。</p></blockquote><ol start="3"><li><p>do Commit</p><p>此阶段也存在两种情况：</p><p>1.所有参与者均反馈Ack响应，即执行真正的事务提交。</p><p>2.任何一个参与者反馈NO，或者等待超时后协调者尚无法收到所有参与者的反馈，即中断事务。</p></li></ol><blockquote><p>提交事务：（所有参与者均反馈Ack响应时）</p><p>(1) 如果协调者处于工作状态，则向所有参与者发出do Commit请求。</p><p>(2) 参与者收到do Commit请求后，会正式执行事务提交，并释放整个事务期间占用的资源。</p><p>(3) 各参与者向协调者反馈Ack完成的消息。</p><p>(4) 协调者收到所有参与者反馈的Ack消息后，即完成事务提交。</p><p>中断事务：（任何一个参与者反馈NO，或者等待超时后协调者尚无法收到所有参与者的反馈时）</p><p>(1) 如果协调者处于工作状态，向所有参与者发出abort请求。</p><p>(2) 参与者使用阶段1中的Undo信息执行回滚操作，并释放整个事务期间占用的资源。</p><p>(3) 各参与者向协调者反馈Ack完成的消息。</p><p>(4) 协调者收到所有参与者反馈的Ack消息后，即完成事务中断。</p></blockquote><p>注意：进入阶段三后，无论协调者出现问题，或者协调者与参与者网络出现问题，都会导致参与者无法接收到协调者发出的 do Commit请求或abort请求。此时，参与者都会在等待超时之后，继续执行事务提交。</p><p><img src="https://gitee.com/Slions/picbed/raw/master/img/%E4%B8%89%E9%98%B6%E6%AE%B5%E6%8F%90%E4%BA%A4.png"></p><h3 id="3-2-2-优缺点"><a href="#3-2-2-优缺点" class="headerlink" title="3.2.2    优缺点"></a>3.2.2    优缺点</h3><p>优点：引入超时机制。同时在协调者和参与者中都引入超时机制。降低了阻塞范围，在等待超时后协调者或参与者会中断事务。避免了协调者单点问题，阶段3中协调者出现问题时，参与者会继续提交事务。</p><p>缺陷：脑裂问题依然存在，即在参与者收到PreCommit请求后等待最终指令，如果此时协调者无法与参与者正常通信，由于网络原因，协调者发送的abort响应没有及时被参与者接收到，那么参与者在等待超时之后执行了commit操作。这样就和其他接到abort命令并执行回滚的参与者之间存在数据不一致的情况。</p><h2 id="3-3-Paxos协议"><a href="#3-3-Paxos协议" class="headerlink" title="3.3    Paxos协议"></a>3.3    Paxos协议</h2><p>上面我们可以看到二阶段和三阶段都可能出现数据不一致的现象，直到提出了paxos后，就持续垄断了分布式一致性算法，Paxos这个名词几乎等同于分布式一致性。</p><p>简单的说下paxos的提出是作者假想了一个叫paxos的希腊城邦，城邦要采用民主提议和投票的方式选出一个最终的决议，但由于城邦的居民没人愿意把精力都放在这种事情上，所以他们只能不定时的参加提议，不定时的来了解提议和投票进展，不定时的表达自己的意见，paxos算法的目标就是让他们按照少数服从多数的方式来最终达成一致意见。</p><p>Paxos的最大特点就是难，不仅难以理解，更难以实现，所以现在大部分的一致性协议都是基于paxos的衍生实现，比如zab，raft。</p><p>在2PC或3PC中，主要有协调者和参与者两种角色，在Paxos中，有三种角色，提议者，接收者和学习者。但是Paxos的基本流程主要在提议者和接收者之间发生。</p><ul><li><p>提议者（proposer）：提出提案，提案包含一个提案ID和一个提议的值。</p></li><li><p>接收者(acceptor)：参与决策，就提议者提出的提案进行承诺接收，如半数以上的接收者同意提案，则提案被通过。</p></li><li><p>学习者(learner)：参与决策，当提议者和接收者达成最终一致后，学习其最终值。</p></li></ul><h3 id="3-3-1-过程介绍"><a href="#3-3-1-过程介绍" class="headerlink" title="3.3.1    过程介绍"></a>3.3.1    过程介绍</h3><p>第一阶段：因为存在多个“提议者”，如果都提意见，那么“接受者“到底应该接受谁？所以要先明确哪个提议者有权提出提议，未来”接受者“们就主要处理这个”提议者“的提议（提出提议时就尽量让意见统一，尽早形成多数派）。</p><p>第二阶段：由上阶段选出的提议者提出提议，”接受者“反馈意见。如果多数接受了一个提议，那么提议就通过了。</p><h3 id="3-3-2-实现细节"><a href="#3-3-2-实现细节" class="headerlink" title="3.3.2    实现细节"></a>3.3.2    实现细节</h3><ol><li>怎么明确谁应该是”合适“的提议者？通过编号。每个”提议者“在第一阶段先报个号，谁号大就当提议者。</li><li>每个”提议者“不会执着于让自己的提议通过，而是每个”提议者“会执着于让提议尽快的达成一致意见。所以如果”提议者“在选举时发现”接受者“之前已经接受过别的”提议者“的提议了，那就算自己赢得了本次选举，也会默默的把自己的提议改成前面”提议者“的提议。（为了尽快的意见统一）</li><li>号的大小很重要，号小的无论啥时候”接受者“都会直接拒绝。</li><li>如果你是提议者，在选举时发现”接受者1“说”他见的提议者的提议是方案1“，而”接受者2“说”他见的提议者的提议是方案2“，那么还是要通过对比”号的大小“，”接受者“在接收提案时会记录下”相关提议者号的大小和提议内容（如果有的话）“，所以只需要判断哪个提议者号大就把自己的提议改成哪个提议者的。</li></ol><h2 id="3-4-zab协议"><a href="#3-4-zab协议" class="headerlink" title="3.4    zab协议"></a>3.4    zab协议</h2><p>Zab借鉴了Paxos算法，但又不像Paxos那样，是一种通用的分布式一致性算法。它是特别为Zookeeper设计的支持崩溃恢复的原子广播协议。Zab协议的全称是 Zookeeper Atomic Broadcast （Zookeeper原子广播）。Zookeeper 是通过 Zab 协议来保证分布式事务的最终一致性。</p><p>基于该协议，zk实现了一种主备模型（即Leader和Follower模型）的系统架构来保证集群中各个副本之间数据的一致性。这里的主备系统架构模型，就是指只有一台客户端（Leader）负责处理外部的<strong>写事务请求</strong>，然后Leader客户端将<strong>数据同步</strong>到其他Follower节点。</p><p>Zookeeper 客户端会随机的链接到 zookeeper 集群中的一个节点，如果是读请求，就直接从当前节点中读取数据；如果是写请求，那么节点就会向 Leader 提交事务，Leader 接收到事务提交，会<strong>广播</strong>该事务，只要<strong>超过半数节点</strong>写入成功，该事务就会被<strong>提交</strong>。</p><p><img src="https://gitee.com/Slions/picbed/raw/master/img/%E4%B8%80%E8%87%B4%E6%80%A73.png"></p><h3 id="3-4-1-实现细节"><a href="#3-4-1-实现细节" class="headerlink" title="3.4.1    实现细节"></a>3.4.1    实现细节</h3><ol><li>使用一个单一的主进程（Leader）来接收并处理客户端的事务请求（也就是写请求），并采用了Zab的原子广播协议，将服务器数据的状态变更以<strong>事务proposal</strong>（事务提议）的形式广播到所有的副本（Follower）进程上去。</li><li>保证一个全局的变更序列被<strong>顺序引用</strong>。<br>Zab要保证同一个Leader发起的事务要按顺序被apply，同时还要保证只有先前Leader的事务被apply之后，新选举出来的Leader才能再次发起事务。</li><li>当主进程出现异常的时候，整个zk集群依旧能正常工作。</li></ol><h3 id="3-4-2-如何实现数据一致性"><a href="#3-4-2-如何实现数据一致性" class="headerlink" title="3.4.2    如何实现数据一致性"></a>3.4.2    如何实现数据一致性</h3><p>可以通过Zab 协议的两种基本的模式：<strong>崩溃恢复</strong>和<strong>消息广播</strong>来研究。</p><p>当整个集群启动过程中，或者当 Leader 服务器出现网络中弄断、崩溃退出或重启等异常时，Zab协议就会 进入崩溃恢复模式，选举产生新的Leader。</p><p>当选举产生了新的 Leader，同时集群中有过半的机器与该 Leader 服务器完成了状态同步（即数据同步）之后，Zab协议就会退出崩溃恢复模式，进入消息广播模式。</p><h4 id="3-4-2-1-消息广播"><a href="#3-4-2-1-消息广播" class="headerlink" title="3.4.2.1    消息广播"></a>3.4.2.1    消息广播</h4><p>1）客户端发起一个写操作请求。</p><p>2）Leader 服务器将客户端的请求转化为事务 Proposal 提案，同时为每个 Proposal 分配一个全局的ID，即zxid。</p><p>3）Leader 服务器为每个 Follower 服务器分配一个单独的队列，然后将需要广播的 Proposal 依次放到队列中取，并且根据 FIFO 策略进行消息发送。</p><p>4）Follower 接收到 Proposal 后，会首先将其以事务日志的方式写入本地磁盘中，写入成功后向 Leader 反馈一个 Ack 响应消息。</p><p>5）Leader 接收到超过半数以上 Follower 的 Ack 响应消息后，即认为消息发送成功，可以发送 commit 消息。</p><p>6）Leader 向所有 Follower 广播 commit 消息，同时自身也会完成事务提交。Follower 接收到 commit 消息后，会将上一条事务提交。</p><h4 id="3-4-2-2-崩溃恢复"><a href="#3-4-2-2-崩溃恢复" class="headerlink" title="3.4.2.2    崩溃恢复"></a>3.4.2.2    崩溃恢复</h4><p>Zab 协议崩溃恢复必须满足以下两个要求：<br>1）Zab 协议需要确保那些已经在 Leader 服务器上提交（Commit）的事务最终被所有的服务器提交。</p><blockquote><p>选择拥有 proposal 最大值（即 zxid 最大） 的节点作为新的 Leader</p></blockquote><p>2）Zab 协议需要确保丢弃那些只在 Leader 上被提出而没有被提交的事务。</p><blockquote><p>Zab 通过zxid 来实现这一目的。一个 zxid 是64位，高 32 是纪元（epoch）编号，每经过一次 Leader选举产生一个新的Leader，其epoch 号 +1。低 32 位是消息计数器，每接收到一条消息这个值 +1，新Leader 选举后这个值重置为 0。</p></blockquote><p>崩溃恢复主要包括两部分：<strong>Leader选举</strong>和<strong>数据恢复</strong>。</p><p><strong>Leader选举</strong></p><p>在 Zab 协议中，为了保证程序的正确运行，整个恢复过程结束后需要选举出一个新的 Leader 服务器。因此 Zab 协议需要一个高效且可靠的 Leader 选举算法，从而确保能够快速选举出新的 Leader 。</p><p>所以Zab通过<code>Fast Leader Election</code>（快速选举）来完成leader选择</p><p>成为 Leader 的条件：</p><p>1）选 epoch 最大的</p><p>2）若 epoch 相等，选 zxid 最大的</p><p>3）若 epoch 和 zxid 相等，选择 server_id 最大的（zoo.cfg中的myid）</p><blockquote><p>节点在选举开始时，都默认投票给自己，当接收其他节点的选票时，会根据上面的 Leader条件 判断并且更改自己的选票，然后重新发送选票给其他节点。当有一个节点的得票超过半数，该节点会设置自己的状态为 Leading ，其他节点会设置自己的状态为 Following。</p></blockquote><p><strong>数据恢复</strong></p><p>1）完成 Leader 选举后（新的 Leader 具有最高的zxid），在正式开始工作之前（接收事务请求，然后提出新的 Proposal），Leader 服务器会首先确认事务日志中的所有的 Proposal 是否已经被集群中过半的服务器 Commit。</p><p>2）Leader 服务器需要确保所有的 Follower 服务器能够接收到每一条事务的 Proposal ，并且能将所有已经提交的事务 Proposal 应用到内存数据中。等到 Follower 将所有尚未同步的事务 Proposal 都从 Leader 服务器上同步过啦并且应用到内存数据中以后，Leader 才会把该 Follower 加入到真正可用的 Follower 列表中。</p><h3 id="3-4-3-zab协议核心"><a href="#3-4-3-zab协议核心" class="headerlink" title="3.4.3    zab协议核心"></a>3.4.3    zab协议核心</h3><p>定义了事务请求的处理方式</p><p><img src="https://gitee.com/Slions/picbed/raw/master/img/zab4.png"></p><h3 id="3-4-4-zab协议原理"><a href="#3-4-4-zab协议原理" class="headerlink" title="3.4.4    zab协议原理"></a>3.4.4    zab协议原理</h3><p>Zab协议要求每个 Leader 都要经历三个阶段：<strong>发现，同步，广播</strong>。</p><p><img src="https://gitee.com/Slions/picbed/raw/master/img/zab5.png"></p><p>运行分析：</p><p>在zab协议的设计中，每一个进程都有可能处于以下三种状态之一。</p><ul><li>LOOKING： Leader选举阶段</li><li>FOLLOWING： Follower服务器和leader保存同步状态</li><li>LEADING： Leader服务器作为主进程领导状态</li></ul><p>代码实现中，多了一种状态：Observing 状态<br>这是 Zookeeper 引入 Observer 之后加入的，Observer 不参与选举，是只读节点，跟 Zab 协议没有关系。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;从互联网的发展可以看出由单机高耦合高资源逐步变成了现在的集群低耦合易扩展的架构，原先的做法都存在一台机器上，保证资源充足，网络稳定的情况下是最好的方案，但随着规模的不断扩大，网络的脆弱性，还有纵向扩展的局限性导致了分布式的出现。当分布式的概念出现后最重要的一点就是如何保证数</summary>
      
    
    
    
    <category term="分布式一致性协议" scheme="https://slions.github.io/categories/%E5%88%86%E5%B8%83%E5%BC%8F%E4%B8%80%E8%87%B4%E6%80%A7%E5%8D%8F%E8%AE%AE/"/>
    
    
    <category term="Distributed protocol" scheme="https://slions.github.io/tags/Distributed-protocol/"/>
    
  </entry>
  
  <entry>
    <title>heketi启动失败问题排查</title>
    <link href="https://slions.github.io/2021/08/08/heketi%E5%90%AF%E5%8A%A8%E5%A4%B1%E8%B4%A5%E9%97%AE%E9%A2%98%E6%8E%92%E6%9F%A5/"/>
    <id>https://slions.github.io/2021/08/08/heketi%E5%90%AF%E5%8A%A8%E5%A4%B1%E8%B4%A5%E9%97%AE%E9%A2%98%E6%8E%92%E6%9F%A5/</id>
    <published>2021-08-08T02:19:09.000Z</published>
    <updated>2021-08-12T11:46:20.485Z</updated>
    
    <content type="html"><![CDATA[<h1 id="问题现象"><a href="#问题现象" class="headerlink" title="问题现象"></a>问题现象</h1><p>日常巡检时发现heketi服务异常，启动失败。查看heketi log报错为invalid page type: 19:10</p><h2 id="环境描述"><a href="#环境描述" class="headerlink" title="环境描述"></a>环境描述</h2><table><thead><tr><th></th><th></th></tr></thead><tbody><tr><td>内核版本</td><td>3.10.0-862.el7.x86_64</td></tr><tr><td>系统版本</td><td>CentOS Linux release 7.5.1804</td></tr><tr><td>kubernetes 版本</td><td>1.17.0</td></tr><tr><td>glusterfs版本</td><td>7.1</td></tr><tr><td>heketi版本</td><td>v9.0</td></tr></tbody></table><h1 id="排查过程"><a href="#排查过程" class="headerlink" title="排查过程"></a>排查过程</h1><p>查看glusterfs各服务运行状态，卷都在线且正常运行。</p><p>重启glusterfs与heketi无效。</p><p>查看heketi的服务启动文件 <code>/usr/bin/heketi-start.sh</code></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">!/bin/bash</span></span><br><span class="line"><span class="meta">#</span><span class="bash"></span></span><br><span class="line"><span class="bash"><span class="comment"># HEKETI_TOPOLOGY_FILE can be passed as an environment variable with the</span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> filename of the initial topology.json. In <span class="keyword">case</span> the heketi.db does not exist</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> yet, this file will be used to populate the database.</span></span><br><span class="line"></span><br><span class="line">: &quot;$&#123;HEKETI_PATH:=/var/lib/heketi&#125;&quot;</span><br><span class="line">: &quot;$&#123;BACKUPDB_PATH:=/backupdb&#125;&quot;</span><br><span class="line">LOG=&quot;$&#123;HEKETI_PATH&#125;/container.log&quot;</span><br><span class="line"></span><br><span class="line">info() &#123;</span><br><span class="line">    echo &quot;$*&quot; | tee -a &quot;$LOG&quot;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">error() &#123;</span><br><span class="line">    echo &quot;error: $*&quot; | tee -a &quot;$LOG&quot; &gt;&amp;2</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">fail() &#123;</span><br><span class="line">    error &quot;$@&quot;</span><br><span class="line">    exit 1</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">info &quot;Setting up heketi database&quot;</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> Ensure the data dir exists</span></span><br><span class="line">mkdir -p &quot;$&#123;HEKETI_PATH&#125;&quot; 2&gt;/dev/null</span><br><span class="line">if [[ $? -ne 0 &amp;&amp; ! -d &quot;$&#123;HEKETI_PATH&#125;&quot; ]]; then</span><br><span class="line">    fail &quot;Failed to create $&#123;HEKETI_PATH&#125;&quot;</span><br><span class="line">fi</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> Test that our volume is writable.</span></span><br><span class="line">touch &quot;$&#123;HEKETI_PATH&#125;/test&quot; &amp;&amp; rm &quot;$&#123;HEKETI_PATH&#125;/test&quot;</span><br><span class="line">if [ $? -ne 0 ]; then</span><br><span class="line">    fail &quot;$&#123;HEKETI_PATH&#125; is read-only&quot;</span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line">if [[ ! -f &quot;$&#123;HEKETI_PATH&#125;/heketi.db&quot; ]]; then</span><br><span class="line">    info &quot;No database file found&quot;</span><br><span class="line">    out=$(mount | grep &quot;$&#123;HEKETI_PATH&#125;&quot; | grep heketidbstorage)</span><br><span class="line">    if [[ $? -eq 0 ]]; then</span><br><span class="line">        info &quot;Database volume found: $&#123;out&#125;&quot;</span><br><span class="line">        info &quot;Database file is expected, waiting...&quot;</span><br><span class="line">        check=0</span><br><span class="line">        while [[ ! -f &quot;$&#123;HEKETI_PATH&#125;/heketi.db&quot; ]]; do</span><br><span class="line">            sleep 5</span><br><span class="line">            if [[ $&#123;check&#125; -eq 5 ]]; then</span><br><span class="line">               fail &quot;Database file did not appear, exiting.&quot;</span><br><span class="line">            fi</span><br><span class="line">            ((check+=1))</span><br><span class="line">        done</span><br><span class="line">    fi</span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line">stat &quot;$&#123;HEKETI_PATH&#125;/heketi.db&quot; 2&gt;/dev/null | tee -a &quot;$&#123;LOG&#125;&quot;</span><br><span class="line"><span class="meta">#</span><span class="bash"> Workaround <span class="keyword">for</span> scenario <span class="built_in">where</span> a lock on the heketi.db has not been</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> released.</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> This code uses a non-blocking flock <span class="keyword">in</span> a loop rather than a blocking</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> lock with timeout due to issues with current gluster and flock</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> ( see rhbz<span class="comment">#1613260 )</span></span></span><br><span class="line">for _ in $(seq 1 60); do</span><br><span class="line">    flock --nonblock &quot;$&#123;HEKETI_PATH&#125;/heketi.db&quot; true</span><br><span class="line">    flock_status=$?</span><br><span class="line">    if [[ $flock_status -eq 0 ]]; then</span><br><span class="line">        break</span><br><span class="line">    fi</span><br><span class="line">    sleep 1</span><br><span class="line">done</span><br><span class="line">if [[ $flock_status -ne 0 ]]; then</span><br><span class="line">    fail &quot;Database file is read-only&quot;</span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line">if [[ -d &quot;$&#123;BACKUPDB_PATH&#125;&quot; ]]; then</span><br><span class="line">    if [[ -f &quot;$&#123;BACKUPDB_PATH&#125;/heketi.db.gz&quot; ]] ; then</span><br><span class="line">        gunzip -c &quot;$&#123;BACKUPDB_PATH&#125;/heketi.db.gz&quot; &gt; &quot;$&#123;BACKUPDB_PATH&#125;/heketi.db&quot;</span><br><span class="line">        if [[ $? -ne 0 ]]; then</span><br><span class="line">            fail &quot;Unable to extract backup database&quot;</span><br><span class="line">        fi</span><br><span class="line">    fi</span><br><span class="line">    if [[ -f &quot;$&#123;BACKUPDB_PATH&#125;/heketi.db&quot; ]] ; then</span><br><span class="line">        cp &quot;$&#123;BACKUPDB_PATH&#125;/heketi.db&quot; &quot;$&#123;HEKETI_PATH&#125;/heketi.db&quot;</span><br><span class="line">        if [[ $? -ne 0 ]]; then</span><br><span class="line">            fail &quot;Unable to copy backup database&quot;</span><br><span class="line">        fi</span><br><span class="line">        info &quot;Copied backup db to $&#123;HEKETI_PATH&#125;/heketi.db&quot;</span><br><span class="line">    fi</span><br><span class="line">fi</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> <span class="keyword">if</span> the heketi.db does not exist and HEKETI_TOPOLOGY_FILE is <span class="built_in">set</span>, start the</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> heketi service <span class="keyword">in</span> the background and load the topology. Once <span class="keyword">done</span>, move the</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> heketi service back to the foreground again.</span></span><br><span class="line">if [[ &quot;$(stat -c %s $&#123;HEKETI_PATH&#125;/heketi.db 2&gt;/dev/null)&quot; == 0 &amp;&amp; -n &quot;$&#123;HEKETI_TOPOLOGY_FILE&#125;&quot; ]]; then</span><br><span class="line">    # start hketi in the background</span><br><span class="line">    /usr/bin/heketi --config=/etc/heketi/heketi.json &amp;</span><br><span class="line"></span><br><span class="line">    # wait until heketi replies</span><br><span class="line">    while ! curl http://localhost:8080/hello; do</span><br><span class="line">        sleep 0.5</span><br><span class="line">    done</span><br><span class="line"></span><br><span class="line">    # load the topology</span><br><span class="line">    if [[ -n &quot;$&#123;HEKETI_ADMIN_KEY&#125;&quot; ]]; then</span><br><span class="line">        HEKETI_SECRET_ARG=&quot;--secret=&#x27;$&#123;HEKETI_ADMIN_KEY&#125;&#x27;&quot;</span><br><span class="line">    fi</span><br><span class="line">    heketi-cli --user=admin &quot;$&#123;HEKETI_SECRET_ARG&#125;&quot; topology load --json=&quot;$&#123;HEKETI_TOPOLOGY_FILE&#125;&quot;</span><br><span class="line">    if [[ $? -ne 0 ]]; then</span><br><span class="line">        # something failed, need to exit with an error</span><br><span class="line">        kill %1</span><br><span class="line">        fail &quot;failed to load topology from $&#123;HEKETI_TOPOLOGY_FILE&#125;&quot;</span><br><span class="line">    fi</span><br><span class="line"></span><br><span class="line">    # bring heketi back to the foreground</span><br><span class="line">    fg %1</span><br><span class="line">else</span><br><span class="line">    # just start in the foreground</span><br><span class="line">    exec /usr/bin/heketi --config=/etc/heketi/heketi.json</span><br><span class="line">fi</span><br></pre></td></tr></table></figure><p>从输出的日志信息可以看出，此时heketidb文件存在，但是再执行以下命令时失败</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">/usr/bin/heketi --config=/etc/heketi/heketi.json</span><br></pre></td></tr></table></figure><p>github上发现了相关issue</p><p><a href="https://github.com/heketi/heketi/issues/1636">https://github.com/heketi/heketi/issues/1636</a></p><p><a href="https://github.com/heketi/heketi/issues/1378">https://github.com/heketi/heketi/issues/1378</a></p><p>相同点是都再重启heketi 服务后发生了 <code>invalid page type</code>的错误，heketi项目的贡献者提到了此报错是由于bolt db文件损坏，没有提及任何产生原因。</p><h1 id="解决方法"><a href="#解决方法" class="headerlink" title="解决方法"></a>解决方法</h1><p>根据描述可知无法采取简单的命令进行恢复，于是</p><ol><li><p>从本地健康的集群导出完整可用的heketidb，执行</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">heketi db export --dbfile heketi.db --jsonfile heketi_new.json</span><br></pre></td></tr></table></figure><p>注意：经测试此时只能从正常的heketidb文件中导出，损坏的db文件无法导出json</p></li><li><p>此时导出的json文件中数据还是原来集群的，需要将问题集群的gfs node/brick/device等数据一一映射，操作大致为：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">① 首先将json文件中的nodeentries与deviceentries处正确映射，此处的node id为随机生成的，不建议更改，以免混乱。device id是每个gfs节点的vg name，存储设备的大小需要进入每个gfs节点执行vgs查看，单位为KB,Bricks处是通过`lvs|grep brick |awk &#x27;&#123;print $1&#125;&#x27;`获得。</span><br><span class="line">② 进入gfs节点执行 gluster volume info，这里拿到volume的user.heketi.id就是之后json文件中的volumeentries id，Bricks处多副本的brick_XXXX是json文件中的volumeentries/Bricks,json文件中volume的gid对应了k8s pv卷中的gid</span><br><span class="line">③ 进入每个gfs节点执行lvs与lsblk得到json中需要的LvmThinPool、size、TpSize</span><br></pre></td></tr></table></figure><p>注意：clusterid与nodeid是由heketi生成的，其余需要与gfs环境一一对应。而且发现需将json文件中的数组按照数字/字母大小排列。</p></li><li><p>将heketi_new.json导成dbfile,执行如下命令</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">heketi db import --dbfile heketi.db --jsonfile heketi01.json</span><br></pre></td></tr></table></figure><p>将此db文件替换损坏的db,重启heketi服务</p></li><li><p>执行 <code>heketi-cli db check</code>检测是否正常</p></li></ol><h1 id="暴力测试"><a href="#暴力测试" class="headerlink" title="暴力测试"></a>暴力测试</h1><p>关于此问题的github只是有人提出了恢复方案，指出为boltdb损坏，没有谈及触发条件，特进行了以下模拟测试。</p><table><thead><tr><th>测试方法</th><th>测试结果</th><th>是否达到期望</th></tr></thead><tbody><tr><td>heketi.db写入脏数据</td><td>服务报错，数据格式错误</td><td>×</td></tr><tr><td>heketi.db增加错误的字段</td><td>服务报错，json格式不正确或未知的字段</td><td>×</td></tr><tr><td>清空heketi.db 数据，并写入数据</td><td>服务报错，显示db文件丢失</td><td>×</td></tr><tr><td>创建卷时，重启heketi</td><td>服务正常运行</td><td>×</td></tr><tr><td>扩容卷时，重启heketi</td><td>服务正常运行</td><td>×</td></tr><tr><td>heketi.db文件设置为只读权限</td><td>服务报错，显示db只读</td><td>×</td></tr><tr><td>删除.glusterfs文件</td><td>服务报错，显示db文件丢失</td><td>×</td></tr><tr><td>无限重启heketi服务</td><td>等待10分钟后停止脚本，服务正常运行</td><td>×</td></tr><tr><td>无限创建(删除)volume</td><td>等待10分钟后停止脚本，服务正常运行</td><td>×</td></tr><tr><td>修改heketi.json配置文件</td><td>服务报错，显示heketi错误</td><td>×</td></tr><tr><td>杀掉heketi进程</td><td>服务正常运行，创建卷不成功，重启即可</td><td>×</td></tr><tr><td>将gfs服务停止，创建卷后。并重启gfs集群</td><td>服务正常运行，创建卷不成功，重启即可</td><td>×</td></tr><tr><td>模拟网络延迟(创建卷，删除卷，重启服务等操作)</td><td>服务正常运行</td><td>×</td></tr><tr><td>删除部分db数据</td><td>服务正常运行</td><td>×</td></tr><tr><td>调整glusterfs与heketi资源触发自动重启</td><td>服务正常运行</td><td>×</td></tr><tr><td>删除heketijson中的几个id信息，导入db文件</td><td>服务启动失败，服务报错Id not found</td><td>×</td></tr><tr><td>模拟zk读写数据文件时重启glusterfs与heketi</td><td>服务正常运行</td><td>×</td></tr><tr><td>模拟客户端读写数据文件时重启glusterfs与heketi</td><td>服务正常运行</td><td>×</td></tr><tr><td>heketi设置0.1c 0.1g同时批量创建删除pvc</td><td>服务正常运行</td><td>×</td></tr></tbody></table><p>未测试出同样的问题，因为报错信息是在执行heketi逻辑时发生的，结合heketi与boltdb源码找到了报错位置，其中boltdb涉及到的内存page与bucket处于黑盒状态，需进一步研究。</p><h1 id="防范策略"><a href="#防范策略" class="headerlink" title="防范策略"></a>防范策略</h1><ol><li><p>github上heketi的贡献者指出会定期收到boltdb文件损坏的报告，并谈到这种情况的出现概率较低，建议关闭heketidbstorage的性能转换设置。</p><p><a href="https://github.com/heketi/heketi/issues/1591">https://github.com/heketi/heketi/issues/1591</a></p><p>因为没能复现此问题也无法验证是否有效。</p></li><li><p>写了个脚本每台0点自动备份heketidb文件，放到了计划任务中，当遇到此类问题时能通过备份文件尽快恢复，但是会有丢失数据的可能，不清楚是否有触发器之类的东西能在用户创建或者删除卷/磁盘时自动触发脚本机制。</p><p>（恢复数据比较繁琐，smartx环境当时10个卷恢复时间为8个小时。）</p></li><li><p>通过脚本在master节点每隔1小时备份下heketi.json文件，命令为</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">heketi-cli db dump &gt;&gt; `date +%Y%m%d%H%M%S`-heketi_backup.json</span><br></pre></td></tr></table></figure><p>（恢复速度快，当出现问题时恢复的数据量较小）</p><p>gfs集群用到最多的操作就是创建/删除存储盘，基本上都在云管页面进行操作，可以在用户操作存储盘后调用此命令来完成备份。如果都在页面操作则就算遇到boltdb问题可秒级恢复，此时也可以动态的监控此备份文件，如果没有数据则说明产生了问题。</p></li></ol>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;问题现象&quot;&gt;&lt;a href=&quot;#问题现象&quot; class=&quot;headerlink&quot; title=&quot;问题现象&quot;&gt;&lt;/a&gt;问题现象&lt;/h1&gt;&lt;p&gt;日常巡检时发现heketi服务异常，启动失败。查看heketi log报错为invalid page type: 19:10</summary>
      
    
    
    
    <category term="存储类" scheme="https://slions.github.io/categories/%E5%AD%98%E5%82%A8%E7%B1%BB/"/>
    
    
    <category term="kubernetes storage" scheme="https://slions.github.io/tags/kubernetes-storage/"/>
    
    <category term="heketi" scheme="https://slions.github.io/tags/heketi/"/>
    
  </entry>
  
  <entry>
    <title>gluster排故纪实</title>
    <link href="https://slions.github.io/2021/08/08/gluster%E6%8E%92%E6%95%85%E7%BA%AA%E5%AE%9E/"/>
    <id>https://slions.github.io/2021/08/08/gluster%E6%8E%92%E6%95%85%E7%BA%AA%E5%AE%9E/</id>
    <published>2021-08-08T01:55:01.000Z</published>
    <updated>2021-08-12T11:46:37.214Z</updated>
    
    <content type="html"><![CDATA[<h1 id="setup-openshift-heketi-storage无空间"><a href="#setup-openshift-heketi-storage无空间" class="headerlink" title="setup-openshift-heketi-storage无空间"></a>setup-openshift-heketi-storage无空间</h1><p><code>glusterfs 4.1.7</code> <code>heketi v9.0.0</code> <code>部署</code></p><p><strong>问题描述</strong></p><p>运行setup-openshift-heketi-storage子命令时heketi-cli报告“无空间”错误：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> heketi-cli setup-openshift-heketi-storage</span></span><br><span class="line"></span><br><span class="line">Error: Failed to allocate new volume: No space</span><br></pre></td></tr></table></figure><p><strong>问题原因</strong></p><p>运行topology load命令的时候，服务端和heketi-cli的版本不匹配造成的。</p><p><strong>解决策略</strong></p><p>停止正在运行的heketi pod：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl scale deployment deploy-heketi --replicas=0</span><br></pre></td></tr></table></figure><p>手动删除存储块设备中的任何签名：</p><p>加载拓扑的操作是在gluster 中添加了Peer，所以需要手动detach peer</p><p>然后继续运行heketi pod：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl scale deployment deploy-heketi --replicas=1</span><br></pre></td></tr></table></figure><p>用匹配版本的heketi-cli重新加载拓扑，然后重试该步骤。</p><h1 id="gfs集群添加磁盘失败"><a href="#gfs集群添加磁盘失败" class="headerlink" title="gfs集群添加磁盘失败"></a>gfs集群添加磁盘失败</h1><p><code>glusterfs 4.1.7</code> <code>heketi v9.0.0</code> <code>部署</code></p><p><strong>问题描述</strong></p><p>给gfs集群添加设备时报错</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Adding device /dev/sdb ... Unable to add device: Unable to execute command on glusterfs-xp1nx:  Can&#x27;t initialize physical volume &quot;/dev/vdb&quot;of volume group &quot;vg_dc649bdf755667e58c5d779f9d900057&quot; without -ff</span><br></pre></td></tr></table></figure><p><strong>问题原因</strong></p><p>原因是/dev/sdb已经创建过了pv,需要删除了重新创建</p><p><strong>解决策略</strong></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">dd if=/dev/zero of=/dev/sdb bs=1k count=1</span><br><span class="line"></span><br><span class="line">blockdev --rereadpt /dev/vdb</span><br></pre></td></tr></table></figure><p>或</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pvcreate -ff --metadatasize=128M --dataalignment=256K /dev/sdb</span><br></pre></td></tr></table></figure><h1 id="创建heketidbstrorage失败"><a href="#创建heketidbstrorage失败" class="headerlink" title="创建heketidbstrorage失败"></a>创建heketidbstrorage失败</h1><p><code>glusterfs 4.1.7</code> <code>heketi v9.0.0</code> <code>部署</code></p><p><strong>问题描述</strong></p><p>给heketi创建持久卷时报错</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> heketi-cli setup-openshift-heketi-storage</span></span><br><span class="line"></span><br><span class="line">Error: Unable to execute command on glusterfs-pbzcj: volume create: heketidbstorage: failed: Staging failed on 192.168.186.10. Error: Host 192.168.186.10 is not in &#x27;Peer in Cluster&#x27; state</span><br><span class="line"></span><br><span class="line">Failed on setup openshift heketi storage</span><br></pre></td></tr></table></figure><p><strong>问题原因</strong></p><p>登录相关pod，发现gluster peer status 显示有问题</p><p><strong>解决策略</strong></p><p>修改/etc/hosts 将所有node节点添加解析记录</p><h1 id="创建heketidb卷引发机器重启"><a href="#创建heketidb卷引发机器重启" class="headerlink" title="创建heketidb卷引发机器重启"></a>创建heketidb卷引发机器重启</h1><p><code>glusterfs 7.1</code> <code>heketi v9.0.0</code> <code>部署</code></p><p><strong>问题描述</strong></p><p>在运行heketi-cli setup-openshift-heketi-storage时发生机器重启，再次登录后发现gluster没有卷信息，但是lvs看已经生成了，手动删除这些lv也会导致机器重启</p><p><strong>问题原因</strong></p><p>系统内核 &lt; 3.10-863引发的bug</p><p><strong>解决策略</strong></p><p>升级系统内核，部署gfs时需保证内核不小于3.10-863</p><h1 id="调整heketi的日志级别"><a href="#调整heketi的日志级别" class="headerlink" title="调整heketi的日志级别"></a>调整heketi的日志级别</h1><p><code>glusterfs 7.1</code> <code>heketi v9.0.0</code> </p><p><strong>问题描述</strong></p><p>heketi的配置文件是通过secrets挂到容器中的，是通过heketi.json文件生成的，查看发现配置文件中并未指明日志的级别。</p><p><strong>解决策略</strong></p><p>默认的日志级别是debug，如果需要修改，则：</p><p>在heketi.json中 “db”: “/var/lib/heketi/heketi.db”,下添加 <code>&quot;loglevel&quot;: &quot;info&quot;,</code></p><p>注：日志级别(none, critical, error, warning, info, debug)</p><p>注意后面的逗号</p><p>重新生成heketi-config-secret，重启heketi pod即可。</p><h1 id="节点Peer-Rejected"><a href="#节点Peer-Rejected" class="headerlink" title="节点Peer Rejected"></a>节点Peer Rejected</h1><p><code>glusterfs 7.1</code> <code>heketi v9.0.0</code> </p><p><strong>问题描述</strong></p><p>删除一个节点的/var/lib/glusterd/*，从其他健康的gfs节点查看集群成员状态此节点已经变成了Peer Rejected 。</p><p><strong>问题原因</strong></p><p>此节点uuid改变</p><p><strong>解决策略</strong></p><p>需要手动恢复：在健康节点查看之前异常节点的uuid。然后修改异常节点的glusterd.info，再把健康节点的uuid拷贝到异常节点的peers目录下，重启异常节点的gfs pod 。</p><h1 id="服务挂载卷消失-Server-authenication-failed"><a href="#服务挂载卷消失-Server-authenication-failed" class="headerlink" title="服务挂载卷消失 Server authenication failed"></a>服务挂载卷消失 Server authenication failed</h1><p><code>glusterfs 7.1</code> <code>heketi v9.0.0</code> <code>kubernetes 1.11.0</code> </p><p><strong>问题描述</strong></p><p>机器断电重启后发现原先挂载了glusterfs存储的nginx容器里pv没了，查看glusterfs和heketi容器都正常运行，客户端nginx容器也能正常跑起来，查看容器的日志并没有什么报错。</p><p><strong>问题原因</strong></p><p>查看event中报错信息：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[2019-07-24 08:58:01.548933] E [MSGID: 114044] [client-handshake.c:1144:client_setvolume_cbk] 0-vol_0b6185b748a309fc93431319db6e8343-client-2: SETVOLUME on remote-host failed: Authentication failed [权限不够]</span><br><span class="line"></span><br><span class="line">[2019-07-24 08:58:01.548976] E [fuse-bridge.c:5338:notify] 0-fuse: Server authenication failed. Shutting down.</span><br></pre></td></tr></table></figure><p>截图如下：</p><p><img src="/doc_picture/gfs1.png" alt="image-gfstrouble"></p><p><strong>解决策略</strong></p><p>编辑 /etc/glusterfs/glusterd.vol添加<code>option rpc-auth-allow-insecure on</code>；</p><p>重启glusterd服务然后删了nginx pod成功找回了挂载卷（数据还在）</p><h1 id="transport-endpoint-is-not-connected"><a href="#transport-endpoint-is-not-connected" class="headerlink" title="transport endpoint is not connected"></a>transport endpoint is not connected</h1><p><code>glusterfs 7.1</code> <code>heketi v9.0.0</code> </p><p><strong>问题描述</strong></p><p>客户端挂载盘报错：transport endpoint is not connected<br><img src="/doc_picture/gfs2.png" alt="image-gfstrouble"></p><p><strong>问题原因</strong></p><p>卷处于关闭状态、服务器间通信出现问题、glusterfs存储系统间数据不一致都会导致此现象产生。</p><p><strong>解决策略</strong></p><p>首先应该检查gfs服务是否正常，如果都是Run的状态则查看对应卷的状态</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl get pods -n glusterfs</span><br></pre></td></tr></table></figure><p>检查对应的卷的状态</p><p>找到对应的pv</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl get pv -n $&lt;namespace&gt; </span><br></pre></td></tr></table></figure><p>找到对应的卷</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl describe pv  $PV |grep Path: </span><br></pre></td></tr></table></figure><p>查看glusterfs volume的状态</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl exec -it -n glusterfs  $&lt;glusterfs_volume&gt;  --  gluster volume info $glusterfs_volume | grep Status </span><br></pre></td></tr></table></figure><p>在客户端查看挂载对应盘的进程是否正常；</p><p>若检查glusterfs状态均为正常，则表明其他原因导致（如节点网络异常等），重启该存储盘异常服务即可解决</p><h1 id="pv-fail状态"><a href="#pv-fail状态" class="headerlink" title="pv fail状态"></a>pv fail状态</h1><p><code>glusterfs 7.1</code> <code>heketi v9.0.0</code> <code>kubernetes 1.11.0</code></p><p><strong>问题描述</strong></p><p>测试时删除pvc时发现pvc已删除但是pv还在，是Failed的状态。<br><img src="/doc_picture/gfs3.png" alt="image-gfstrouble"></p><p>使用heketi-cli volume delete删除卷报错：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">heketi-cli volume delete 3cf6a16d635c63a7ff7023a44d37c805</span></span><br><span class="line">Error: Failed to set up volume delete: The target exists, contains other items, or is in use.</span><br></pre></td></tr></table></figure><p>先手动将pv删掉</p><p>登录其中一台gluster执行如下命令，删除卷时发现报错：</p><p>failed： some of the peers are down</p><p><img src="/doc_picture/gfs4.png" alt="image-gfstrouble"></p><p><strong>问题原因</strong></p><p>因为我测试时把gfs集群中一台机器关机了，手动把这台剔除集群，发现可以删除了。<br><img src="/doc_picture/gfs5.png" alt="image-gfstrouble"></p><p>上面这只是测试，不建议使用，因为heketi中的残留数据还在。</p><p><strong>解决策略</strong></p><p>最好的方式时恢复那台关机的节点。</p><p>如果删除卷必须在信任池中节点都在才可以，那么必须保证信任池的健康。</p><h1 id="gfs狂刷日志导致的节点down"><a href="#gfs狂刷日志导致的节点down" class="headerlink" title="gfs狂刷日志导致的节点down"></a>gfs狂刷日志导致的节点down</h1><p><code>glusterfs 4.1.7</code> <code>heketi v9.0.0</code> <code>kubernetes 1.11.0</code> </p><p><strong>问题描述</strong></p><p>其中一个gfs节点启动失败，一直在重启。</p><p><strong>问题原因</strong></p><p>登录此节点df发现根分区使用率为100%，发现gfs服务频繁刷新产生日志导致。<br><img src="/doc_picture/gfs7.png" alt="image-gfstrouble"></p><p>每次卷的重建，客户端进程glusterfs都会被重启，但进程重启的过程中，新的进程产生，旧的进程并没有被关闭，并且还在持续调用被删除的卷，所以glusterfshd.log日志不停地在输出卷无法连接的错误。在K8s 1.11.0+gfs 4.1.7上可以通过批量创建卷再批量删除卷模拟此场景。</p><p><strong>解决策略</strong></p><p>临时解决方案先把大的日志备份压缩，重启此pods。如果本地资源紧张也可以直接删除问题日志。</p><p>永久解决：</p><p>gfs7.1版本已修复此问题。</p><p>要保证存储节点有足够的根分区或给/var/挂一块足够的盘（最少30G）</p><h1 id="创建pvc-pending"><a href="#创建pvc-pending" class="headerlink" title="创建pvc pending"></a>创建pvc pending</h1><p><code>glusterfs 7.1</code> <code>heketi v9.0.0</code>  <code>kubernetes 1.17.0</code></p><p><strong>问题描述</strong></p><p>在健康的三节点gfs集群进行测试，进入其中一个gfs pods手动关闭glusterd服务，在k8s中创建pvc,一直pending。查看glusterfs日志：<br><img src="/doc_picture/gfs8.png" alt="image-gfstrouble"></p><p><strong>问题原因</strong></p><p>创建卷找不到可用的3副本</p><p><strong>解决策略</strong></p><p>把glusterd启动即可。</p><h1 id="heketi-空间计算与实际brick不匹配"><a href="#heketi-空间计算与实际brick不匹配" class="headerlink" title="heketi 空间计算与实际brick不匹配"></a>heketi 空间计算与实际brick不匹配</h1><p><code>glusterfs 7.1</code> <code>heketi v9.0.0</code>  <code>kubernetes 1.17.0</code></p><p><strong>问题描述</strong></p><p>创建pvc时发现报错没有空间<br><img src="/doc_picture/gfs9.png" alt="image-gfstrouble"></p><p>查看heketi中的gfs集群状态发现只创建了3个G的卷但是却显示使用了19个G，没有剩余空间了<br><img src="/doc_picture/gfs10.png" alt="image-gfstrouble"></p><p>查看db中是否存在待处理请求导致数据不一致的。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">heketi-cli server operations info</span><br><span class="line">heketi-cli server operations list</span><br></pre></td></tr></table></figure><p>发现没有异常状态<br><img src="/doc_picture/gfs11.png" alt="image-gfstrouble"></p><p>检查db中的状态</p><p>heketi-cli db check发现异常<br><img src="/doc_picture/gfs12.png" alt="image-gfstrouble"></p><p><strong>问题原因</strong></p><p>heketi与实际的存储设备使用空间不匹配</p><p><strong>解决策略</strong></p><p>尝试使用设备同步命令测试：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">heketi-cli device resync $&lt;device&gt;</span><br></pre></td></tr></table></figure><h1 id="heketi与gfs卷数量不一致"><a href="#heketi与gfs卷数量不一致" class="headerlink" title="heketi与gfs卷数量不一致"></a>heketi与gfs卷数量不一致</h1><p><code>glusterfs 7.1</code> <code>heketi v9.0.0</code>  <code>kubernetes 1.17.0</code></p><p><strong>问题描述</strong></p><p>通过heketi查看到的volume数量与gluster查看到的不一致</p><p>通过heketi查看到的volume卷：1个10G、1个2G（heketidb）、3个1G。<br><img src="/doc_picture/gfs13.png" alt="image-gfstrouble"><br><img src="/doc_picture/gfs14.png" alt="image-gfstrouble"></p><p>通过gluster查看volume卷，可以确认实际使用的卷为3个。1个10G、1个2G（heketidb）、2个1G。<br><img src="/doc_picture/gfs15.png" alt="image-gfstrouble"></p><p><strong>解决策略</strong></p><p>确认异常卷id信息</p><p><img src="/doc_picture/gfs16.png" alt="image-gfstrouble"></p><p>此时如果通过heketi-cli命令删除（heketi-cli volume delete VOLUME_ID）将会报设备繁忙：target is busy。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Error: umount: /var/lib/heketi/mounts/vg_NAME/brick_XXXXXX: target is busy.</span><br><span class="line">(In some cases useful info about processes that use the device is found by lsof(8) or fuser(1))</span><br></pre></td></tr></table></figure><p>确认gfs节点上的异常lv设备<br><img src="/doc_picture/gfs18.png" alt="image-gfstrouble"></p><p>查看该设备被那些进程或文件所占用。如下可以看到该brick被进程占用，所以删除时会报设备繁忙。<br><img src="/doc_picture/gfs19.png" alt="image-gfstrouble"></p><p>通过fuser -kuc NAME进行搜索并且杀死进程后，可通过heketi-cli命令正常删除异常volume卷。</p><p><img src="/doc_picture/gfs20.png" alt="image-gfstrouble"></p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;setup-openshift-heketi-storage无空间&quot;&gt;&lt;a href=&quot;#setup-openshift-heketi-storage无空间&quot; class=&quot;headerlink&quot; title=&quot;setup-openshift-heketi-sto</summary>
      
    
    
    
    <category term="存储类" scheme="https://slions.github.io/categories/%E5%AD%98%E5%82%A8%E7%B1%BB/"/>
    
    
    <category term="glusterfs" scheme="https://slions.github.io/tags/glusterfs/"/>
    
    <category term="kubernetes storage" scheme="https://slions.github.io/tags/kubernetes-storage/"/>
    
  </entry>
  
  <entry>
    <title>修改centos7主机名</title>
    <link href="https://slions.github.io/2021/08/07/%E4%BF%AE%E6%94%B9centos7%E4%B8%BB%E6%9C%BA%E5%90%8D/"/>
    <id>https://slions.github.io/2021/08/07/%E4%BF%AE%E6%94%B9centos7%E4%B8%BB%E6%9C%BA%E5%90%8D/</id>
    <published>2021-08-07T13:45:26.000Z</published>
    <updated>2021-08-12T11:47:14.611Z</updated>
    
    <content type="html"><![CDATA[<p>这篇讲解下如何修改Centos7的主机名。</p><p>Centos7中主机名分为了3类：</p><ol><li><p>static类</p><p>就是我们常说的主机名。由<code>/etc/hostname</code>文件决定。</p></li><li><p>transient类</p><p>我们常说的临时主机名。默认在系统启动的时候会根据<code>/etc/hostname</code>文件中的静态主机名进行初始化。</p></li><li><p>pretty类</p><p>它可以提供非标准的主机名。它可以包含特殊符号，例如空格。</p></li></ol><p><code>/etc/hostname</code>文件没有主机名的时候，在系统启动的时候，内核会将transient初始化为<code>localhost.localdomain</code>。</p><h1 id="主机名修改方式"><a href="#主机名修改方式" class="headerlink" title="主机名修改方式"></a>主机名修改方式</h1><h2 id="hostname"><a href="#hostname" class="headerlink" title="hostname"></a>hostname</h2><p>使用<code>hostname</code>命令修改主机名，它修改是transient主机名，即临时生效的主机名。</p><h2 id="etc-hostname"><a href="#etc-hostname" class="headerlink" title="/etc/hostname"></a>/etc/hostname</h2><p>直接修改/etc/hostname文件，它瞬时生效+永久生效。</p><h2 id="nmtui"><a href="#nmtui" class="headerlink" title="nmtui"></a>nmtui</h2><p>通过<code>nmtui</code>命令在图形化界面修改主机名。会直接修改/etc/hostname文件，他会瞬时生效+永久生效的。</p><h2 id="hostnamectl"><a href="#hostnamectl" class="headerlink" title="hostnamectl"></a>hostnamectl</h2><p>使用<code>hostnamectl</code>命令，它可以修改并查看static、transient或pretty三种主机名。当它修改了static主机名时，会直接写入*/etc/hostname*文件中，因此它也是瞬时生效+永久生效的。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> hostnamectl --<span class="built_in">help</span></span></span><br><span class="line">hostnamectl [OPTIONS...] COMMAND ...</span><br><span class="line"></span><br><span class="line">Query or change system hostname.</span><br><span class="line"></span><br><span class="line">  -h --help              Show this help</span><br><span class="line">     --version           Show package version</span><br><span class="line">     --no-ask-password   Do not prompt for password</span><br><span class="line">  -H --host=[USER@]HOST  Operate on remote host</span><br><span class="line">  -M --machine=CONTAINER Operate on local container</span><br><span class="line">     --transient         Only set transient hostname</span><br><span class="line">     --static            Only set static hostname</span><br><span class="line">     --pretty            Only set pretty hostname</span><br><span class="line"></span><br><span class="line">Commands:</span><br><span class="line">  status                 Show current hostname settings</span><br><span class="line">  set-hostname NAME      Set system hostname</span><br><span class="line">  set-icon-name NAME     Set icon name for host</span><br><span class="line">  set-chassis NAME       Set chassis type for host</span><br><span class="line">  set-deployment NAME    Set deployment environment for host</span><br><span class="line">  set-location NAME      Set location for host</span><br><span class="line"></span><br></pre></td></tr></table></figure><h3 id="查看主机名"><a href="#查看主机名" class="headerlink" title="查看主机名"></a>查看主机名</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">[root@slions_pc1 ~]#  hostnamectl</span><br><span class="line">   Static hostname: slions_pc1</span><br><span class="line">         Icon name: computer-vm</span><br><span class="line">           Chassis: vm</span><br><span class="line">        Machine ID: 72c6db2a36e54cc584626da0118ed9ca</span><br><span class="line">           Boot ID: 2cbdd44b889d40b29f932e9462e8ec15</span><br><span class="line">    Virtualization: vmware</span><br><span class="line">  Operating System: CentOS Linux 7 (Core)</span><br><span class="line">       CPE OS Name: cpe:/o:centos:centos:7</span><br><span class="line">            Kernel: Linux 3.10.0-957.el7.x86_64</span><br><span class="line">      Architecture: x86-64</span><br></pre></td></tr></table></figure><h3 id="同时修改3种主机名"><a href="#同时修改3种主机名" class="headerlink" title="同时修改3种主机名"></a>同时修改3种主机名</h3><p>当同时修改了pretty和(static | transient)中的一种时，将取pretty名的简化部分作为static主机名。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">[root@slions_pc1 ~]# hostnamectl set-hostname slions</span><br><span class="line">[root@slions_pc1 ~]# hostname</span><br><span class="line">slions</span><br><span class="line">[root@slions_pc1 ~]# cat /etc/hostname</span><br><span class="line">slions</span><br><span class="line">[root@slions_pc1 ~]# hostnamectl status</span><br><span class="line">   Static hostname: slions</span><br><span class="line">         Icon name: computer-vm</span><br><span class="line">           Chassis: vm</span><br><span class="line">        Machine ID: 72c6db2a36e54cc584626da0118ed9ca</span><br><span class="line">           Boot ID: 2cbdd44b889d40b29f932e9462e8ec15</span><br><span class="line">    Virtualization: vmware</span><br><span class="line">  Operating System: CentOS Linux 7 (Core)</span><br><span class="line">       CPE OS Name: cpe:/o:centos:centos:7</span><br><span class="line">            Kernel: Linux 3.10.0-957.el7.x86_64</span><br><span class="line">      Architecture: x86-64</span><br><span class="line">[root@slions_pc1 ~]# hostnamectl --pretty</span><br></pre></td></tr></table></figure><p>可以从结果中看到，只改变了static和transient(内核动态维护的，一定会改变)，而pretty却没设置成功。这是因为这里给出的主机名”slions”是一个符合主机名标准的名称。如果指定一个非标准的主机名，例如包含特殊符号，那么也会设置pretty。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">[root@slions_pc1 ~]# hostnamectl set-hostname &quot;slions1 slions2&quot;</span><br><span class="line">[root@slions_pc1 ~]# hostnamectl status</span><br><span class="line">   Static hostname: slions1slions2</span><br><span class="line">   Pretty hostname: slions1 slions2</span><br><span class="line">         Icon name: computer-vm</span><br><span class="line">           Chassis: vm</span><br><span class="line">        Machine ID: 72c6db2a36e54cc584626da0118ed9ca</span><br><span class="line">           Boot ID: 2cbdd44b889d40b29f932e9462e8ec15</span><br><span class="line">    Virtualization: vmware</span><br><span class="line">  Operating System: CentOS Linux 7 (Core)</span><br><span class="line">       CPE OS Name: cpe:/o:centos:centos:7</span><br><span class="line">            Kernel: Linux 3.10.0-957.el7.x86_64</span><br><span class="line">      Architecture: x86-64</span><br></pre></td></tr></table></figure><p>pretty hostname已经改变，且static hostname是它的”简化版”。</p><h3 id="修改某种类型的主机名"><a href="#修改某种类型的主机名" class="headerlink" title="修改某种类型的主机名"></a>修改某种类型的主机名</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">hostnamectl set-hostname NAME --static</span><br><span class="line">hostnamectl set-hostname NAME --transient</span><br><span class="line">hostnamectl set-hostname NAME --pretty</span><br><span class="line"><span class="meta">#</span><span class="bash"> 还可以同时修改两种</span></span><br><span class="line">hostnamectl set-hostname NAME --static --transient</span><br><span class="line">hostnamectl set-hostname NAME --static --pretty</span><br><span class="line">hostnamectl set-hostname NAME --transient --pretty</span><br></pre></td></tr></table></figure><h3 id="修改、查看远程主机的主机名"><a href="#修改、查看远程主机的主机名" class="headerlink" title="修改、查看远程主机的主机名"></a>修改、查看远程主机的主机名</h3><p>使用”-H”或”–host”选项。连接基于SSH。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hostnamectl -H [USER@]HOST set-hostname NAME</span><br><span class="line">hostnamectl -H [USER@]HOST status</span><br></pre></td></tr></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;这篇讲解下如何修改Centos7的主机名。&lt;/p&gt;
&lt;p&gt;Centos7中主机名分为了3类：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;p&gt;static类&lt;/p&gt;
&lt;p&gt;就是我们常说的主机名。由&lt;code&gt;/etc/hostname&lt;/code&gt;文件决定。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;</summary>
      
    
    
    
    <category term="linux系统" scheme="https://slions.github.io/categories/linux%E7%B3%BB%E7%BB%9F/"/>
    
    
    <category term="linux" scheme="https://slions.github.io/tags/linux/"/>
    
  </entry>
  
  <entry>
    <title>linux误删文件恢复思路</title>
    <link href="https://slions.github.io/2021/08/07/linux%E8%AF%AF%E5%88%A0%E6%96%87%E4%BB%B6%E6%81%A2%E5%A4%8D%E6%80%9D%E8%B7%AF/"/>
    <id>https://slions.github.io/2021/08/07/linux%E8%AF%AF%E5%88%A0%E6%96%87%E4%BB%B6%E6%81%A2%E5%A4%8D%E6%80%9D%E8%B7%AF/</id>
    <published>2021-08-07T10:36:34.000Z</published>
    <updated>2021-08-12T11:47:27.415Z</updated>
    
    <content type="html"><![CDATA[<p>使用linux时对于执行删除操作要慎之又慎，特别是重要的数据最好提前备份。当然，如果真的删除了一个文件时，我们也要冷静思考，想想如何通过其他手段弥补或减小损失。</p><p>在解决问题前，我们先了解下涉及到的基本概念：</p><ul><li><p>我们看到的文件实际上是一个指向inode的链接, inode链接包含了文件的所有属性, 比如权限和所有者, 数据块地址(文件存储在磁盘的这些数据块中)。当你删除(rm)一个文件, 实际删除了指向inode的链接, 并没有删除inode的内容，进程可能还在使用。 只有当inode的所有链接完全移去，然后对应的后端数据块才会写入新的数据。</p></li><li><p>proc是linux的一个伪文件系统，用户和应用程序可以通过 proc 得到系统的信息，并可以改变内核的某些参数。</p></li><li><p>系统上的进程在/proc都有一个目录和自己的名字， 里面包含了一个fd(文件描述符)子目录(进程需要打开文件的所有链接). 如果从文件系统中删除一个文件, 此处还有一个inode的引用:<code>/proc/进程号/fd/文件描述符</code></p></li><li><p>lsof（List Open Files） 命令可以查看你进程开打的文件，打开文件的进程，进程打开的端口(TCP、UDP)等等。</p></li></ul><h1 id="模拟误删场景"><a href="#模拟误删场景" class="headerlink" title="模拟误删场景"></a>模拟误删场景</h1><h2 id="准备环境"><a href="#准备环境" class="headerlink" title="准备环境"></a>准备环境</h2><p>创建一个测试文件testfile，持续监听此文件。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> <span class="keyword">while</span> <span class="literal">true</span>;<span class="keyword">do</span> <span class="built_in">echo</span> `date +%F-%T` &gt;&gt; testfile;sleep 1;<span class="keyword">done</span> &amp;</span></span><br><span class="line">[1] 12354</span><br><span class="line"><span class="meta">$</span><span class="bash"> ls</span></span><br><span class="line">testfile</span><br><span class="line"><span class="meta">$</span><span class="bash"> tail -f testfile</span></span><br><span class="line">2021-08-07-18:31:46</span><br><span class="line">2021-08-07-18:31:47</span><br><span class="line">2021-08-07-18:31:48</span><br><span class="line">2021-08-07-18:31:49</span><br><span class="line">2021-08-07-18:31:50</span><br><span class="line">2021-08-07-18:31:51</span><br><span class="line">2021-08-07-18:31:52</span><br><span class="line">2021-08-07-18:31:53</span><br><span class="line">2021-08-07-18:31:54</span><br><span class="line">2021-08-07-18:31:55</span><br><span class="line">2021-08-07-18:31:56</span><br><span class="line">2021-08-07-18:31:57</span><br><span class="line">2021-08-07-18:31:58</span><br><span class="line">2021-08-07-18:31:59</span><br><span class="line">2021-08-07-18:32:00</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>开启另一个终端，将testfile删除。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> rm  testfile</span></span><br><span class="line">rm：是否删除普通文件 &quot;testfile&quot;？y</span><br></pre></td></tr></table></figure><h2 id="恢复数据"><a href="#恢复数据" class="headerlink" title="恢复数据"></a>恢复数据</h2><p>使用lsof命令查看testfile文件句柄是否释放。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> lsof|grep testfile</span></span><br><span class="line">tail      12376                root    3r      REG              253,0       720     686557 /home/slions/testfile (deleted)</span><br></pre></td></tr></table></figure><p>第一列是进程的名称(命令名), 第二列是进程号(PID), 第四列是文件描述符，r说明是读操作。</p><p>现在我们可以知道12376进程仍有打开文件, 文件描述符是3。</p><p>从/proc里面拷贝出删除前的数据。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> cp /proc/12376/fd/3 testfile.old</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> ls</span></span><br><span class="line">testfile  testfile.old</span><br></pre></td></tr></table></figure><p>接着查看下文件内容，恢复的数据可以完美对应。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> tail testfile.old</span></span><br><span class="line">2021-08-07-18:32:12</span><br><span class="line">2021-08-07-18:32:13</span><br><span class="line">2021-08-07-18:32:14</span><br><span class="line">2021-08-07-18:32:15</span><br><span class="line">2021-08-07-18:32:16</span><br><span class="line">2021-08-07-18:32:17</span><br><span class="line">2021-08-07-18:32:18</span><br><span class="line">2021-08-07-18:32:19</span><br><span class="line">2021-08-07-18:32:20</span><br><span class="line">2021-08-07-18:32:21</span><br><span class="line"><span class="meta">$</span><span class="bash"> head -5 testfile</span></span><br><span class="line">2021-08-07-18:32:22</span><br><span class="line">2021-08-07-18:32:23</span><br><span class="line">2021-08-07-18:32:24</span><br><span class="line">2021-08-07-18:32:25</span><br><span class="line">2021-08-07-18:32:26</span><br></pre></td></tr></table></figure><p>最后要说的一点还是执行删除前三思后行，为了防止误操作带来的损失可以参考<a href="/2021/08/01/alias%E4%B8%8Erm%E7%9A%84%E6%9C%80%E4%BD%B3%E5%AE%9E%E8%B7%B5/" title="之前的文章">之前的文章</a>，提高安全性，愿各位维护的程序永不下线，机器永不宕机。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;使用linux时对于执行删除操作要慎之又慎，特别是重要的数据最好提前备份。当然，如果真的删除了一个文件时，我们也要冷静思考，想想如何通过其他手段弥补或减小损失。&lt;/p&gt;
&lt;p&gt;在解决问题前，我们先了解下涉及到的基本概念：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;我们看到的文件实际上</summary>
      
    
    
    
    <category term="linux系统" scheme="https://slions.github.io/categories/linux%E7%B3%BB%E7%BB%9F/"/>
    
    
    <category term="linux" scheme="https://slions.github.io/tags/linux/"/>
    
  </entry>
  
  <entry>
    <title>逻辑卷管理LVM</title>
    <link href="https://slions.github.io/2021/08/06/%E9%80%BB%E8%BE%91%E5%8D%B7%E7%AE%A1%E7%90%86LVM/"/>
    <id>https://slions.github.io/2021/08/06/%E9%80%BB%E8%BE%91%E5%8D%B7%E7%AE%A1%E7%90%86LVM/</id>
    <published>2021-08-06T14:18:11.000Z</published>
    <updated>2021-08-12T11:47:44.954Z</updated>
    
    <content type="html"><![CDATA[<p>总所周知不论是机械硬盘还是固态磁盘大小都是固定的，当磁盘空间满了后只能通过删除无用数据来保持磁盘的可用性，这篇介绍的LVM(Logical Volume Manager)是建立在磁盘和分区之上的一个逻辑层，用来提高磁盘分区管理的灵活性，可以随时随地的扩缩容分区大小。</p><h1 id="LVM术语"><a href="#LVM术语" class="headerlink" title="LVM术语"></a>LVM术语</h1><ul><li><p>Physical Volume(PV)</p><p>实际分区需要调整 System ID 成为 LVM 表示(8e) ，然后经过 pvcreate 命令将他转为 LVM 最低层的 PV, 然后才能使用磁盘。</p></li><li><p>Volume Group(VG)</p><p>将多个PV组合起来，使用vgcreate命令创建成卷组，这样卷组包含了多个PV就比较大了，相当于重新整合了多个分区后得到的磁盘。虽然VG是整合多个PV的，但是创建VG时会将VG所有的空间根据指定的PE大小划分为多个PE，在LVM模式下的存储都以PE为单元，类似于文件系统的Block。</p></li><li><p>Physical Extent(PE)</p><p>LVM 预设使用 4MB 的 PE 区块，每个 LV 最多允许有 65534 个 PE ，即 256GB 。PE 属于 LVM 最小存储区。</p></li><li><p>Logical Volume(LV)</p><p>VG相当于整合过的硬盘，那么LV就相当于分区，只不过该分区是通过VG来划分的。VG中有很多PE单元，可以指定将多少个PE划分给一个LV，也可以直接指定大小(如多少兆)来划分。划分为LV之后就相当于划分了分区，只需再对LV进行格式化即可变成普通的文件系统。</p></li><li><p>Logical extent(LE)</p><p>PE是物理存储单元，而LE则是逻辑存储单元，也即为lv中的逻辑存储单元，和pe的大小是一样的。从vg中划分lv，实际上是从vg中划分vg中的pe，只不过划分lv后它不再称为pe，而是成为le。</p></li></ul><p><strong>LVM之所以能够伸缩容量，其原因就在于能够将LV里空闲的PE移出，或向LV中添加空闲的PE。</strong></p><h1 id="LVM的写入机制"><a href="#LVM的写入机制" class="headerlink" title="LVM的写入机制"></a>LVM的写入机制</h1><p>LV是从VG中划分出来的，LV中的PE很可能来自于多个PV。在向LV存储数据时，有多种存储机制，其中两种是：</p><ul><li>线性模式(linear)：先写完来自于同一个PV的PE，再写来自于下一个PV的PE。</li><li>条带模式(striped)：一份数据拆分成多份，分别写入该LV对应的每个PV中，所以读写性能较好，类似于RAID 0。</li></ul><p>尽管striped读写性能较好也<strong>不建议</strong>使用该模式，因为lvm的着重点在于弹性容量扩展而非性能，要实现性能应该使用RAID来实现，而且使用striped模式时要进行容量的扩展和收缩将比较麻烦。默认的是使用线性模式。</p><h1 id="LVM实现图解"><a href="#LVM实现图解" class="headerlink" title="LVM实现图解"></a>LVM实现图解</h1><p><img src="/doc_picture/lvm.jpg" alt="lvm"></p><h1 id="LVM的实现"><a href="#LVM的实现" class="headerlink" title="LVM的实现"></a>LVM的实现</h1><p>看下我本地的环境，/dev/sdb已经分好了4个区，并且system id都设置为了8e。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> lsblk</span></span><br><span class="line">NAME            MAJ:MIN RM  SIZE RO TYPE MOUNTPOINT</span><br><span class="line">sda               8:0    0   20G  0 disk</span><br><span class="line">├─sda1            8:1    0    1G  0 part /boot</span><br><span class="line">└─sda2            8:2    0   19G  0 part</span><br><span class="line">  ├─centos-root 253:0    0   18G  0 lvm  /</span><br><span class="line">  └─centos-swap 253:1    0 1020M  0 lvm  [SWAP]</span><br><span class="line">sdb               8:16   0   20G  0 disk</span><br><span class="line">├─sdb1            8:17   0    5G  0 part</span><br><span class="line">├─sdb2            8:18   0    2G  0 part</span><br><span class="line">├─sdb3            8:19   0   10G  0 part</span><br><span class="line">└─sdb4            8:20   0    3G  0 part</span><br><span class="line">sr0              11:0    1   10G  0 rom</span><br><span class="line"><span class="meta">$</span><span class="bash"> fdisk -l /dev/sdb</span></span><br><span class="line"></span><br><span class="line">磁盘 /dev/sdb：21.5 GB, 21474836480 字节，41943040 个扇区</span><br><span class="line">Units = 扇区 of 1 * 512 = 512 bytes</span><br><span class="line">扇区大小(逻辑/物理)：512 字节 / 512 字节</span><br><span class="line">I/O 大小(最小/最佳)：512 字节 / 512 字节</span><br><span class="line">磁盘标签类型：dos</span><br><span class="line">磁盘标识符：0x76721574</span><br><span class="line"></span><br><span class="line">   设备 Boot      Start         End      Blocks   Id  System</span><br><span class="line">/dev/sdb1            2048    10487807     5242880   8e  Linux LVM</span><br><span class="line">/dev/sdb2        10487808    14682111     2097152   8e  Linux LVM</span><br><span class="line">/dev/sdb3        14682112    35653631    10485760   8e  Linux LVM</span><br><span class="line">/dev/sdb4        35653632    41943039     3144704   8e  Linux LVM</span><br><span class="line"></span><br></pre></td></tr></table></figure><h2 id="管理pv"><a href="#管理pv" class="headerlink" title="管理pv"></a>管理pv</h2><p>管理PV有几个命令：pvscan、pvdisplay、pvcreate、pvremove和pvmove。</p><p>命令很简单，基本都不需要任何选项。</p><table><thead><tr><th>功能</th><th>命令</th></tr></thead><tbody><tr><td>创建pv</td><td>pvcreate</td></tr><tr><td>扫描并列出所有的pv</td><td>pvscan</td></tr><tr><td>列出pv属性信息</td><td>pvdisplay</td></tr><tr><td>移除pv</td><td>pvremove</td></tr><tr><td>移动pv中的数据</td><td>pvmove</td></tr></tbody></table><h3 id="创建PV"><a href="#创建PV" class="headerlink" title="创建PV"></a>创建PV</h3><p>将/dev/sdb[1-3]创建为pv。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> pvcreate /dev/sdb[1-3]</span></span><br><span class="line">  Physical volume &quot;/dev/sdb1&quot; successfully created.</span><br><span class="line">  Physical volume &quot;/dev/sdb2&quot; successfully created.</span><br><span class="line">  Physical volume &quot;/dev/sdb3&quot; successfully created.</span><br><span class="line"></span><br></pre></td></tr></table></figure><h3 id="查看pv属性"><a href="#查看pv属性" class="headerlink" title="查看pv属性"></a>查看pv属性</h3><p>使用pvscan来查看哪些pv和基本属性。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> pvscan</span></span><br><span class="line">  PV /dev/sda2   VG centos          lvm2 [&lt;19.00 GiB / 0    free]</span><br><span class="line">  PV /dev/sdb3                      lvm2 [10.00 GiB]</span><br><span class="line">  PV /dev/sdb1                      lvm2 [5.00 GiB]</span><br><span class="line">  PV /dev/sdb2                      lvm2 [2.00 GiB]</span><br><span class="line">  Total: 4 [&lt;36.00 GiB] / in use: 1 [&lt;19.00 GiB] / in no VG: 3 [17.00 GiB]</span><br></pre></td></tr></table></figure><p>使用pvdisplay查看其中一个pv的属性信息。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> pvdisplay /dev/sdb1</span></span><br><span class="line">  &quot;/dev/sdb1&quot; is a new physical volume of &quot;5.00 GiB&quot;</span><br><span class="line">  --- NEW Physical volume ---</span><br><span class="line">  PV Name               /dev/sdb1</span><br><span class="line">  VG Name</span><br><span class="line">  PV Size               5.00 GiB</span><br><span class="line">  Allocatable           NO</span><br><span class="line">  PE Size               0</span><br><span class="line">  Total PE              0</span><br><span class="line">  Free PE               0</span><br><span class="line">  Allocated PE          0</span><br><span class="line">  PV UUID               viN4Oq-ZeFx-3wTz-4zXf-0Z4N-Urcf-F2eMGi</span><br></pre></td></tr></table></figure><h3 id="查看pe分布"><a href="#查看pe分布" class="headerlink" title="查看pe分布"></a>查看pe分布</h3><p><code>pvdisplay  -m</code>可以查看该设备中PE的使用分布图。以下是某次显示结果。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> pvdisplay  -m /dev/sda2</span></span><br><span class="line">  --- Physical volume ---</span><br><span class="line">  PV Name               /dev/sda2</span><br><span class="line">  VG Name               centos</span><br><span class="line">  PV Size               &lt;19.00 GiB / not usable 3.00 MiB</span><br><span class="line">  Allocatable           yes (but full)</span><br><span class="line">  PE Size               4.00 MiB</span><br><span class="line">  Total PE              4863</span><br><span class="line">  Free PE               0</span><br><span class="line">  Allocated PE          4863</span><br><span class="line">  PV UUID               0TXR4i-laLf-DqVh-lNVK-ybSt-io3c-QKDj5w</span><br><span class="line"></span><br><span class="line">  --- Physical Segments ---</span><br><span class="line">  Physical extent 0 to 4607:                    # 说明第0-4607的PE正被使用。PV中PE的序号是从0开始编号的</span><br><span class="line">    Logical volume      /dev/centos/root</span><br><span class="line">    Logical extents     0 to 4607               # 该PE在LV中的0-4607的LE位置上</span><br><span class="line">  Physical extent 4608 to 4862:                 # 说明4608-4862的PE正使用</span><br><span class="line">    Logical volume      /dev/centos/swap</span><br><span class="line">    Logical extents     0 to 254                # 该PE在LV中的位置是0-254</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>知道了PE的分布，就可以轻松地使用pvmove命令在设备之间进行PE数据的移动。具体关于pvmove的用法，可以自行百度，因为LVM缩容用处不大。</p><h3 id="删除pv"><a href="#删除pv" class="headerlink" title="删除pv"></a>删除pv</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> pvremove /dev/sdb3</span></span><br><span class="line">  Labels on physical volume &quot;/dev/sdb3&quot; successfully wiped.</span><br><span class="line"><span class="meta">$</span><span class="bash"> pvscan</span></span><br><span class="line">  PV /dev/sda2   VG centos          lvm2 [&lt;19.00 GiB / 0    free]</span><br><span class="line">  PV /dev/sdb1                      lvm2 [5.00 GiB]</span><br><span class="line">  PV /dev/sdb2                      lvm2 [2.00 GiB]</span><br><span class="line">  Total: 3 [&lt;26.00 GiB] / in use: 1 [&lt;19.00 GiB] / in no VG: 2 [7.00 GiB]</span><br></pre></td></tr></table></figure><h2 id="管理VG"><a href="#管理VG" class="headerlink" title="管理VG"></a>管理VG</h2><table><thead><tr><th>功能</th><th>命令</th></tr></thead><tbody><tr><td>创建vg</td><td>vgcreate</td></tr><tr><td>扫描并列出所有的vg</td><td>vgscan</td></tr><tr><td>列出vg属性信息</td><td>vgdisplay</td></tr><tr><td>移除vg</td><td>vgremove</td></tr><tr><td>从vg中移除pv</td><td>vgreduce</td></tr><tr><td>将pv添加到vg中</td><td>vgextend</td></tr><tr><td>修改vg属性</td><td>vgchange</td></tr></tbody></table><h3 id="创建vg"><a href="#创建vg" class="headerlink" title="创建vg"></a>创建vg</h3><p>创建一个名为slions_vg1的vg，并将/dev/sdb1与/dev/sdb2加入此vg，指定pe大小为8M（默认为4M）</p><blockquote><p>创建vg后，是很难再修改pe大小的，只有空数据的vg可以修改，但这样还不如重新创建vg。</p><p>创建了vg实际上是在/dev目录下管理了一个vg目录/dev/slions_vg1，不过只有在创建了lv该目录才会被创建，而该vg中创建lv，将会在该目录下生成链接文件指向/dev/dm设备。</p></blockquote><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> vgcreate -s 8M slions_vg1 /dev/sdb[1-2]</span></span><br><span class="line">  Volume group &quot;slions_vg1&quot; successfully created</span><br></pre></td></tr></table></figure><h3 id="查看vg属性"><a href="#查看vg属性" class="headerlink" title="查看vg属性"></a>查看vg属性</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> vgscan</span></span><br><span class="line">  Reading volume groups from cache.</span><br><span class="line">  Found volume group &quot;slions_vg1&quot; using metadata type lvm2</span><br><span class="line">  Found volume group &quot;centos&quot; using metadata type lvm2</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> vgdisplay slions_vg1</span></span><br><span class="line">  --- Volume group ---</span><br><span class="line">  VG Name               slions_vg1</span><br><span class="line">  System ID</span><br><span class="line">  Format                lvm2</span><br><span class="line">  Metadata Areas        2</span><br><span class="line">  Metadata Sequence No  1</span><br><span class="line">  VG Access             read/write</span><br><span class="line">  VG Status             resizable</span><br><span class="line">  MAX LV                0</span><br><span class="line">  Cur LV                0</span><br><span class="line">  Open LV               0</span><br><span class="line">  Max PV                0</span><br><span class="line">  Cur PV                2</span><br><span class="line">  Act PV                2</span><br><span class="line">  VG Size               6.98 GiB</span><br><span class="line">  PE Size               8.00 MiB</span><br><span class="line">  Total PE              894</span><br><span class="line">  Alloc PE / Size       0 / 0</span><br><span class="line">  Free  PE / Size       894 / 6.98 GiB</span><br><span class="line">  VG UUID               PVvkTd-yRWc-PgGt-kBjy-loN3-Unmm-z37JrX</span><br></pre></td></tr></table></figure><h3 id="移除pv"><a href="#移除pv" class="headerlink" title="移除pv"></a>移除pv</h3><p>从slions_vg1中移除一个pv，/dev/sdb2，再vgdisplay，发现pv少了一个，pe相应的也减少了。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> vgreduce slions_vg1 /dev/sdb2</span></span><br><span class="line">  Removed &quot;/dev/sdb2&quot; from volume group &quot;slions_vg1&quot;</span><br><span class="line"><span class="meta">$</span><span class="bash"> vgdisplay slions_vg1</span></span><br><span class="line">  --- Volume group ---</span><br><span class="line">  VG Name               slions_vg1</span><br><span class="line">  System ID</span><br><span class="line">  Format                lvm2</span><br><span class="line">  Metadata Areas        1</span><br><span class="line">  Metadata Sequence No  2</span><br><span class="line">  VG Access             read/write</span><br><span class="line">  VG Status             resizable</span><br><span class="line">  MAX LV                0</span><br><span class="line">  Cur LV                0</span><br><span class="line">  Open LV               0</span><br><span class="line">  Max PV                0</span><br><span class="line">  Cur PV                1</span><br><span class="line">  Act PV                1</span><br><span class="line">  VG Size               4.99 GiB</span><br><span class="line">  PE Size               8.00 MiB</span><br><span class="line">  Total PE              639</span><br><span class="line">  Alloc PE / Size       0 / 0</span><br><span class="line">  Free  PE / Size       639 / 4.99 GiB</span><br><span class="line">  VG UUID               PVvkTd-yRWc-PgGt-kBjy-loN3-Unmm-z37JrX</span><br><span class="line"></span><br></pre></td></tr></table></figure><h3 id="添加pv"><a href="#添加pv" class="headerlink" title="添加pv"></a>添加pv</h3><p>再将刚才删掉的/dev/sdb2添加入slions_vg1中。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> vgextend slions_vg1 /dev/sdb2</span></span><br><span class="line">  Volume group &quot;slions_vg1&quot; successfully extended</span><br></pre></td></tr></table></figure><h3 id="设置vg的状态"><a href="#设置vg的状态" class="headerlink" title="设置vg的状态"></a>设置vg的状态</h3><p>vgchange用于设置卷组的活动状态，卷组的激活状态主要影响的是lv。使用-a选项来设置。</p><p>将slions_vg1设置为活动状态。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> vgchange -a y slions_vg1</span></span><br><span class="line">  0 logical volume(s) in volume group &quot;slions_vg1&quot; now active</span><br></pre></td></tr></table></figure><p>将slions_vg1设置为非活动状态。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> vgchange -a n  slions_vg1</span></span><br><span class="line">  0 logical volume(s) in volume group &quot;slions_vg1&quot; now active</span><br></pre></td></tr></table></figure><h2 id="管理LV"><a href="#管理LV" class="headerlink" title="管理LV"></a>管理LV</h2><table><thead><tr><th>功能</th><th>命令</th></tr></thead><tbody><tr><td>创建lv</td><td>lvcreate</td></tr><tr><td>扫描并列出所有的lv</td><td>lvscan</td></tr><tr><td>列出lv属性信息</td><td>lvdisplay</td></tr><tr><td>移除lv</td><td>lvremove</td></tr><tr><td>缩小lv容量</td><td>lvreduce(lvresize)</td></tr><tr><td>增大lv容量</td><td>lvextend(lvresize)</td></tr><tr><td>改变lv容量</td><td>lvresize</td></tr></tbody></table><h3 id="创建lv"><a href="#创建lv" class="headerlink" title="创建lv"></a>创建lv</h3><p>命令格式：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">lvcreate &#123;-L size(M/G) | -l PEnum&#125; -n lv_name vg_name</span><br><span class="line">-L：根据大小来创建lv，即分配多大空间给此lv</span><br><span class="line">-l：根据PE的数量来创建lv，即分配多少个pe给此lv</span><br><span class="line">-n：指定lv的名称</span><br></pre></td></tr></table></figure><p>当前slions_vg1有894个PE，大小为6.98G。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> vgdisplay slions_vg1 |grep PE</span></span><br><span class="line">  PE Size               8.00 MiB</span><br><span class="line">  Total PE              894</span><br><span class="line">  Alloc PE / Size       0 / 0</span><br><span class="line">  Free  PE / Size       894 / 6.98 GiB</span><br></pre></td></tr></table></figure><p>使用-L和-l分别创建名称为slions_lv1和slions_lv2的lv。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> lvcreate -L 3G -n slions_lv1 slions_vg1</span></span><br><span class="line">  Logical volume &quot;slions_lv1&quot; created.</span><br><span class="line"><span class="meta">$</span><span class="bash"> lvcreate -l +100%FREE -n slions_lv2 slions_vg1   <span class="comment">#使用剩余所有的PE</span></span></span><br><span class="line">  Logical volume &quot;slions_lv2&quot; created.</span><br></pre></td></tr></table></figure><p>创建lv后，将在/dev/firstvg目录中创建对应lv名称的软链接文件，同时也在/dev/mapper目录下创建链接文件，它们都指向/dev/dm设备。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> ll /dev/slions_vg1/</span></span><br><span class="line">总用量 0</span><br><span class="line">lrwxrwxrwx. 1 root root 7 8月   7 10:49 slions_lv1 -&gt; ../dm-2</span><br><span class="line">lrwxrwxrwx. 1 root root 7 8月   7 10:50 slions_lv2 -&gt; ../dm-3</span><br><span class="line"><span class="meta">$</span><span class="bash"> ll /dev/mapper/</span></span><br><span class="line">总用量 0</span><br><span class="line">lrwxrwxrwx. 1 root root       7 8月   6 21:58 centos-root -&gt; ../dm-0</span><br><span class="line">lrwxrwxrwx. 1 root root       7 8月   6 21:58 centos-swap -&gt; ../dm-1</span><br><span class="line">crw-------. 1 root root 10, 236 8月   6 21:58 control</span><br><span class="line">lrwxrwxrwx. 1 root root       7 8月   7 10:49 slions_vg1-slions_lv1 -&gt; ../dm-2</span><br><span class="line">lrwxrwxrwx. 1 root root       7 8月   7 10:50 slions_vg1-slions_lv2 -&gt; ../dm-3</span><br></pre></td></tr></table></figure><h3 id="查看lv属性"><a href="#查看lv属性" class="headerlink" title="查看lv属性"></a>查看lv属性</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> lvscan</span></span><br><span class="line">  ACTIVE            &#x27;/dev/slions_vg1/slions_lv1&#x27; [3.00 GiB] inherit</span><br><span class="line">  ACTIVE            &#x27;/dev/slions_vg1/slions_lv2&#x27; [3.98 GiB] inherit</span><br><span class="line">  ACTIVE            &#x27;/dev/centos/root&#x27; [18.00 GiB] inherit</span><br><span class="line">  ACTIVE            &#x27;/dev/centos/swap&#x27; [1020.00 MiB] inherit</span><br><span class="line"><span class="meta">$</span><span class="bash"> lvdisplay /dev/slions_vg1/slions_lv1</span></span><br><span class="line">  --- Logical volume ---</span><br><span class="line">  LV Path                /dev/slions_vg1/slions_lv1</span><br><span class="line">  LV Name                slions_lv1</span><br><span class="line">  VG Name                slions_vg1</span><br><span class="line">  LV UUID                aUGzfw-sypb-96Ux-hRxn-JXeS-2UIL-812m4T</span><br><span class="line">  LV Write Access        read/write</span><br><span class="line">  LV Creation host, time slions_pc1, 2021-08-07 10:49:52 +0800</span><br><span class="line">  LV Status              available</span><br><span class="line"><span class="meta">  #</span><span class="bash"> open                 0</span></span><br><span class="line">  LV Size                3.00 GiB</span><br><span class="line">  Current LE             384</span><br><span class="line">  Segments               1</span><br><span class="line">  Allocation             inherit</span><br><span class="line">  Read ahead sectors     auto</span><br><span class="line">  - currently set to     256</span><br><span class="line">  Block device           253:2</span><br><span class="line"></span><br></pre></td></tr></table></figure><h3 id="格式化lv"><a href="#格式化lv" class="headerlink" title="格式化lv"></a>格式化lv</h3><p>我们通过格式化lv使其形成文件系统，就可以和普通磁盘一样挂载使用了。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> mkfs.xfs /dev/slions_vg1/slions_lv1</span></span><br><span class="line">meta-data=/dev/slions_vg1/slions_lv1 isize=512    agcount=4, agsize=196608 blks</span><br><span class="line">         =                       sectsz=512   attr=2, projid32bit=1</span><br><span class="line">         =                       crc=1        finobt=0, sparse=0</span><br><span class="line">data     =                       bsize=4096   blocks=786432, imaxpct=25</span><br><span class="line">         =                       sunit=0      swidth=0 blks</span><br><span class="line">naming   =version 2              bsize=4096   ascii-ci=0 ftype=1</span><br><span class="line">log      =internal log           bsize=4096   blocks=2560, version=2</span><br><span class="line">         =                       sectsz=512   sunit=0 blks, lazy-count=1</span><br><span class="line">realtime =none                   extsz=4096   blocks=0, rtextents=0</span><br><span class="line"><span class="meta">$</span><span class="bash"> mkdir /xfs_dir</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> mount /dev/slions_vg1/slions_lv1 /xfs_dir/</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> df -Th|grep xfs_dir</span></span><br><span class="line">/dev/mapper/slions_vg1-slions_lv1 xfs       3.0G   33M  3.0G    2% /xfs_dir</span><br><span class="line"><span class="meta">$</span><span class="bash"> lsblk</span></span><br><span class="line">NAME                      MAJ:MIN RM  SIZE RO TYPE MOUNTPOINT</span><br><span class="line">sda                         8:0    0   20G  0 disk</span><br><span class="line">├─sda1                      8:1    0    1G  0 part /boot</span><br><span class="line">└─sda2                      8:2    0   19G  0 part</span><br><span class="line">  ├─centos-root           253:0    0   18G  0 lvm  /</span><br><span class="line">  └─centos-swap           253:1    0 1020M  0 lvm  [SWAP]</span><br><span class="line">sdb                         8:16   0   20G  0 disk</span><br><span class="line">├─sdb1                      8:17   0    5G  0 part</span><br><span class="line">│ ├─slions_vg1-slions_lv1 253:2    0    3G  0 lvm  /xfs_dir</span><br><span class="line">│ └─slions_vg1-slions_lv2 253:3    0    4G  0 lvm</span><br><span class="line">├─sdb2                      8:18   0    2G  0 part</span><br><span class="line">│ └─slions_vg1-slions_lv2 253:3    0    4G  0 lvm</span><br><span class="line">├─sdb3                      8:19   0   10G  0 part</span><br><span class="line">└─sdb4                      8:20   0    3G  0 part</span><br><span class="line">sr0                        11:0    1   10G  0 rom</span><br></pre></td></tr></table></figure><p>也可以使用file -s查看lv的文件系统，由于/dev/slions_vg1和/dev/mapper下的lv都是链接到/dev/下块设备的链接文件，所以只能对块设备进行查看，否则查看的结果也仅仅只是个链接文件类型。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> file -s /dev/dm-2</span></span><br><span class="line">/dev/dm-2: SGI XFS filesystem data (blksz 4096, inosz 512, v2 dirs)</span><br></pre></td></tr></table></figure><h2 id="LVM扩容"><a href="#LVM扩容" class="headerlink" title="LVM扩容"></a>LVM扩容</h2><p>在文章的开头已经说了，lvm最大的优势就是其可伸缩性，而其伸缩性又更偏重于扩容，这是使用lvm的最大原因。</p><blockquote><p>扩容的实质是将vg中空闲的pe添加到lv中，所以只要vg中有空闲的pe，就可以进行扩容，即使没有空闲的pe，也可以添加pv，将pv加入到vg中增加空闲pe。</p></blockquote><h3 id="扩容流程"><a href="#扩容流程" class="headerlink" title="扩容流程"></a>扩容流程</h3><ol><li>使用lvextend或者lvresize添加更多的pe或容量到lv中</li><li>使用resize2fs命令(xfs则使用xfs_growfs)将lv增加后的容量增加到对应的文件系统中</li></ol><h3 id="扩容示例"><a href="#扩容示例" class="headerlink" title="扩容示例"></a>扩容示例</h3><p>将/dev/sdb3创建为pv并加入slions_vg1，查看此时vg的pe状态，已经多了1279个PE，空余大小为9.99g。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> pvcreate /dev/sdb3</span></span><br><span class="line">  Physical volume &quot;/dev/sdb3&quot; successfully created.</span><br><span class="line"><span class="meta">$</span><span class="bash"> vgextend slions_vg1 /dev/sdb3</span></span><br><span class="line">  Volume group &quot;slions_vg1&quot; successfully extended</span><br><span class="line"><span class="meta">$</span><span class="bash"> vgdisplay slions_vg1 |grep PE</span></span><br><span class="line">  PE Size               8.00 MiB</span><br><span class="line">  Total PE              2173</span><br><span class="line">  Alloc PE / Size       894 / 6.98 GiB</span><br><span class="line">  Free  PE / Size       1279 / 9.99 GiB</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>将其全部添加到slions_lv1中，有两种方式添加：按容量大小添加和按PE数量添加。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> umount /dev/slions_vg1/slions_lv1</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> lvextend -L +5G /dev/slions_vg1/slions_lv1</span></span><br><span class="line">  Size of logical volume slions_vg1/slions_lv1 changed from 3.00 GiB (384 extents) to 8.00 GiB (1024 extents).</span><br><span class="line">  Logical volume slions_vg1/slions_lv1 successfully resized.</span><br><span class="line"><span class="meta">$</span><span class="bash"> vgdisplay slions_vg1 |grep PE</span></span><br><span class="line">  PE Size               8.00 MiB</span><br><span class="line">  Total PE              2173</span><br><span class="line">  Alloc PE / Size       1534 / 11.98 GiB</span><br><span class="line">  Free  PE / Size       639 / 4.99 GiB</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">$</span><span class="bash"> lvextend -l +639 /dev/slions_vg1/slions_lv1</span></span><br><span class="line">  Size of logical volume slions_vg1/slions_lv1 changed from 8.00 GiB (1024 extents) to 12.99 GiB (1663 extents).</span><br><span class="line">  Logical volume slions_vg1/slions_lv1 successfully resized.</span><br><span class="line"><span class="meta">$</span><span class="bash"> lvscan</span></span><br><span class="line">  ACTIVE            &#x27;/dev/slions_vg1/slions_lv1&#x27; [12.99 GiB] inherit</span><br><span class="line">  ACTIVE            &#x27;/dev/slions_vg1/slions_lv2&#x27; [3.98 GiB] inherit</span><br><span class="line">  ACTIVE            &#x27;/dev/centos/root&#x27; [18.00 GiB] inherit</span><br><span class="line">  ACTIVE            &#x27;/dev/centos/swap&#x27; [1020.00 MiB] inherit</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>也可以使用lvresize来增加lv的容量方法和lvextend一样。如：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> lvresize -L +5G /dev/slions_vg1/slions_lv1</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> lvresize -l +639 /dev/slions_vg1/slions_lv1</span></span><br></pre></td></tr></table></figure><p>将slions_lv1挂载，查看该lv对应文件系统的容量。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> df -Th /xfs_dir/</span></span><br><span class="line">文件系统                          类型  容量  已用  可用 已用% 挂载点</span><br><span class="line">/dev/mapper/slions_vg1-slions_lv1 xfs   3.0G   33M  3.0G    2% /xfs_dir</span><br></pre></td></tr></table></figure><p>容量并没有增加，因为只是lv的容量增加了，而文件系统的容量却没有增加。</p><p>需要使用resize2fs工具来改变ext文件系统的大小，如果是xfs文件系统，则使用xfs_growfs。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> xfs_growfs /xfs_dir/</span></span><br><span class="line">meta-data=/dev/mapper/slions_vg1-slions_lv1 isize=512    agcount=4, agsize=196608 blks</span><br><span class="line">         =                       sectsz=512   attr=2, projid32bit=1</span><br><span class="line">         =                       crc=1        finobt=0 spinodes=0</span><br><span class="line">data     =                       bsize=4096   blocks=786432, imaxpct=25</span><br><span class="line">         =                       sunit=0      swidth=0 blks</span><br><span class="line">naming   =version 2              bsize=4096   ascii-ci=0 ftype=1</span><br><span class="line">log      =internal               bsize=4096   blocks=2560, version=2</span><br><span class="line">         =                       sectsz=512   sunit=0 blks, lazy-count=1</span><br><span class="line">realtime =none                   extsz=4096   blocks=0, rtextents=0</span><br><span class="line">data blocks changed from 786432 to 3405824</span><br><span class="line"><span class="meta">$</span><span class="bash"> df -Th /xfs_dir/</span></span><br><span class="line">文件系统                          类型  容量  已用  可用 已用% 挂载点</span><br><span class="line">/dev/mapper/slions_vg1-slions_lv1 xfs    13G   33M   13G    1% /xfs_dir</span><br><span class="line"></span><br></pre></td></tr></table></figure><h1 id="END"><a href="#END" class="headerlink" title="END"></a>END</h1><p>其实LVM里还包括了缩容与快照的功能，使用场景不多，这里也不多阐述，且现在大多文件系统都为xfs，其也不支持收缩，想要了解的可自行百度。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;总所周知不论是机械硬盘还是固态磁盘大小都是固定的，当磁盘空间满了后只能通过删除无用数据来保持磁盘的可用性，这篇介绍的LVM(Logical Volume Manager)是建立在磁盘和分区之上的一个逻辑层，用来提高磁盘分区管理的灵活性，可以随时随地的扩缩容分区大小。&lt;/p&gt;</summary>
      
    
    
    
    <category term="存储类" scheme="https://slions.github.io/categories/%E5%AD%98%E5%82%A8%E7%B1%BB/"/>
    
    
    <category term="linux" scheme="https://slions.github.io/tags/linux/"/>
    
  </entry>
  
  <entry>
    <title>Virtualbox虚机使用介绍</title>
    <link href="https://slions.github.io/2021/08/05/Virtualbox%E8%99%9A%E6%9C%BA%E4%BD%BF%E7%94%A8%E4%BB%8B%E7%BB%8D/"/>
    <id>https://slions.github.io/2021/08/05/Virtualbox%E8%99%9A%E6%9C%BA%E4%BD%BF%E7%94%A8%E4%BB%8B%E7%BB%8D/</id>
    <published>2021-08-05T11:24:23.000Z</published>
    <updated>2021-08-12T11:48:04.057Z</updated>
    
    <content type="html"><![CDATA[<p> VMware workstation是最常见也是最常用的虚拟机工具，但是很多公司不会购买其商用授权，VirtualBox 也是一款虚拟化产品，重要的是其<strong>开源免费</strong>，好早之前在本地安装使用了下，感觉与VMware workstation比还是不太习惯，包括虚拟机的安装和设置也要重新习惯，本文主要讲下安装好软件后如何完成基础设置与网络配置，安装部分比较简单请自行解决。</p><h1 id="新建虚机"><a href="#新建虚机" class="headerlink" title="新建虚机"></a>新建虚机</h1><ol><li>进入virtualbox界面点击新建则开始创建，设置你虚机的名字，存储位置，和基础的系统版本（之前VMware导入virtualbox会报错有冲突，如果想尝试导入的可自行百度）。</li></ol><p><img src="/doc_picture/virtualbox1.png" alt="image-20210805193414305"></p><ol start="2"><li> 下一步设置根分区的大小。</li></ol><p><img src="/doc_picture/virtualbox2.png" alt="image-20210805193500544"></p><ol start="3"><li>确定后设置你的虚机，先选择虚机的启动镜像。这里的iso就决定了你的系统版本与内核版本。</li></ol><p><img src="/doc_picture/virtualbox3.png" alt="image-20210805193540746"></p><ol start="4"><li>配置网卡模式，先那桥接模式测试，下面会说明这几个模式的区别。（之后的环境需配置成两块网卡，一个nat，一个仅主机）</li></ol><p><img src="/doc_picture/virtualbox4.png" alt="image-20210805193612460"></p><ol start="5"><li>确认无误后，启动虚机。</li></ol><p><img src="/doc_picture/virtualbox5.png" alt="image-20210805193650102"></p><ol start="6"><li>接下来的步骤就和VMware虚机安装一样了，建议选择基础设施服务器，要不默认是最小化安装，会少很多命令。（鼠标切换是方向键旁边的CTRL）</li></ol><p><img src="/doc_picture/virtualbox6.png" alt="image-20210805193717617"></p><h1 id="配置网卡"><a href="#配置网卡" class="headerlink" title="配置网卡"></a>配置网卡</h1><ol><li>进入虚机可以看到我们的网卡会通过dhcp自动获取到一个ip,为了防止重启后ip发生改变，需要我们手动配置一个同子网的ip。</li></ol><p><img src="/doc_picture/virtualbox7.png" alt="image-20210805193815935"></p><ol start="2"><li><p>vim /etc/sysconfig/network-scripts/ifcfg-enp0s3</p><p>关键的是BOOTPROTO要设置成static。</p></li></ol><p><img src="/doc_picture/virtualbox8.png" alt="image-20210805193903367"></p><ol start="3"><li>重启网卡</li></ol><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> systemctl restart network</span></span><br></pre></td></tr></table></figure><ol start="4"><li>查看网卡信息已经变成我们设置的ip，ping 百度测试ok。</li></ol><p><img src="/doc_picture/virtualbox9.png" alt="image-20210805194051944"></p><p>​    主机通过ssh连接本地虚机。ok</p><p><img src="/doc_picture/virtualbox10.png" alt="image-20210805194139658"></p><h1 id="克隆虚拟机"><a href="#克隆虚拟机" class="headerlink" title="克隆虚拟机"></a>克隆虚拟机</h1><p>处在关机状态的虚机才能克隆，点击想要克隆的虚机右键选择复制，重要的是下面的重新初始化所有网卡的MAC地址，这样就可以重新分配地址，防止两个虚拟机同时开启时发生冲突。</p><p><img src="/doc_picture/virtualbox11.png" alt="image-20210805194313661"></p><p>为了保持两个虚拟机的独立性，建议采用完全拷贝的模式。</p><p><img src="/doc_picture/virtualbox12.png" alt="image-20210805194359103"></p><p>等待克隆完成就行了，之后修改主机名，修改网卡地址，和vmware操作一样。</p><h1 id="网卡模式说明"><a href="#网卡模式说明" class="headerlink" title="网卡模式说明"></a>网卡模式说明</h1><p>我们可以在安装虚机时会让我们设置网卡的方式，这些也可以通过安装好虚机后修改。</p><p>下面解释下常用的网卡模式意义。</p><p><img src="/doc_picture/virtualbox13.png" alt="image-20210805194504261"></p><h2 id="NAT"><a href="#NAT" class="headerlink" title="NAT"></a>NAT</h2><p>Guest访问网络的所有数据都是由主机做了一层路由转换，Guest并不真实存在于网络中，主机与网络中的任何机器都不能查看和访问到Guest的存在。</p><ul><li>Guest可以访问主机能访问到的所有网络，但是对于主机以及主机网络上的其他机器，Guest又是不可见的，甚至主机也访问不到Guest。</li><li>虚拟机与主机的关系：只能单向访问，虚拟机可以通过网络访问到主机，主机无法通过网络访问到虚拟机。</li><li>虚拟机与网络中其他主机的关系：只能单向访问，虚拟机可以访问到网络中其他主机，其他主机不能通过网络访问到虚拟机。</li><li>虚拟机与虚拟机的关系：相互不能访问，虚拟机与虚拟机各自完全独立，相互间无法通过网络访问彼此。</li></ul><h2 id="Bridged-Adapter"><a href="#Bridged-Adapter" class="headerlink" title="Bridged Adapter"></a>Bridged Adapter</h2><p>它与主机网卡在用一个子网中，访问外网会直接走本地的网卡出去。这时虚拟机能被分配到一个网络中独立的IP，所有网络功能完全和在网络中的真实机器一样。</p><ul><li>虚拟机与主机的关系：可以相互访问，因为虚拟机在真实网络段中有独立IP，主机与虚拟机处于同一网络段中，彼此可以通过各自IP相互访问。</li><li>虚拟机于网络中其他主机的关系：可以相互访问，同样因为虚拟机在真实网络段中有独立IP，虚拟机与所有网络其他主机处于同一网络段中，彼此可以通过各自IP相互访问。</li><li>虚拟机与虚拟机的关系：可以相互访问，原因同上。</li></ul><h2 id="Internal（内部网络）"><a href="#Internal（内部网络）" class="headerlink" title="Internal（内部网络）"></a>Internal（内部网络）</h2><p>内网模式，顾名思义就是内部网络模式：</p><ul><li>虚拟机与外网完全断开，只实现虚拟机于虚拟机之间的内部网络模式。</li><li>虚拟机与主机的关系：不能相互访问，彼此不属于同一个网络，无法相互访问。</li><li>虚拟机与网络中其他主机的关系：不能相互访问，理由同上。</li><li>虚拟机与虚拟机的关系：可以相互访问，前提是在设置网络时，两台虚拟机设置同一网络名称。</li></ul><h2 id="Host-only-Adapter"><a href="#Host-only-Adapter" class="headerlink" title="Host-only Adapter"></a>Host-only Adapter</h2><p>仅主机模式，从名字可以看出只有当前主机可以连接，网上看到可以通过网卡共享和网卡桥接来实现访问外网，那不如直接配成桥接模式。</p><ul><li>虚拟机不可以上网</li><li>虚拟机与虚拟机的关系：可以相互访问</li><li>虚拟机与主机的关系：可以相互访问（注意虚拟机与主机通信是通过主机的名为VirtualBox Host-Only Network的网卡，因此ip是该网卡ip 192.168.56.1，而不是你现在正在上网所用的ip，可以自己配置网段）</li></ul><blockquote><p>可以看出桥接与nat都可以使虚机联网，这里建议使用<strong>双网卡</strong>（nat+仅主机模式）方式，因为桥接网卡设置为使用wifi网卡的话，则断网与更换wifi都会导致虚机不可用，所以推荐使用nat连接外网，使用仅主机来连接本地ssh终端。</p></blockquote><blockquote><p>nat默认使用10.0.2.0/24网段</p><p>仅主机默认使用192.168.56.0/24网段</p></blockquote><h1 id="修改网卡名（可选项）"><a href="#修改网卡名（可选项）" class="headerlink" title="修改网卡名（可选项）"></a>修改网卡名（可选项）</h1><p>可以看到系统默认为我们添加的网卡名是enp0s3和enp0s8，不好记也容易弄混，下面我会演示如何设置成标准的网卡名。</p><ol><li><p>修改系统启动程序文件，/etc/sysconfig/grub</p><p>在GRUB_CMDLINE_LINUX后添加<code>net.ifnames=0 biosdevname=0 </code>保存退出。</p></li></ol><p><img src="/doc_picture/virtualbox14.png" alt="image-20210805195105039"></p><p>   2.更新grub信息。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> grub2-mkconfig -o /boot/grub2/grub.cfg</span></span><br></pre></td></tr></table></figure><p><img src="/doc_picture/virtualbox15.png" alt="image-20210805195158475"></p><ol start="3"><li>重启系统</li></ol><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> reboot</span></span><br></pre></td></tr></table></figure><ol start="4"><li>查看网卡已经变成了eth0与eth1(ip要重新设置下)。</li></ol><p><img src="/doc_picture/virtualbox16.png" alt="image-20210805195316288"></p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt; VMware workstation是最常见也是最常用的虚拟机工具，但是很多公司不会购买其商用授权，VirtualBox 也是一款虚拟化产品，重要的是其&lt;strong&gt;开源免费&lt;/strong&gt;，好早之前在本地安装使用了下，感觉与VMware workstation比还是</summary>
      
    
    
    
    <category term="虚拟机" scheme="https://slions.github.io/categories/%E8%99%9A%E6%8B%9F%E6%9C%BA/"/>
    
    
    <category term="虚拟机相关" scheme="https://slions.github.io/tags/%E8%99%9A%E6%8B%9F%E6%9C%BA%E7%9B%B8%E5%85%B3/"/>
    
  </entry>
  
  <entry>
    <title>df与du结果不一致问题</title>
    <link href="https://slions.github.io/2021/08/03/df%E4%B8%8Edu%E7%BB%93%E6%9E%9C%E4%B8%8D%E4%B8%80%E8%87%B4%E9%97%AE%E9%A2%98/"/>
    <id>https://slions.github.io/2021/08/03/df%E4%B8%8Edu%E7%BB%93%E6%9E%9C%E4%B8%8D%E4%B8%80%E8%87%B4%E9%97%AE%E9%A2%98/</id>
    <published>2021-08-03T10:37:53.000Z</published>
    <updated>2021-08-12T11:48:22.751Z</updated>
    
    <content type="html"><![CDATA[<p><code>df</code>与<code>du</code>命令都是运维人员常用的检测存储空间大小的命令，平时我们并不太关注这两个命令的差别，但是经常会遇到这样一种场景，使用df查看到空间使用率已经非常高了，但使用du命令排查时发现不存在占用空间大的文件， 两者间的结果不一致。</p><h1 id="du与df"><a href="#du与df" class="headerlink" title="du与df"></a>du与df</h1><p><strong>du</strong>，disk usage,是通过搜索文件来计算每个文件的大小然后累加，du能看到的文件只是一些当前存在的，没有被删除的。他计算的大小就是当前他认为存在的所有文件大小的累加和。</p><p><strong>df</strong>，disk free，通过文件系统来快速获取空间大小的信息，当我们删除一个文件的时候，这个文件不是马上就在文件系统当中消失了，而是暂时消失了，当所有程序都不用时，才会根据OS的规则释放掉已经删除的文件， df记录的是通过文件系统获取到的文件的大小，他比du强的地方就是能够看到已经删除的文件，而且计算大小的时候，把这一部分的空间也加上了，更精确了。</p><p>因此,如果用户删除了一个正在运行的应用所打开的某个目录下的文件，则du命令返回的值显示出减去了该文件后的目录的大小。但df命令并不显示减去该文件后的大小。直到该运行的应用关闭了这个打开的文件，df返回的值才显示出减去了该文件后的文件系统的使用情况。</p><p>通过lsof工具我们可以直观的排查到具体的问题进程，以便解决：</p><h1 id="模拟案例"><a href="#模拟案例" class="headerlink" title="模拟案例"></a>模拟案例</h1><p>分别创建一个500M和1000M大小的文件。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> dd <span class="keyword">if</span>=/dev/zero of=500mFile bs=1M count=500</span></span><br><span class="line">记录了500+0 的读入</span><br><span class="line">记录了500+0 的写出</span><br><span class="line">524288000字节(524 MB)已复制，1.34728 秒，389 MB/秒</span><br><span class="line"><span class="meta">$</span><span class="bash"> dd <span class="keyword">if</span>=/dev/zero of=1000mFile bs=1M count=1000</span></span><br><span class="line">记录了1000+0 的读入</span><br><span class="line">记录了1000+0 的写出</span><br><span class="line">1048576000字节(1.0 GB)已复制，13.3469 秒，78.6 MB/秒</span><br><span class="line"><span class="meta">$</span><span class="bash"> du -ha *</span></span><br><span class="line">1000M   1000mFile</span><br><span class="line">500M    500mFile</span><br><span class="line"><span class="meta">$</span><span class="bash"> df -h `<span class="built_in">pwd</span>`</span></span><br><span class="line">文件系统                 容量  已用  可用 已用% 挂载点</span><br><span class="line">/dev/mapper/centos-root   18G  4.0G   15G   22% /</span><br></pre></td></tr></table></figure><p>然后开两个终端分别使用tail命令查看。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 终端1执行</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> tail -f 500mFile</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 终端2执行</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> tail -f 1000mFile</span></span><br></pre></td></tr></table></figure><p>再开启一个终端使用lsof命令查看这两个文件的状态。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> lsof |grep File</span></span><br><span class="line">tail      10105                root    3r      REG              253,0  524288000   51733031 /home/slions/500mFile</span><br><span class="line">tail      10106                root    3r      REG              253,0 1048576000   51738061 /home/slions/1000mFile</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>这个时候我们使用rm命令来删除了这两个文件。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> rm -rf 1000mFile 500mFile</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> du -ha .</span></span><br><span class="line">0       .</span><br><span class="line"><span class="meta">$</span><span class="bash"> df -h `<span class="built_in">pwd</span>`</span></span><br><span class="line">文件系统                 容量  已用  可用 已用% 挂载点</span><br><span class="line">/dev/mapper/centos-root   18G  4.0G   15G   22% /</span><br></pre></td></tr></table></figure><p>可以看到du已经显示为0，df无任何变化。通过lsof再看下进程状态。(sort -nrk 7是进行大小排序)</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> lsof |grep File |sort -nrk 7</span></span><br><span class="line">tail      10106                root    3r      REG              253,0 1048576000   51738061 /home/slions/1000mFile (deleted)</span><br><span class="line">tail      10105                root    3r      REG              253,0  524288000   51733031 /home/slions/500mFile (deleted)</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>这个deleted表示该已经删除了的文件，但是文件句柄未释放。</p><p>想要释放此句柄直接kill掉对应进程就好了。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> <span class="built_in">kill</span> -9 10106</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> <span class="built_in">kill</span> -9 10105</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> df -h `<span class="built_in">pwd</span>`</span></span><br><span class="line">文件系统                 容量  已用  可用 已用% 挂载点</span><br><span class="line">/dev/mapper/centos-root   18G  2.5G   16G   14% /</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>可以看到，df的已用容量对应减少了1.5G。</p><p>在日常的运维工作中，我们可以直接通过以下命令来快速定位未释放文件句柄的进程，从而进行解决。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> lsof |grep deleted</span></span><br></pre></td></tr></table></figure><h1 id="建议"><a href="#建议" class="headerlink" title="建议"></a>建议</h1><p>在日常运维过程中，如果我们需要删除比较大的文件 可以使用 <code>&gt; filename </code>，这种可以直接释放磁盘空间，使用 rm 如果有进程在访问文件，则有可能出现磁盘空间不释放的情况。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;&lt;code&gt;df&lt;/code&gt;与&lt;code&gt;du&lt;/code&gt;命令都是运维人员常用的检测存储空间大小的命令，平时我们并不太关注这两个命令的差别，但是经常会遇到这样一种场景，使用df查看到空间使用率已经非常高了，但使用du命令排查时发现不存在占用空间大的文件， 两者间的结果不</summary>
      
    
    
    
    <category term="linux系统" scheme="https://slions.github.io/categories/linux%E7%B3%BB%E7%BB%9F/"/>
    
    
    <category term="linux" scheme="https://slions.github.io/tags/linux/"/>
    
  </entry>
  
  <entry>
    <title>磁盘分区工具</title>
    <link href="https://slions.github.io/2021/08/02/%E7%A3%81%E7%9B%98%E5%88%86%E5%8C%BA%E5%B7%A5%E5%85%B7/"/>
    <id>https://slions.github.io/2021/08/02/%E7%A3%81%E7%9B%98%E5%88%86%E5%8C%BA%E5%B7%A5%E5%85%B7/</id>
    <published>2021-08-02T12:40:24.000Z</published>
    <updated>2021-08-12T11:48:46.808Z</updated>
    
    <content type="html"><![CDATA[<p>在我们日常的运维工作中，磁盘分区是必备的一项技能，掌握了就可以更好的规划存储空间，提高资源的利用率。</p><p>常见的磁盘分区工具有<code>fdisk</code>、<code>parted</code>、<code>gdisk</code></p><p>使用方式有些许的差异，除了都支持交互型操作外，parted天生支持非交互的能力，而fdisk与gdisk需要我们来换种思路实现非交互式。</p><blockquote><p>关于这些工具交互式的操作命令比较简单，可以直接通过man手册来巩固，以下主要介绍下如何实现非交互式，后续使磁盘分区操作脚本化。</p></blockquote><h1 id="fdisk"><a href="#fdisk" class="headerlink" title="fdisk"></a>fdisk</h1><p>一般情况下，我们都是选择使用fdisk工具来进行分区，而常用的fdisk这个工具对分区是有大小限制的，它只能划分<strong>小于2T</strong>的磁盘，所以在划大于2T磁盘分区的时候fdisk就无法满足要求了。</p><p>fdisk工具用来分MBR磁盘上的区。要分GPT磁盘上的区，可以使用gdisk。parted工具对这两种格式的磁盘分区都支持。</p><p>fdisk操作全部是在内存中执行的，必须保存生效。保存后，内核还未识别该分区，可以查看/proc/partition目录下存在的文件，这些文件是能被内核识别的分区。运行partprobe或partx命令重新读取分区表让内核识别新的分区，内核识别后才可以格式化。而且分区结束时按w保存分区表有时候会失败，提示重启，这时候运行partprobe命令可以代替重启就生效。</p><h1 id="gdisk"><a href="#gdisk" class="headerlink" title="gdisk"></a>gdisk</h1><p>gdisk用来划分gpt分区，需要单独安装这个工具包。</p><h1 id="Parted"><a href="#Parted" class="headerlink" title="Parted"></a>Parted</h1><p>parted支持mbr格式和gpt格式的磁盘分区。它的强大在于可以一步到位而不需要不断的交互式输入(也可以交互式)。</p><p>parted分区工具是实时的，所以每一步操作都是直接写入磁盘而不是写进内存，它不像fdisk/gdisk还需要w命令将内存中的结果保存到磁盘中。</p><h1 id="fdisk实现非交互"><a href="#fdisk实现非交互" class="headerlink" title="fdisk实现非交互"></a>fdisk实现非交互</h1><p>fdisk实现非交互的原理是从标准输入中读取，每读取一行传递一次操作。</p><p>所以可以有两种方式：</p><ul><li>使用echo和管道传递</li><li>将操作写入到文件中，从文件中读取。</li></ul><p>例如：下面的命令创建了两个分区。使用默认值时传递空行即可。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> <span class="built_in">echo</span> -e <span class="string">&quot;n\np\n1\n\n+5G\nn\np\n2\n\n+1G\nw\n&quot;</span>  | fdisk /dev/sdb</span></span><br></pre></td></tr></table></figure><p>如果要传递的操作很多，则可以将它们写入到一个文件中，从文件中读取。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> <span class="built_in">echo</span> -e <span class="string">&quot;n\np\n1\n\n+5G\nn\np\n2\n\n+1G\nw\n&quot;</span> &gt;/tmp/a.txt</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> fdisk /dev/sdb &lt;/tmp/a.txt</span></span><br></pre></td></tr></table></figure><h1 id="gdisk实现非交互"><a href="#gdisk实现非交互" class="headerlink" title="gdisk实现非交互"></a>gdisk实现非交互</h1><p>原理同fdisk。</p><p>例如：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> <span class="built_in">echo</span> -e <span class="string">&quot;n\n1\n\n+3G\n\nw\nY\n&quot;</span> | gdisk /dev/sdb</span></span><br></pre></td></tr></table></figure><p>上面传递的各参数意义为：</p><p>新建分区，分区number为1，使用默认开始扇区位置，分区大小+3G，使用默认分区类型，保存，确认。</p><h1 id="parted实现非交互"><a href="#parted实现非交互" class="headerlink" title="parted实现非交互"></a>parted实现非交互</h1><p>parted命令只能一次非交互一个命令中的所有动作。如下所示：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">parted /dev/sdb mklabel msdos                 # 设置硬盘flag(msdos/gpt)</span><br><span class="line">parted /dev/sdb mkpart primary ext4 1 1000   # Mbr格式分区，分别是partition type/fstype/start/end</span><br><span class="line">parted /dev/sdb mkpart 1 ext4 1M 10240M      # gpt格式分区，分别是name/fstype/start/end</span><br><span class="line">parted /dev/sdb mkpart 1 10G 15G             # 省略fstype的交互式分区</span><br><span class="line">parted /dev/sdb rm 1                         # 删除分区</span><br><span class="line">parted /dev/sdb p                            # 输出信息</span><br></pre></td></tr></table></figure><p>如果不确定分区的起点大小，可以加上-s选项使用script模式，该模式下parted将回答一切默认值，如yes、no。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;在我们日常的运维工作中，磁盘分区是必备的一项技能，掌握了就可以更好的规划存储空间，提高资源的利用率。&lt;/p&gt;
&lt;p&gt;常见的磁盘分区工具有&lt;code&gt;fdisk&lt;/code&gt;、&lt;code&gt;parted&lt;/code&gt;、&lt;code&gt;gdisk&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;使用方式</summary>
      
    
    
    
    <category term="linux系统" scheme="https://slions.github.io/categories/linux%E7%B3%BB%E7%BB%9F/"/>
    
    
    <category term="linux" scheme="https://slions.github.io/tags/linux/"/>
    
  </entry>
  
  <entry>
    <title>ceph mimic对接k8s 1.17.0(ceph-csi)</title>
    <link href="https://slions.github.io/2021/08/02/ceph%20mimic%E5%AF%B9%E6%8E%A5k8s%201.17.0/"/>
    <id>https://slions.github.io/2021/08/02/ceph%20mimic%E5%AF%B9%E6%8E%A5k8s%201.17.0/</id>
    <published>2021-08-02T03:57:37.000Z</published>
    <updated>2021-08-12T11:49:30.731Z</updated>
    
    <content type="html"><![CDATA[<p>最近测试ceph rbd在kubernetes的自动扩容问题，之前K8s v1.11.0时的策略是先找到目标卷，使用rbd resize命令对此卷扩容，找到挂载此卷的客户端宿主机，执行xfs_growfs等刷新文件系统的命令。查看网上资料k8s 在1.15版本后，ExpandInUsePersistentVolume功能被开启。意思大概就是不需要挂载到容器即可扩容PVC。按网上的手册设置相关参数并没啥用，发现github有人提到ceph-csi可以实现自动扩容pvc的功能。</p><p><img src="/doc_picture/ceph-1.png" alt="image-20210802120109616"></p><p>这里简单说下csi是啥，全称是Container Storage Interface，旨在能为容器编排引擎和存储系统间建立一套标准的存储调用接口，通过该接口能为容器编排引擎提供存储服务。</p><p>csi之前，k8s提供的存储服务通过一种“in-tree”的方式提供的，这种方式需要将存储提供者的代码逻辑放到K8S的代码库中运行，调用引擎与插件间属于强耦合。</p><h1 id="环境描述"><a href="#环境描述" class="headerlink" title="环境描述"></a>环境描述</h1><p>k8s版本：kubernetes  v1.17.0</p><p>ceph版本：ceph mimic</p><table><thead><tr><th><strong>主机名</strong></th><th>ip地址</th><th><strong>系统版本</strong>\内核版本</th><th><strong>角色</strong></th></tr></thead><tbody><tr><td>ceph1</td><td>192.168.186.10</td><td>centos 7.6\3.10.0-957.el7.x86_64</td><td>K8s_master,ceph mon,osd,mds</td></tr><tr><td>ceph2</td><td>192.168.186.11</td><td>centos 7.6\3.10.0-957.el7.x86_64</td><td>K8s_node,ceph mon,osd,mds</td></tr><tr><td>ceph3</td><td>192.168.186.12</td><td>centos 7.6\3.10.0-957.el7.x86_64</td><td>K8s_node,ceph mon,osd,mds</td></tr></tbody></table><h1 id="整体架构图"><a href="#整体架构图" class="headerlink" title="整体架构图"></a>整体架构图</h1><blockquote><p>根据ceph官方描述：<br>ceph-csi默认情况下使用RBD内核模块，这些模块可能不支持所有Ceph CRUSH可调参数或RBD图像功能。</p></blockquote><p><img src="/doc_picture/ceph-2.png" alt="image-20210802121043578"></p><h1 id="安装步骤"><a href="#安装步骤" class="headerlink" title="安装步骤"></a>安装步骤</h1><p>我本地已经提前安装好了kubernetes与ceph,以下仅叙述如何对接。</p><h2 id="1-创建存储池"><a href="#1-创建存储池" class="headerlink" title="1.    创建存储池"></a>1.    创建存储池</h2><p>ceph在L版本之后就不会创建默认的rbd池了，我们需要建立一个单独的存储池给kubernetes使用。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> ceph osd pool create kubernetes 64 64</span></span><br></pre></td></tr></table></figure><p>初始化新创建的池。</p><blockquote><p>这里的初始化池操作在jewel版本是不需要的，jewel之后的版本在创建了池后还需要开启对应的应用授权（rbd,cephfs,rgw）,命令为</p><p><code>ceph osd pool application enable &lt;pool-name&gt; &lt;app-name&gt;</code></p></blockquote><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> rbd pool init kubernetes</span></span><br></pre></td></tr></table></figure><p>这里查看创建的kubernetes池自动加入了rbd池。</p><p><img src="/doc_picture/ceph-3.png" alt="image-20210802121704441"></p><h2 id="2-配置ceph-csi"><a href="#2-配置ceph-csi" class="headerlink" title="2.    配置ceph-csi"></a>2.    配置ceph-csi</h2><p>设置ceph客户端身份验证。</p><blockquote><p>官方提供的命令是：</p><p><code>ceph auth get-or-create client.kubernetes mon &#39;profile rbd&#39; osd &#39;profile rbd pool=kubernetes&#39; mgr &#39;profile rbd pool=kubernetes&#39;</code></p></blockquote><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> ceph auth get-or-create client.kubernetes mon <span class="string">&#x27;allow r&#x27;</span> osd <span class="string">&#x27;allow rwx pool=kubernetes&#x27;</span> -o ceph.client.kubernetes.keyring</span></span><br></pre></td></tr></table></figure><p>生成文件中的key使用user的key，后面配置中是需要用到的</p><p><img src="/doc_picture/ceph-4.png" alt="image-20210802121940950"></p><h2 id="3-生成ceph-csi的configmap"><a href="#3-生成ceph-csi的configmap" class="headerlink" title="3.    生成ceph-csi的configmap"></a>3.    生成ceph-csi的configmap</h2><p><img src="/doc_picture/ceph-5.png" alt="image-20210802122044038"></p><p>这里一共有两个需要使用的信息，第一个是fsid(集群id)，第二个是监控节点信息。</p><blockquote><p>看到有人查询到的监控节点信息有2个版本（v1和v2），目前的ceph-csi只支持V1版本的协议，所以监控节点那里我们只能用v1的那个IP和端口号，我这里不需要改动</p></blockquote><p>编写对应的configmap。</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">ConfigMap</span></span><br><span class="line"><span class="attr">data:</span></span><br><span class="line">  <span class="attr">config.json:</span> <span class="string">|-</span></span><br><span class="line"><span class="string">    [</span></span><br><span class="line"><span class="string">      &#123;</span></span><br><span class="line"><span class="string">        &quot;clusterID&quot;: &quot;10594fb3-68f3-4c97-8e0b-df80ba2a6745&quot;,</span></span><br><span class="line"><span class="string">        &quot;monitors&quot;: [</span></span><br><span class="line"><span class="string">          &quot;192.168.186.10:6789&quot;,</span></span><br><span class="line"><span class="string">          &quot;192.168.186.11:6789&quot;,</span></span><br><span class="line"><span class="string">          &quot;192.168.186.12:6789&quot;</span></span><br><span class="line"><span class="string">        ]</span></span><br><span class="line"><span class="string">      &#125;</span></span><br><span class="line"><span class="string">    ]</span></span><br><span class="line"><span class="string"></span><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">ceph-csi-config</span></span><br></pre></td></tr></table></figure><p>部署：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> kubectl apply -f csi-config-map.yaml</span></span><br></pre></td></tr></table></figure><h2 id="4-生成ceph-csi认证的secret"><a href="#4-生成ceph-csi认证的secret" class="headerlink" title="4.    生成ceph-csi认证的secret"></a>4.    生成ceph-csi认证的secret</h2><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Secret</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">csi-rbd-secret</span></span><br><span class="line">  <span class="attr">namespace:</span> <span class="string">default</span></span><br><span class="line"><span class="attr">stringData:</span></span><br><span class="line">  <span class="attr">userID:</span> <span class="string">kubernetes</span></span><br><span class="line">  <span class="attr">userKey:</span> <span class="string">AQBEpRdf2MXxFxAA8JGQQhTX1XIHPSSbw72Gqw==</span></span><br></pre></td></tr></table></figure><p>这里就用到了之前生成的用户的用户id(kubernetes)和key</p><p>部署：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> kubectl apply -f csi-rbd-secret.yaml</span></span><br></pre></td></tr></table></figure><h2 id="5-配置ceph-csi插件"><a href="#5-配置ceph-csi插件" class="headerlink" title="5.    配置ceph-csi插件"></a>5.    配置ceph-csi插件</h2><p>这里的插件就是配置kubernetes上的rbac和提供存储功能的容器。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> kubectl apply -f csi-provisioner-rbac.yaml</span></span><br></pre></td></tr></table></figure><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">$</span> <span class="string">cat</span> <span class="string">csi-provisioner-rbac.yaml</span></span><br><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">ServiceAccount</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">rbd-csi-provisioner</span></span><br><span class="line"></span><br><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">ClusterRole</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">rbac.authorization.k8s.io/v1</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">rbd-external-provisioner-runner</span></span><br><span class="line"><span class="attr">aggregationRule:</span></span><br><span class="line">  <span class="attr">clusterRoleSelectors:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">matchLabels:</span></span><br><span class="line">        <span class="attr">rbac.rbd.csi.ceph.com/aggregate-to-rbd-external-provisioner-runner:</span> <span class="string">&quot;true&quot;</span></span><br><span class="line"><span class="attr">rules:</span> []</span><br><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">ClusterRole</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">rbac.authorization.k8s.io/v1</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">rbd-external-provisioner-runner-rules</span></span><br><span class="line">  <span class="attr">labels:</span></span><br><span class="line">    <span class="attr">rbac.rbd.csi.ceph.com/aggregate-to-rbd-external-provisioner-runner:</span> <span class="string">&quot;true&quot;</span></span><br><span class="line"><span class="attr">rules:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">apiGroups:</span> [<span class="string">&quot;&quot;</span>]</span><br><span class="line">    <span class="attr">resources:</span> [<span class="string">&quot;nodes&quot;</span>]</span><br><span class="line">    <span class="attr">verbs:</span> [<span class="string">&quot;get&quot;</span>, <span class="string">&quot;list&quot;</span>, <span class="string">&quot;watch&quot;</span>]</span><br><span class="line">  <span class="bullet">-</span> <span class="attr">apiGroups:</span> [<span class="string">&quot;&quot;</span>]</span><br><span class="line">    <span class="attr">resources:</span> [<span class="string">&quot;secrets&quot;</span>]</span><br><span class="line">    <span class="attr">verbs:</span> [<span class="string">&quot;get&quot;</span>, <span class="string">&quot;list&quot;</span>]</span><br><span class="line">  <span class="bullet">-</span> <span class="attr">apiGroups:</span> [<span class="string">&quot;&quot;</span>]</span><br><span class="line">    <span class="attr">resources:</span> [<span class="string">&quot;events&quot;</span>]</span><br><span class="line">    <span class="attr">verbs:</span> [<span class="string">&quot;list&quot;</span>, <span class="string">&quot;watch&quot;</span>, <span class="string">&quot;create&quot;</span>, <span class="string">&quot;update&quot;</span>, <span class="string">&quot;patch&quot;</span>]</span><br><span class="line">  <span class="bullet">-</span> <span class="attr">apiGroups:</span> [<span class="string">&quot;&quot;</span>]</span><br><span class="line">    <span class="attr">resources:</span> [<span class="string">&quot;persistentvolumes&quot;</span>]</span><br><span class="line">    <span class="attr">verbs:</span> [<span class="string">&quot;get&quot;</span>, <span class="string">&quot;list&quot;</span>, <span class="string">&quot;watch&quot;</span>, <span class="string">&quot;create&quot;</span>, <span class="string">&quot;update&quot;</span>, <span class="string">&quot;delete&quot;</span>, <span class="string">&quot;patch&quot;</span>]</span><br><span class="line">  <span class="bullet">-</span> <span class="attr">apiGroups:</span> [<span class="string">&quot;&quot;</span>]</span><br><span class="line">    <span class="attr">resources:</span> [<span class="string">&quot;persistentvolumeclaims&quot;</span>]</span><br><span class="line">    <span class="attr">verbs:</span> [<span class="string">&quot;get&quot;</span>, <span class="string">&quot;list&quot;</span>, <span class="string">&quot;watch&quot;</span>, <span class="string">&quot;update&quot;</span>]</span><br><span class="line">  <span class="bullet">-</span> <span class="attr">apiGroups:</span> [<span class="string">&quot;&quot;</span>]</span><br><span class="line">    <span class="attr">resources:</span> [<span class="string">&quot;persistentvolumeclaims/status&quot;</span>]</span><br><span class="line">    <span class="attr">verbs:</span> [<span class="string">&quot;update&quot;</span>, <span class="string">&quot;patch&quot;</span>]</span><br><span class="line">  <span class="bullet">-</span> <span class="attr">apiGroups:</span> [<span class="string">&quot;storage.k8s.io&quot;</span>]</span><br><span class="line">    <span class="attr">resources:</span> [<span class="string">&quot;storageclasses&quot;</span>]</span><br><span class="line">    <span class="attr">verbs:</span> [<span class="string">&quot;get&quot;</span>, <span class="string">&quot;list&quot;</span>, <span class="string">&quot;watch&quot;</span>]</span><br><span class="line">  <span class="bullet">-</span> <span class="attr">apiGroups:</span> [<span class="string">&quot;snapshot.storage.k8s.io&quot;</span>]</span><br><span class="line">    <span class="attr">resources:</span> [<span class="string">&quot;volumesnapshots&quot;</span>]</span><br><span class="line">    <span class="attr">verbs:</span> [<span class="string">&quot;get&quot;</span>, <span class="string">&quot;list&quot;</span>]</span><br><span class="line">  <span class="bullet">-</span> <span class="attr">apiGroups:</span> [<span class="string">&quot;snapshot.storage.k8s.io&quot;</span>]</span><br><span class="line">    <span class="attr">resources:</span> [<span class="string">&quot;volumesnapshotcontents&quot;</span>]</span><br><span class="line">    <span class="attr">verbs:</span> [<span class="string">&quot;create&quot;</span>, <span class="string">&quot;get&quot;</span>, <span class="string">&quot;list&quot;</span>, <span class="string">&quot;watch&quot;</span>, <span class="string">&quot;update&quot;</span>, <span class="string">&quot;delete&quot;</span>]</span><br><span class="line">  <span class="bullet">-</span> <span class="attr">apiGroups:</span> [<span class="string">&quot;snapshot.storage.k8s.io&quot;</span>]</span><br><span class="line">    <span class="attr">resources:</span> [<span class="string">&quot;volumesnapshotclasses&quot;</span>]</span><br><span class="line">    <span class="attr">verbs:</span> [<span class="string">&quot;get&quot;</span>, <span class="string">&quot;list&quot;</span>, <span class="string">&quot;watch&quot;</span>]</span><br><span class="line">  <span class="bullet">-</span> <span class="attr">apiGroups:</span> [<span class="string">&quot;storage.k8s.io&quot;</span>]</span><br><span class="line">    <span class="attr">resources:</span> [<span class="string">&quot;volumeattachments&quot;</span>]</span><br><span class="line">    <span class="attr">verbs:</span> [<span class="string">&quot;get&quot;</span>, <span class="string">&quot;list&quot;</span>, <span class="string">&quot;watch&quot;</span>, <span class="string">&quot;update&quot;</span>, <span class="string">&quot;patch&quot;</span>]</span><br><span class="line">  <span class="bullet">-</span> <span class="attr">apiGroups:</span> [<span class="string">&quot;storage.k8s.io&quot;</span>]</span><br><span class="line">    <span class="attr">resources:</span> [<span class="string">&quot;csinodes&quot;</span>]</span><br><span class="line">    <span class="attr">verbs:</span> [<span class="string">&quot;get&quot;</span>, <span class="string">&quot;list&quot;</span>, <span class="string">&quot;watch&quot;</span>]</span><br><span class="line">  <span class="bullet">-</span> <span class="attr">apiGroups:</span> [<span class="string">&quot;snapshot.storage.k8s.io&quot;</span>]</span><br><span class="line">    <span class="attr">resources:</span> [<span class="string">&quot;volumesnapshotcontents/status&quot;</span>]</span><br><span class="line">    <span class="attr">verbs:</span> [<span class="string">&quot;update&quot;</span>]</span><br><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">ClusterRoleBinding</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">rbac.authorization.k8s.io/v1</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">rbd-csi-provisioner-role</span></span><br><span class="line"><span class="attr">subjects:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">kind:</span> <span class="string">ServiceAccount</span></span><br><span class="line">    <span class="attr">name:</span> <span class="string">rbd-csi-provisioner</span></span><br><span class="line">    <span class="attr">namespace:</span> <span class="string">default</span></span><br><span class="line"><span class="attr">roleRef:</span></span><br><span class="line">  <span class="attr">kind:</span> <span class="string">ClusterRole</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">rbd-external-provisioner-runner</span></span><br><span class="line">  <span class="attr">apiGroup:</span> <span class="string">rbac.authorization.k8s.io</span></span><br><span class="line"></span><br><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Role</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">rbac.authorization.k8s.io/v1</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="comment"># replace with non-default namespace name</span></span><br><span class="line">  <span class="attr">namespace:</span> <span class="string">default</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">rbd-external-provisioner-cfg</span></span><br><span class="line"><span class="attr">rules:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">apiGroups:</span> [<span class="string">&quot;&quot;</span>]</span><br><span class="line">    <span class="attr">resources:</span> [<span class="string">&quot;configmaps&quot;</span>]</span><br><span class="line">    <span class="attr">verbs:</span> [<span class="string">&quot;get&quot;</span>, <span class="string">&quot;list&quot;</span>, <span class="string">&quot;watch&quot;</span>, <span class="string">&quot;create&quot;</span>, <span class="string">&quot;delete&quot;</span>]</span><br><span class="line">  <span class="bullet">-</span> <span class="attr">apiGroups:</span> [<span class="string">&quot;coordination.k8s.io&quot;</span>]</span><br><span class="line">    <span class="attr">resources:</span> [<span class="string">&quot;leases&quot;</span>]</span><br><span class="line">    <span class="attr">verbs:</span> [<span class="string">&quot;get&quot;</span>, <span class="string">&quot;watch&quot;</span>, <span class="string">&quot;list&quot;</span>, <span class="string">&quot;delete&quot;</span>, <span class="string">&quot;update&quot;</span>, <span class="string">&quot;create&quot;</span>]</span><br><span class="line"></span><br><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">RoleBinding</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">rbac.authorization.k8s.io/v1</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">rbd-csi-provisioner-role-cfg</span></span><br><span class="line">  <span class="comment"># replace with non-default namespace name</span></span><br><span class="line">  <span class="attr">namespace:</span> <span class="string">default</span></span><br><span class="line"><span class="attr">subjects:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">kind:</span> <span class="string">ServiceAccount</span></span><br><span class="line">    <span class="attr">name:</span> <span class="string">rbd-csi-provisioner</span></span><br><span class="line">    <span class="comment"># replace with non-default namespace name</span></span><br><span class="line">    <span class="attr">namespace:</span> <span class="string">default</span></span><br><span class="line"><span class="attr">roleRef:</span></span><br><span class="line">  <span class="attr">kind:</span> <span class="string">Role</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">rbd-external-provisioner-cfg</span></span><br><span class="line">  <span class="attr">apiGroup:</span> <span class="string">rbac.authorization.k8s.io</span></span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> kubectl apply -f csi-nodeplugin-rbac.yaml</span></span><br></pre></td></tr></table></figure><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">$</span> <span class="string">cat</span> <span class="string">csi-nodeplugin-rbac.yaml</span></span><br><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">ServiceAccount</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">rbd-csi-nodeplugin</span></span><br><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">ClusterRole</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">rbac.authorization.k8s.io/v1</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">rbd-csi-nodeplugin</span></span><br><span class="line"><span class="attr">aggregationRule:</span></span><br><span class="line">  <span class="attr">clusterRoleSelectors:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">matchLabels:</span></span><br><span class="line">        <span class="attr">rbac.rbd.csi.ceph.com/aggregate-to-rbd-csi-nodeplugin:</span> <span class="string">&quot;true&quot;</span></span><br><span class="line"><span class="attr">rules:</span> []</span><br><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">ClusterRole</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">rbac.authorization.k8s.io/v1</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">rbd-csi-nodeplugin-rules</span></span><br><span class="line">  <span class="attr">labels:</span></span><br><span class="line">    <span class="attr">rbac.rbd.csi.ceph.com/aggregate-to-rbd-csi-nodeplugin:</span> <span class="string">&quot;true&quot;</span></span><br><span class="line"><span class="attr">rules:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">apiGroups:</span> [<span class="string">&quot;&quot;</span>]</span><br><span class="line">    <span class="attr">resources:</span> [<span class="string">&quot;nodes&quot;</span>]</span><br><span class="line">    <span class="attr">verbs:</span> [<span class="string">&quot;get&quot;</span>]</span><br><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">ClusterRoleBinding</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">rbac.authorization.k8s.io/v1</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">rbd-csi-nodeplugin</span></span><br><span class="line"><span class="attr">subjects:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">kind:</span> <span class="string">ServiceAccount</span></span><br><span class="line">    <span class="attr">name:</span> <span class="string">rbd-csi-nodeplugin</span></span><br><span class="line">    <span class="attr">namespace:</span> <span class="string">default</span></span><br><span class="line"><span class="attr">roleRef:</span></span><br><span class="line">  <span class="attr">kind:</span> <span class="string">ClusterRole</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">rbd-csi-nodeplugin</span></span><br><span class="line">  <span class="attr">apiGroup:</span> <span class="string">rbac.authorization.k8s.io</span></span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> kubectl apply -f csi-rbdplugin-provisioner.yaml</span></span><br></pre></td></tr></table></figure><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">$</span> <span class="string">cat</span> <span class="string">csi-rbdplugin-provisioner.yaml</span></span><br><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Service</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">csi-rbdplugin-provisioner</span></span><br><span class="line">  <span class="attr">labels:</span></span><br><span class="line">    <span class="attr">app:</span> <span class="string">csi-metrics</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">selector:</span></span><br><span class="line">    <span class="attr">app:</span> <span class="string">csi-rbdplugin-provisioner</span></span><br><span class="line">  <span class="attr">ports:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">http-metrics</span></span><br><span class="line">      <span class="attr">port:</span> <span class="number">8080</span></span><br><span class="line">      <span class="attr">protocol:</span> <span class="string">TCP</span></span><br><span class="line">      <span class="attr">targetPort:</span> <span class="number">8680</span></span><br><span class="line"></span><br><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Deployment</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">apps/v1</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">csi-rbdplugin-provisioner</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">replicas:</span> <span class="number">3</span></span><br><span class="line">  <span class="attr">selector:</span></span><br><span class="line">    <span class="attr">matchLabels:</span></span><br><span class="line">      <span class="attr">app:</span> <span class="string">csi-rbdplugin-provisioner</span></span><br><span class="line">  <span class="attr">template:</span></span><br><span class="line">    <span class="attr">metadata:</span></span><br><span class="line">      <span class="attr">labels:</span></span><br><span class="line">        <span class="attr">app:</span> <span class="string">csi-rbdplugin-provisioner</span></span><br><span class="line">    <span class="attr">spec:</span></span><br><span class="line">      <span class="attr">serviceAccount:</span> <span class="string">rbd-csi-provisioner</span></span><br><span class="line">      <span class="attr">containers:</span></span><br><span class="line">        <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">csi-provisioner</span></span><br><span class="line">          <span class="attr">image:</span> <span class="string">quay.io/k8scsi/csi-provisioner:v1.6.0</span></span><br><span class="line">          <span class="attr">args:</span></span><br><span class="line">            <span class="bullet">-</span> <span class="string">&quot;--csi-address=$(ADDRESS)&quot;</span></span><br><span class="line">            <span class="bullet">-</span> <span class="string">&quot;--v=5&quot;</span></span><br><span class="line">            <span class="bullet">-</span> <span class="string">&quot;--timeout=150s&quot;</span></span><br><span class="line">            <span class="bullet">-</span> <span class="string">&quot;--retry-interval-start=500ms&quot;</span></span><br><span class="line">            <span class="bullet">-</span> <span class="string">&quot;--enable-leader-election=true&quot;</span></span><br><span class="line">            <span class="bullet">-</span> <span class="string">&quot;--leader-election-type=leases&quot;</span></span><br><span class="line">            <span class="bullet">-</span> <span class="string">&quot;--feature-gates=Topology=true&quot;</span></span><br><span class="line">          <span class="attr">env:</span></span><br><span class="line">            <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">ADDRESS</span></span><br><span class="line">              <span class="attr">value:</span> <span class="string">unix:///csi/csi-provisioner.sock</span></span><br><span class="line">          <span class="attr">imagePullPolicy:</span> <span class="string">&quot;IfNotPresent&quot;</span></span><br><span class="line">          <span class="attr">volumeMounts:</span></span><br><span class="line">            <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">socket-dir</span></span><br><span class="line">              <span class="attr">mountPath:</span> <span class="string">/csi</span></span><br><span class="line">        <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">csi-snapshotter</span></span><br><span class="line">          <span class="attr">image:</span> <span class="string">quay.io/k8scsi/csi-snapshotter:v2.1.0</span></span><br><span class="line">          <span class="attr">args:</span></span><br><span class="line">            <span class="bullet">-</span> <span class="string">&quot;--csi-address=$(ADDRESS)&quot;</span></span><br><span class="line">            <span class="bullet">-</span> <span class="string">&quot;--v=5&quot;</span></span><br><span class="line">            <span class="bullet">-</span> <span class="string">&quot;--timeout=150s&quot;</span></span><br><span class="line">            <span class="bullet">-</span> <span class="string">&quot;--leader-election=true&quot;</span></span><br><span class="line">          <span class="attr">env:</span></span><br><span class="line">            <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">ADDRESS</span></span><br><span class="line">              <span class="attr">value:</span> <span class="string">unix:///csi/csi-provisioner.sock</span></span><br><span class="line">          <span class="attr">imagePullPolicy:</span> <span class="string">&quot;IfNotPresent&quot;</span></span><br><span class="line">          <span class="attr">securityContext:</span></span><br><span class="line">            <span class="attr">privileged:</span> <span class="literal">true</span></span><br><span class="line">          <span class="attr">volumeMounts:</span></span><br><span class="line">            <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">socket-dir</span></span><br><span class="line">              <span class="attr">mountPath:</span> <span class="string">/csi</span></span><br><span class="line">        <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">csi-attacher</span></span><br><span class="line">          <span class="attr">image:</span> <span class="string">quay.io/k8scsi/csi-attacher:v2.1.1</span></span><br><span class="line">          <span class="attr">args:</span></span><br><span class="line">            <span class="bullet">-</span> <span class="string">&quot;--v=5&quot;</span></span><br><span class="line">            <span class="bullet">-</span> <span class="string">&quot;--csi-address=$(ADDRESS)&quot;</span></span><br><span class="line">            <span class="bullet">-</span> <span class="string">&quot;--leader-election=true&quot;</span></span><br><span class="line">            <span class="bullet">-</span> <span class="string">&quot;--retry-interval-start=500ms&quot;</span></span><br><span class="line">          <span class="attr">env:</span></span><br><span class="line">            <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">ADDRESS</span></span><br><span class="line">              <span class="attr">value:</span> <span class="string">/csi/csi-provisioner.sock</span></span><br><span class="line">          <span class="attr">imagePullPolicy:</span> <span class="string">&quot;IfNotPresent&quot;</span></span><br><span class="line">          <span class="attr">volumeMounts:</span></span><br><span class="line">            <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">socket-dir</span></span><br><span class="line">              <span class="attr">mountPath:</span> <span class="string">/csi</span></span><br><span class="line">        <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">csi-resizer</span></span><br><span class="line">          <span class="attr">image:</span> <span class="string">quay.io/k8scsi/csi-resizer:v0.5.0</span></span><br><span class="line">          <span class="attr">args:</span></span><br><span class="line">            <span class="bullet">-</span> <span class="string">&quot;--csi-address=$(ADDRESS)&quot;</span></span><br><span class="line">            <span class="bullet">-</span> <span class="string">&quot;--v=5&quot;</span></span><br><span class="line">            <span class="bullet">-</span> <span class="string">&quot;--csiTimeout=150s&quot;</span></span><br><span class="line">            <span class="bullet">-</span> <span class="string">&quot;--leader-election&quot;</span></span><br><span class="line">            <span class="bullet">-</span> <span class="string">&quot;--retry-interval-start=500ms&quot;</span></span><br><span class="line">          <span class="attr">env:</span></span><br><span class="line">            <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">ADDRESS</span></span><br><span class="line">              <span class="attr">value:</span> <span class="string">unix:///csi/csi-provisioner.sock</span></span><br><span class="line">          <span class="attr">imagePullPolicy:</span> <span class="string">&quot;IfNotPresent&quot;</span></span><br><span class="line">          <span class="attr">volumeMounts:</span></span><br><span class="line">            <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">socket-dir</span></span><br><span class="line">              <span class="attr">mountPath:</span> <span class="string">/csi</span></span><br><span class="line">        <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">csi-rbdplugin</span></span><br><span class="line">          <span class="attr">securityContext:</span></span><br><span class="line">            <span class="attr">privileged:</span> <span class="literal">true</span></span><br><span class="line">            <span class="attr">capabilities:</span></span><br><span class="line">              <span class="attr">add:</span> [<span class="string">&quot;SYS_ADMIN&quot;</span>]</span><br><span class="line">          <span class="comment"># for stable functionality replace canary with latest release version</span></span><br><span class="line">          <span class="attr">image:</span> <span class="string">quay.io/cephcsi/cephcsi:canary</span></span><br><span class="line">          <span class="attr">args:</span></span><br><span class="line">            <span class="bullet">-</span> <span class="string">&quot;--nodeid=$(NODE_ID)&quot;</span></span><br><span class="line">            <span class="bullet">-</span> <span class="string">&quot;--type=rbd&quot;</span></span><br><span class="line">            <span class="bullet">-</span> <span class="string">&quot;--controllerserver=true&quot;</span></span><br><span class="line">            <span class="bullet">-</span> <span class="string">&quot;--endpoint=$(CSI_ENDPOINT)&quot;</span></span><br><span class="line">            <span class="bullet">-</span> <span class="string">&quot;--v=5&quot;</span></span><br><span class="line">            <span class="bullet">-</span> <span class="string">&quot;--drivername=rbd.csi.ceph.com&quot;</span></span><br><span class="line">            <span class="bullet">-</span> <span class="string">&quot;--pidlimit=-1&quot;</span></span><br><span class="line">            <span class="bullet">-</span> <span class="string">&quot;--rbdhardmaxclonedepth=8&quot;</span></span><br><span class="line">            <span class="bullet">-</span> <span class="string">&quot;--rbdsoftmaxclonedepth=4&quot;</span></span><br><span class="line">          <span class="attr">env:</span></span><br><span class="line">            <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">POD_IP</span></span><br><span class="line">              <span class="attr">valueFrom:</span></span><br><span class="line">                <span class="attr">fieldRef:</span></span><br><span class="line">                  <span class="attr">fieldPath:</span> <span class="string">status.podIP</span></span><br><span class="line">            <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">NODE_ID</span></span><br><span class="line">              <span class="attr">valueFrom:</span></span><br><span class="line">                <span class="attr">fieldRef:</span></span><br><span class="line">                  <span class="attr">fieldPath:</span> <span class="string">spec.nodeName</span></span><br><span class="line">            <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">CSI_ENDPOINT</span></span><br><span class="line">              <span class="attr">value:</span> <span class="string">unix:///csi/csi-provisioner.sock</span></span><br><span class="line">          <span class="attr">imagePullPolicy:</span> <span class="string">&quot;IfNotPresent&quot;</span></span><br><span class="line">          <span class="attr">volumeMounts:</span></span><br><span class="line">            <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">socket-dir</span></span><br><span class="line">              <span class="attr">mountPath:</span> <span class="string">/csi</span></span><br><span class="line">            <span class="bullet">-</span> <span class="attr">mountPath:</span> <span class="string">/dev</span></span><br><span class="line">              <span class="attr">name:</span> <span class="string">host-dev</span></span><br><span class="line">            <span class="bullet">-</span> <span class="attr">mountPath:</span> <span class="string">/sys</span></span><br><span class="line">              <span class="attr">name:</span> <span class="string">host-sys</span></span><br><span class="line">            <span class="bullet">-</span> <span class="attr">mountPath:</span> <span class="string">/lib/modules</span></span><br><span class="line">              <span class="attr">name:</span> <span class="string">lib-modules</span></span><br><span class="line">              <span class="attr">readOnly:</span> <span class="literal">true</span></span><br><span class="line">            <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">ceph-csi-config</span></span><br><span class="line">              <span class="attr">mountPath:</span> <span class="string">/etc/ceph-csi-config/</span></span><br><span class="line">           <span class="comment"># - name: ceph-csi-encryption-kms-config</span></span><br><span class="line">           <span class="comment">#   mountPath: /etc/ceph-csi-encryption-kms-config/</span></span><br><span class="line">            <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">keys-tmp-dir</span></span><br><span class="line">              <span class="attr">mountPath:</span> <span class="string">/tmp/csi/keys</span></span><br><span class="line">        <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">liveness-prometheus</span></span><br><span class="line">          <span class="attr">image:</span> <span class="string">quay.io/cephcsi/cephcsi:canary</span></span><br><span class="line">          <span class="attr">args:</span></span><br><span class="line">            <span class="bullet">-</span> <span class="string">&quot;--type=liveness&quot;</span></span><br><span class="line">            <span class="bullet">-</span> <span class="string">&quot;--endpoint=$(CSI_ENDPOINT)&quot;</span></span><br><span class="line">            <span class="bullet">-</span> <span class="string">&quot;--metricsport=8680&quot;</span></span><br><span class="line">            <span class="bullet">-</span> <span class="string">&quot;--metricspath=/metrics&quot;</span></span><br><span class="line">            <span class="bullet">-</span> <span class="string">&quot;--polltime=60s&quot;</span></span><br><span class="line">            <span class="bullet">-</span> <span class="string">&quot;--timeout=3s&quot;</span></span><br><span class="line">          <span class="attr">env:</span></span><br><span class="line">            <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">CSI_ENDPOINT</span></span><br><span class="line">              <span class="attr">value:</span> <span class="string">unix:///csi/csi-provisioner.sock</span></span><br><span class="line">            <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">POD_IP</span></span><br><span class="line">              <span class="attr">valueFrom:</span></span><br><span class="line">                <span class="attr">fieldRef:</span></span><br><span class="line">                  <span class="attr">fieldPath:</span> <span class="string">status.podIP</span></span><br><span class="line">          <span class="attr">volumeMounts:</span></span><br><span class="line">            <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">socket-dir</span></span><br><span class="line">              <span class="attr">mountPath:</span> <span class="string">/csi</span></span><br><span class="line">          <span class="attr">imagePullPolicy:</span> <span class="string">&quot;IfNotPresent&quot;</span></span><br><span class="line">      <span class="attr">volumes:</span></span><br><span class="line">        <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">host-dev</span></span><br><span class="line">          <span class="attr">hostPath:</span></span><br><span class="line">            <span class="attr">path:</span> <span class="string">/dev</span></span><br><span class="line">        <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">host-sys</span></span><br><span class="line">          <span class="attr">hostPath:</span></span><br><span class="line">            <span class="attr">path:</span> <span class="string">/sys</span></span><br><span class="line">        <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">lib-modules</span></span><br><span class="line">          <span class="attr">hostPath:</span></span><br><span class="line">            <span class="attr">path:</span> <span class="string">/lib/modules</span></span><br><span class="line">        <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">socket-dir</span></span><br><span class="line">          <span class="attr">emptyDir:</span> &#123;</span><br><span class="line">            <span class="attr">medium:</span> <span class="string">&quot;Memory&quot;</span></span><br><span class="line">          &#125;</span><br><span class="line">        <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">ceph-csi-config</span></span><br><span class="line">          <span class="attr">configMap:</span></span><br><span class="line">            <span class="attr">name:</span> <span class="string">ceph-csi-config</span></span><br><span class="line">        <span class="comment">#- name: ceph-csi-encryption-kms-config</span></span><br><span class="line">        <span class="comment">#  configMap:</span></span><br><span class="line">        <span class="comment">#    name: ceph-csi-encryption-kms-config</span></span><br><span class="line">        <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">keys-tmp-dir</span></span><br><span class="line">          <span class="attr">emptyDir:</span> &#123;</span><br><span class="line">            <span class="attr">medium:</span> <span class="string">&quot;Memory&quot;</span></span><br><span class="line">          &#125;</span><br></pre></td></tr></table></figure><p>上面yaml文件中注释的部分是之前测试报错没有找到cm，官方文档没有创建此文件，这里注释掉无影响。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> kubectl  apply  -f  csi-rbdplugin.yaml</span></span><br></pre></td></tr></table></figure><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">$</span> <span class="string">cat</span> <span class="string">csi-rbdplugin.yaml</span></span><br><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">DaemonSet</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">apps/v1</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">csi-rbdplugin</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">selector:</span></span><br><span class="line">    <span class="attr">matchLabels:</span></span><br><span class="line">      <span class="attr">app:</span> <span class="string">csi-rbdplugin</span></span><br><span class="line">  <span class="attr">template:</span></span><br><span class="line">    <span class="attr">metadata:</span></span><br><span class="line">      <span class="attr">labels:</span></span><br><span class="line">        <span class="attr">app:</span> <span class="string">csi-rbdplugin</span></span><br><span class="line">    <span class="attr">spec:</span></span><br><span class="line">      <span class="attr">serviceAccount:</span> <span class="string">rbd-csi-nodeplugin</span></span><br><span class="line">      <span class="attr">hostNetwork:</span> <span class="literal">true</span></span><br><span class="line">      <span class="attr">hostPID:</span> <span class="literal">true</span></span><br><span class="line">      <span class="comment"># to use e.g. Rook orchestrated cluster, and mons&#x27; FQDN is</span></span><br><span class="line">      <span class="comment"># resolved through k8s service, set dns policy to cluster first</span></span><br><span class="line">      <span class="attr">dnsPolicy:</span> <span class="string">ClusterFirstWithHostNet</span></span><br><span class="line">      <span class="attr">containers:</span></span><br><span class="line">        <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">driver-registrar</span></span><br><span class="line">          <span class="comment"># This is necessary only for systems with SELinux, where</span></span><br><span class="line">          <span class="comment"># non-privileged sidecar containers cannot access unix domain socket</span></span><br><span class="line">          <span class="comment"># created by privileged CSI driver container.</span></span><br><span class="line">          <span class="attr">securityContext:</span></span><br><span class="line">            <span class="attr">privileged:</span> <span class="literal">true</span></span><br><span class="line">          <span class="attr">image:</span> <span class="string">quay.io/k8scsi/csi-node-driver-registrar:v1.3.0</span></span><br><span class="line">          <span class="attr">args:</span></span><br><span class="line">            <span class="bullet">-</span> <span class="string">&quot;--v=5&quot;</span></span><br><span class="line">            <span class="bullet">-</span> <span class="string">&quot;--csi-address=/csi/csi.sock&quot;</span></span><br><span class="line">            <span class="bullet">-</span> <span class="string">&quot;--kubelet-registration-path=/data/kubelet/plugins/rbd.csi.ceph.com/csi.sock&quot;</span></span><br><span class="line">          <span class="attr">lifecycle:</span></span><br><span class="line">            <span class="attr">preStop:</span></span><br><span class="line">              <span class="attr">exec:</span></span><br><span class="line">                <span class="attr">command:</span> [</span><br><span class="line">                  <span class="string">&quot;/bin/sh&quot;</span>, <span class="string">&quot;-c&quot;</span>,</span><br><span class="line">                  <span class="string">&quot;rm -rf /registration/rbd.csi.ceph.com \</span></span><br><span class="line"><span class="string">                  /registration/rbd.csi.ceph.com-reg.sock&quot;</span></span><br><span class="line">                ]</span><br><span class="line">          <span class="attr">env:</span></span><br><span class="line">            <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">KUBE_NODE_NAME</span></span><br><span class="line">              <span class="attr">valueFrom:</span></span><br><span class="line">                <span class="attr">fieldRef:</span></span><br><span class="line">                  <span class="attr">fieldPath:</span> <span class="string">spec.nodeName</span></span><br><span class="line">          <span class="attr">volumeMounts:</span></span><br><span class="line">            <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">socket-dir</span></span><br><span class="line">              <span class="attr">mountPath:</span> <span class="string">/csi</span></span><br><span class="line">            <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">registration-dir</span></span><br><span class="line">              <span class="attr">mountPath:</span> <span class="string">/registration</span></span><br><span class="line">        <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">csi-rbdplugin</span></span><br><span class="line">          <span class="attr">securityContext:</span></span><br><span class="line">            <span class="attr">privileged:</span> <span class="literal">true</span></span><br><span class="line">            <span class="attr">capabilities:</span></span><br><span class="line">              <span class="attr">add:</span> [<span class="string">&quot;SYS_ADMIN&quot;</span>]</span><br><span class="line">            <span class="attr">allowPrivilegeEscalation:</span> <span class="literal">true</span></span><br><span class="line">          <span class="comment"># for stable functionality replace canary with latest release version</span></span><br><span class="line">          <span class="attr">image:</span> <span class="string">quay.io/cephcsi/cephcsi:canary</span></span><br><span class="line">          <span class="attr">args:</span></span><br><span class="line">            <span class="bullet">-</span> <span class="string">&quot;--nodeid=$(NODE_ID)&quot;</span></span><br><span class="line">            <span class="bullet">-</span> <span class="string">&quot;--type=rbd&quot;</span></span><br><span class="line">            <span class="bullet">-</span> <span class="string">&quot;--nodeserver=true&quot;</span></span><br><span class="line">            <span class="bullet">-</span> <span class="string">&quot;--endpoint=$(CSI_ENDPOINT)&quot;</span></span><br><span class="line">            <span class="bullet">-</span> <span class="string">&quot;--v=5&quot;</span></span><br><span class="line">            <span class="bullet">-</span> <span class="string">&quot;--drivername=rbd.csi.ceph.com&quot;</span></span><br><span class="line">            <span class="comment"># If topology based provisioning is desired, configure required</span></span><br><span class="line">            <span class="comment"># node labels representing the nodes topology domain</span></span><br><span class="line">            <span class="comment"># and pass the label names below, for CSI to consume and advertize</span></span><br><span class="line">            <span class="comment"># its equivalent topology domain</span></span><br><span class="line">            <span class="comment"># - &quot;--domainlabels=failure-domain/region,failure-domain/zone&quot;</span></span><br><span class="line">          <span class="attr">env:</span></span><br><span class="line">            <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">POD_IP</span></span><br><span class="line">              <span class="attr">valueFrom:</span></span><br><span class="line">                <span class="attr">fieldRef:</span></span><br><span class="line">                  <span class="attr">fieldPath:</span> <span class="string">status.podIP</span></span><br><span class="line">            <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">NODE_ID</span></span><br><span class="line">              <span class="attr">valueFrom:</span></span><br><span class="line">                <span class="attr">fieldRef:</span></span><br><span class="line">                  <span class="attr">fieldPath:</span> <span class="string">spec.nodeName</span></span><br><span class="line">            <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">CSI_ENDPOINT</span></span><br><span class="line">              <span class="attr">value:</span> <span class="string">unix:///csi/csi.sock</span></span><br><span class="line">          <span class="attr">imagePullPolicy:</span> <span class="string">&quot;IfNotPresent&quot;</span></span><br><span class="line">          <span class="attr">volumeMounts:</span></span><br><span class="line">            <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">socket-dir</span></span><br><span class="line">              <span class="attr">mountPath:</span> <span class="string">/csi</span></span><br><span class="line">            <span class="bullet">-</span> <span class="attr">mountPath:</span> <span class="string">/dev</span></span><br><span class="line">              <span class="attr">name:</span> <span class="string">host-dev</span></span><br><span class="line">            <span class="bullet">-</span> <span class="attr">mountPath:</span> <span class="string">/sys</span></span><br><span class="line">              <span class="attr">name:</span> <span class="string">host-sys</span></span><br><span class="line">            <span class="bullet">-</span> <span class="attr">mountPath:</span> <span class="string">/run/mount</span></span><br><span class="line">              <span class="attr">name:</span> <span class="string">host-mount</span></span><br><span class="line">            <span class="bullet">-</span> <span class="attr">mountPath:</span> <span class="string">/lib/modules</span></span><br><span class="line">              <span class="attr">name:</span> <span class="string">lib-modules</span></span><br><span class="line">              <span class="attr">readOnly:</span> <span class="literal">true</span></span><br><span class="line">            <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">ceph-csi-config</span></span><br><span class="line">              <span class="attr">mountPath:</span> <span class="string">/etc/ceph-csi-config/</span></span><br><span class="line">           <span class="comment"># - name: ceph-csi-encryption-kms-config</span></span><br><span class="line">           <span class="comment">#   mountPath: /etc/ceph-csi-encryption-kms-config/</span></span><br><span class="line">            <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">plugin-dir</span></span><br><span class="line">              <span class="attr">mountPath:</span> <span class="string">/data/kubelet/plugins</span></span><br><span class="line">              <span class="attr">mountPropagation:</span> <span class="string">&quot;Bidirectional&quot;</span></span><br><span class="line">            <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">mountpoint-dir</span></span><br><span class="line">              <span class="attr">mountPath:</span> <span class="string">/data/kubelet/pods</span></span><br><span class="line">              <span class="attr">mountPropagation:</span> <span class="string">&quot;Bidirectional&quot;</span></span><br><span class="line">            <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">keys-tmp-dir</span></span><br><span class="line">              <span class="attr">mountPath:</span> <span class="string">/tmp/csi/keys</span></span><br><span class="line">        <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">liveness-prometheus</span></span><br><span class="line">          <span class="attr">securityContext:</span></span><br><span class="line">            <span class="attr">privileged:</span> <span class="literal">true</span></span><br><span class="line">          <span class="attr">image:</span> <span class="string">quay.io/cephcsi/cephcsi:canary</span></span><br><span class="line">          <span class="attr">args:</span></span><br><span class="line">            <span class="bullet">-</span> <span class="string">&quot;--type=liveness&quot;</span></span><br><span class="line">            <span class="bullet">-</span> <span class="string">&quot;--endpoint=$(CSI_ENDPOINT)&quot;</span></span><br><span class="line">            <span class="bullet">-</span> <span class="string">&quot;--metricsport=8680&quot;</span></span><br><span class="line">            <span class="bullet">-</span> <span class="string">&quot;--metricspath=/metrics&quot;</span></span><br><span class="line">            <span class="bullet">-</span> <span class="string">&quot;--polltime=60s&quot;</span></span><br><span class="line">            <span class="bullet">-</span> <span class="string">&quot;--timeout=3s&quot;</span></span><br><span class="line">          <span class="attr">env:</span></span><br><span class="line">            <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">CSI_ENDPOINT</span></span><br><span class="line">              <span class="attr">value:</span> <span class="string">unix:///csi/csi.sock</span></span><br><span class="line">            <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">POD_IP</span></span><br><span class="line">              <span class="attr">valueFrom:</span></span><br><span class="line">                <span class="attr">fieldRef:</span></span><br><span class="line">                  <span class="attr">fieldPath:</span> <span class="string">status.podIP</span></span><br><span class="line">          <span class="attr">volumeMounts:</span></span><br><span class="line">            <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">socket-dir</span></span><br><span class="line">              <span class="attr">mountPath:</span> <span class="string">/csi</span></span><br><span class="line">          <span class="attr">imagePullPolicy:</span> <span class="string">&quot;IfNotPresent&quot;</span></span><br><span class="line">      <span class="attr">volumes:</span></span><br><span class="line">        <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">socket-dir</span></span><br><span class="line">          <span class="attr">hostPath:</span></span><br><span class="line">            <span class="attr">path:</span> <span class="string">/data/kubelet/plugins/rbd.csi.ceph.com</span></span><br><span class="line">            <span class="attr">type:</span> <span class="string">DirectoryOrCreate</span></span><br><span class="line">        <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">plugin-dir</span></span><br><span class="line">          <span class="attr">hostPath:</span></span><br><span class="line">            <span class="attr">path:</span> <span class="string">/data/kubelet/plugins</span></span><br><span class="line">            <span class="attr">type:</span> <span class="string">Directory</span></span><br><span class="line">        <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">mountpoint-dir</span></span><br><span class="line">          <span class="attr">hostPath:</span></span><br><span class="line">            <span class="attr">path:</span> <span class="string">/data/kubelet/pods</span></span><br><span class="line">            <span class="attr">type:</span> <span class="string">DirectoryOrCreate</span></span><br><span class="line">        <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">registration-dir</span></span><br><span class="line">          <span class="attr">hostPath:</span></span><br><span class="line">            <span class="attr">path:</span> <span class="string">/data/kubelet/plugins_registry/</span></span><br><span class="line">            <span class="attr">type:</span> <span class="string">Directory</span></span><br><span class="line">        <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">host-dev</span></span><br><span class="line">          <span class="attr">hostPath:</span></span><br><span class="line">            <span class="attr">path:</span> <span class="string">/dev</span></span><br><span class="line">        <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">host-sys</span></span><br><span class="line">          <span class="attr">hostPath:</span></span><br><span class="line">            <span class="attr">path:</span> <span class="string">/sys</span></span><br><span class="line">        <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">host-mount</span></span><br><span class="line">          <span class="attr">hostPath:</span></span><br><span class="line">            <span class="attr">path:</span> <span class="string">/run/mount</span></span><br><span class="line">        <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">lib-modules</span></span><br><span class="line">          <span class="attr">hostPath:</span></span><br><span class="line">            <span class="attr">path:</span> <span class="string">/lib/modules</span></span><br><span class="line">        <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">ceph-csi-config</span></span><br><span class="line">          <span class="attr">configMap:</span></span><br><span class="line">            <span class="attr">name:</span> <span class="string">ceph-csi-config</span></span><br><span class="line">        <span class="comment">#- name: ceph-csi-encryption-kms-config</span></span><br><span class="line">        <span class="comment">#  configMap:</span></span><br><span class="line">        <span class="comment">#    name: ceph-csi-encryption-kms-config</span></span><br><span class="line">        <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">keys-tmp-dir</span></span><br><span class="line">          <span class="attr">emptyDir:</span> &#123;</span><br><span class="line">            <span class="attr">medium:</span> <span class="string">&quot;Memory&quot;</span></span><br><span class="line">          &#125;</span><br><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="comment"># This is a service to expose the liveness metrics</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Service</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">csi-metrics-rbdplugin</span></span><br><span class="line">  <span class="attr">labels:</span></span><br><span class="line">    <span class="attr">app:</span> <span class="string">csi-metrics</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">ports:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">http-metrics</span></span><br><span class="line">      <span class="attr">port:</span> <span class="number">8080</span></span><br><span class="line">      <span class="attr">protocol:</span> <span class="string">TCP</span></span><br><span class="line">      <span class="attr">targetPort:</span> <span class="number">8680</span></span><br><span class="line">  <span class="attr">selector:</span></span><br><span class="line">    <span class="attr">app:</span> <span class="string">csi-rbdplugin</span></span><br></pre></td></tr></table></figure><p>查看容器是否正常运行。</p><p><img src="/doc_picture/ceph-6.png" alt="image-20210802123641214"></p><h2 id="6-使用ceph块设备"><a href="#6-使用ceph块设备" class="headerlink" title="6.    使用ceph块设备"></a>6.    使用ceph块设备</h2><h3 id="创建storageclass"><a href="#创建storageclass" class="headerlink" title="创建storageclass"></a>创建storageclass</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> kubectl  apply  -f  csi-rbd-sc-filesystem.yaml</span></span><br></pre></td></tr></table></figure><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">$</span> <span class="string">cat</span> <span class="string">csi-rbd-sc-filesystem.yaml</span></span><br><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">storage.k8s.io/v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">StorageClass</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">   <span class="attr">name:</span> <span class="string">csi-rbd-sc-filesystem</span></span><br><span class="line"><span class="attr">provisioner:</span> <span class="string">rbd.csi.ceph.com</span></span><br><span class="line"><span class="attr">parameters:</span></span><br><span class="line">   <span class="attr">clusterID:</span> <span class="string">10594fb3-68f3-4c97-8e0b-df80ba2a6745</span></span><br><span class="line">   <span class="attr">imageFeatures:</span> <span class="string">layering</span></span><br><span class="line">   <span class="attr">pool:</span> <span class="string">kubernetes</span></span><br><span class="line">   <span class="attr">csi.storage.k8s.io/provisioner-secret-name:</span> <span class="string">csi-rbd-secret</span></span><br><span class="line">   <span class="attr">csi.storage.k8s.io/provisioner-secret-namespace:</span> <span class="string">default</span></span><br><span class="line">   <span class="attr">csi.storage.k8s.io/node-stage-secret-name:</span> <span class="string">csi-rbd-secret</span></span><br><span class="line">   <span class="attr">csi.storage.k8s.io/node-stage-secret-namespace:</span> <span class="string">default</span></span><br><span class="line">   <span class="attr">csi.storage.k8s.io/controller-expand-secret-name:</span> <span class="string">csi-rbd-secret</span></span><br><span class="line">   <span class="attr">csi.storage.k8s.io/controller-expand-secret-namespace:</span> <span class="string">default</span></span><br><span class="line">   <span class="attr">csi.storage.k8s.io/fstype:</span> <span class="string">ext4</span></span><br><span class="line"><span class="attr">reclaimPolicy:</span> <span class="string">Delete</span></span><br><span class="line"><span class="attr">allowVolumeExpansion:</span> <span class="literal">true</span></span><br><span class="line"><span class="attr">mountOptions:</span></span><br><span class="line">   <span class="bullet">-</span> <span class="string">discard</span></span><br></pre></td></tr></table></figure><p>其中：</p><p> <code>csi.storage.k8s.io/controller-expand-secret-name: csi-rbd-secret</code></p><p><code>csi.storage.k8s.io/controller-expand-secret-namespace: default</code></p><p><code>allowVolumeExpansion: true</code></p><p>以上三个参数都是在ceph-csi支持动态扩容时需要具备的参数</p><h3 id="创建pvc"><a href="#创建pvc" class="headerlink" title="创建pvc"></a>创建pvc</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> kubectl  apply  -f  filesystem-pvc2.yaml</span></span><br></pre></td></tr></table></figure><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">$</span> <span class="string">cat</span> <span class="string">filesystem-pvc2.yaml</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">PersistentVolumeClaim</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">filesystem-pvc-2</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">accessModes:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">ReadWriteOnce</span></span><br><span class="line">  <span class="attr">volumeMode:</span> <span class="string">Filesystem</span></span><br><span class="line">  <span class="attr">resources:</span></span><br><span class="line">    <span class="attr">requests:</span></span><br><span class="line">      <span class="attr">storage:</span> <span class="string">200Mi</span></span><br><span class="line">  <span class="attr">storageClassName:</span> <span class="string">csi-rbd-sc-filesystem</span></span><br></pre></td></tr></table></figure><blockquote><p>这里除了filesystem还可以设置成block，区别就是文件系统是直接挂在文件夹下，block相当于是裸设备，有些场景服务为了性能会直接操作裸磁盘就可以用到了。</p></blockquote><p>这个时候查看卷就创建出来了。</p><p><img src="/doc_picture/ceph-7.png" alt="image-20210802124356580"></p><h2 id="存储盘扩容测试"><a href="#存储盘扩容测试" class="headerlink" title="存储盘扩容测试"></a>存储盘扩容测试</h2><p>以上面创建出来的200Mi的filesystem-pvc-2卷举例，查看pvc与pv都是200M。</p><p><img src="/doc_picture/ceph-8.png" alt="image-20210802124511152"></p><p><img src="/doc_picture/ceph-9.png" alt="image-20210802124518604"></p><p>在线修改pvc中的大小，200M修改为700M，保存退出。</p><p><img src="/doc_picture/ceph-10.png" alt="image-20210802124539312"></p><p>查看此时的pvc与pv状态</p><p><img src="/doc_picture/ceph-11.png" alt="image-20210802124656194"></p><p><img src="/doc_picture/ceph-12.png" alt="image-20210802124713132"></p><p>会发现pv已经变为了700M,pvc没有改变，查看pvc的详细信息。</p><p><img src="/doc_picture/ceph-13.png" alt="image-20210802124732583"></p><p>状态栏中已经说的很清楚了，重启文件系统就可以生效了（客户端）。</p><h2 id="扩容缺陷"><a href="#扩容缺陷" class="headerlink" title="扩容缺陷"></a>扩容缺陷</h2><p>原先的卷空间如果扩容到1G及以下会按照实际申请大小来创建，如果申请扩容大小超出1G会自动以GB为单位补全，如下例子：</p><p>将之前创建的filesystem-pvc-2扩容到1.2G</p><p><img src="/doc_picture/ceph-14.png" alt="image-20210802133810652"></p><p>保存退出，查看pv的大小，补为了2G</p><p><img src="/doc_picture/ceph-15.png" alt="image-20210802133825273"></p><p>查看ceph端的rbd大小也是2G</p><p><img src="/doc_picture/ceph-16.png" alt="image-20210802133841028"></p><h1 id="参考文档"><a href="#参考文档" class="headerlink" title="参考文档"></a>参考文档</h1><p>Kubernetes存储介绍系列 ——CSI plugin设计：<a href="http://newto.me/k8s-csi-design/">http://newto.me/k8s-csi-design/</a></p><p>Kubernetes 兼容 CSI 做的工作： <a href="https://www.kubernetes.org.cn/4618.html">https://www.kubernetes.org.cn/4618.html</a></p><p>kubernetes部署csi:     <a href="#using-ceph-block-devices">https://docs.ceph.com/docs/master/rbd/rbd-kubernetes/#using-ceph-block-devices</a></p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;最近测试ceph rbd在kubernetes的自动扩容问题，之前K8s v1.11.0时的策略是先找到目标卷，使用rbd resize命令对此卷扩容，找到挂载此卷的客户端宿主机，执行xfs_growfs等刷新文件系统的命令。查看网上资料k8s 在1.15版本后，Expan</summary>
      
    
    
    
    <category term="k8s存储" scheme="https://slions.github.io/categories/k8s%E5%AD%98%E5%82%A8/"/>
    
    
    <category term="kubernetes storage" scheme="https://slions.github.io/tags/kubernetes-storage/"/>
    
    <category term="ceph" scheme="https://slions.github.io/tags/ceph/"/>
    
  </entry>
  
  <entry>
    <title>创建开机自启程序（下篇）</title>
    <link href="https://slions.github.io/2021/08/01/%E5%88%9B%E5%BB%BA%E5%BC%80%E6%9C%BA%E8%87%AA%E5%90%AF%E7%A8%8B%E5%BA%8F%EF%BC%88%E4%B8%8B%E7%AF%87%EF%BC%89/"/>
    <id>https://slions.github.io/2021/08/01/%E5%88%9B%E5%BB%BA%E5%BC%80%E6%9C%BA%E8%87%AA%E5%90%AF%E7%A8%8B%E5%BA%8F%EF%BC%88%E4%B8%8B%E7%AF%87%EF%BC%89/</id>
    <published>2021-08-01T07:27:11.000Z</published>
    <updated>2021-08-12T11:50:06.494Z</updated>
    
    <content type="html"><![CDATA[<a href="/2021/08/01/%E5%88%9B%E5%BB%BA%E5%BC%80%E6%9C%BA%E8%87%AA%E5%90%AF%E7%A8%8B%E5%BA%8F%EF%BC%88%E4%B8%8A%E7%AF%87%EF%BC%89/" title="上篇">上篇</a>讲述了通过rc.local可以实现程序的开机自启，这篇说说centOS7以后的推荐方式`systemd service`。<h1 id="systemd-service"><a href="#systemd-service" class="headerlink" title="systemd service"></a>systemd service</h1><p>systemd Service是systemd提供的用于管理服务启动、停止和相关操作的功能，它极大的简化了服务管理的配置过程，用户只需要配置几项指令即可。</p><p>systemd service是systemd所管理的其中一项内容。实际上，systemd service是Systemd Unit的一种，除了Service，systemd还有其他几种类型的unit，比如service、socket、slice、scope、target等等。在这里，暂时了解两项内容：</p><ul><li><p>Service类型，定义服务程序的启动、停止、重启等操作和进程相关属性</p></li><li><p>Target类型，主要目的是对Service(也可以是其它Unit)进行分组、归类，可以包含一个或多个Service Unit(也可以是其它Unit)</p></li></ul><blockquote><p>systemd管理服务的一些亮点：</p><ol><li>用户可以直接在Service配置文件中定义CGroup相关指令来对该服务程序做资源限制。</li><li>用户可以选择Journal日志而非采用rsyslog，这意味着用户可以不用单独去配置rsyslog，而且可以直接通过systemctl或journalctl命令来查看某服务的日志信息。当然，该功能并不适用于所有情况，比如用户需要管理日志时</li><li>Systemd Service还有其它一些特性，比如可以动态修改服务管理配置文件，比如可以并行启动非依赖的服务，从而加速开机过程等等。</li></ol></blockquote><h1 id="systemd服务配置文件存放路径"><a href="#systemd服务配置文件存放路径" class="headerlink" title="systemd服务配置文件存放路径"></a>systemd服务配置文件存放路径</h1><p>systemd 默认从目录<code>/etc/systemd/system/</code>读取配置文件。里面存放的大部分文件都是符号链接，真正的配置文件存放在<code>/usr/lib/systemd/system/</code>，如果用户需要，可以将服务配置文件手动存放至用户配置目录<code>/etc/systemd/system</code>下。该目录下的服务配置文件可以是普通.service文件，也可以是链接至<code>/usr/lib/systemd/system</code>目录下服务配置文件的软链接。</p><p>位于<code>/usr/lib/systemd/system</code>下的服务配置文件:</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">[root@slions_pc1 ~]# ls -l /usr/lib/systemd/system/*.service|head -10</span><br><span class="line">-rw-r--r--. 1 root root  275 11月 14 2018 /usr/lib/systemd/system/abrt-ccpp.service</span><br><span class="line">-rw-r--r--. 1 root root  380 11月 14 2018 /usr/lib/systemd/system/abrtd.service</span><br><span class="line">-rw-r--r--. 1 root root  361 11月 14 2018 /usr/lib/systemd/system/abrt-oops.service</span><br><span class="line">-rw-r--r--. 1 root root  266 11月 14 2018 /usr/lib/systemd/system/abrt-pstoreoops.service</span><br><span class="line">-rw-r--r--. 1 root root  262 11月 14 2018 /usr/lib/systemd/system/abrt-vmcore.service</span><br><span class="line">-rw-r--r--. 1 root root  311 11月 14 2018 /usr/lib/systemd/system/abrt-xorg.service</span><br><span class="line">-rw-r--r--. 1 root root  275 10月 31 2018 /usr/lib/systemd/system/arp-ethers.service</span><br><span class="line">-rw-r--r--. 1 root root  222 10月 31 2018 /usr/lib/systemd/system/atd.service</span><br><span class="line">-rw-r--r--. 1 root root 1384 8月   8 2019 /usr/lib/systemd/system/auditd.service</span><br><span class="line">lrwxrwxrwx. 1 root root   14 5月  21 16:45 /usr/lib/systemd/system/autovt@.service -&gt; getty@.service</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>下面这些目录(*.target.wants)定义各种类型下需要运行的服务：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">[root@slions_pc1 ~]# ls -1dF /etc/systemd/system/*</span><br><span class="line">/etc/systemd/system/basic.target.wants/</span><br><span class="line">/etc/systemd/system/dbus-org.freedesktop.NetworkManager.service@</span><br><span class="line">/etc/systemd/system/dbus-org.freedesktop.nm-dispatcher.service@</span><br><span class="line">/etc/systemd/system/default.target@</span><br><span class="line">/etc/systemd/system/default.target.wants/</span><br><span class="line">/etc/systemd/system/getty.target.wants/</span><br><span class="line">/etc/systemd/system/local-fs.target.wants/</span><br><span class="line">/etc/systemd/system/multi-user.target.wants/</span><br><span class="line">/etc/systemd/system/network-online.target.wants/</span><br><span class="line">/etc/systemd/system/sockets.target.wants/</span><br><span class="line">/etc/systemd/system/sysinit.target.wants/</span><br><span class="line">/etc/systemd/system/system-update.target.wants/</span><br><span class="line">/etc/systemd/system/vmtoolsd.service.requires/</span><br></pre></td></tr></table></figure><p>/etc/systemd/system/multi-user.target.wants下的服务配置文件，几乎都是软链接</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">[root@slions_pc1 ~]# ls -l /etc/systemd/system/multi-user.target.wants/ | awk &#x27;&#123;print $9,$10,$11&#125;&#x27;</span><br><span class="line">abrt-ccpp.service -&gt; /usr/lib/systemd/system/abrt-ccpp.service</span><br><span class="line">abrtd.service -&gt; /usr/lib/systemd/system/abrtd.service</span><br><span class="line">abrt-oops.service -&gt; /usr/lib/systemd/system/abrt-oops.service</span><br><span class="line">abrt-vmcore.service -&gt; /usr/lib/systemd/system/abrt-vmcore.service</span><br><span class="line">abrt-xorg.service -&gt; /usr/lib/systemd/system/abrt-xorg.service</span><br><span class="line">atd.service -&gt; /usr/lib/systemd/system/atd.service</span><br><span class="line">auditd.service -&gt; /usr/lib/systemd/system/auditd.service</span><br><span class="line">chronyd.service -&gt; /usr/lib/systemd/system/chronyd.service</span><br><span class="line">crond.service -&gt; /usr/lib/systemd/system/crond.service</span><br><span class="line">irqbalance.service -&gt; /usr/lib/systemd/system/irqbalance.service</span><br><span class="line">kdump.service -&gt; /usr/lib/systemd/system/kdump.service</span><br><span class="line">libstoragemgmt.service -&gt; /usr/lib/systemd/system/libstoragemgmt.service</span><br><span class="line">mdmonitor.service -&gt; /usr/lib/systemd/system/mdmonitor.service</span><br></pre></td></tr></table></figure><h1 id="systemd-service文件格式"><a href="#systemd-service文件格式" class="headerlink" title="systemd service文件格式"></a>systemd service文件格式</h1><p>基本的配置文件格式如下：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">[Unit]</span><br><span class="line">Description = some descriptions</span><br><span class="line">Documentation = man:xxx(8) man:xxx_config(5)</span><br><span class="line">Requires = xxx1.target xxx2.target</span><br><span class="line">After = yyy1.target yyy2.target</span><br><span class="line"></span><br><span class="line">[Service]</span><br><span class="line">Type = &lt;TYPE&gt;</span><br><span class="line">ExecStart = &lt;CMD_for_START&gt;</span><br><span class="line">ExecStop = &lt;CMD_for_STOP&gt;</span><br><span class="line">ExecReload = &lt;CMD_for_RELOAD&gt;</span><br><span class="line">Restart = &lt;WHEN_TO_RESTART&gt;</span><br><span class="line">RestartSec = &lt;TIME&gt;</span><br><span class="line"></span><br><span class="line">[Install]</span><br><span class="line">WantedBy = xxx.target yy.target</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>一个.Service配置文件分为三部分：</p><ul><li>Unit：定义该服务作为Unit角色时相关的属性</li><li>Service：定义本服务相关的属性</li><li>Install：定义本服务在设置服务开机自启动时相关的属性。换句话说，只有在创建/移除服务配置文件的软链接时，Install段才会派上用场。这一配置段不是必须的，<strong>当未配置[Install]时，设置开机自启动或禁止开机自启动的操作将无任何效果</strong></li></ul><h1 id="参考文档"><a href="#参考文档" class="headerlink" title="参考文档"></a>参考文档</h1><p><a href="http://www.jinbuguo.com/systemd/systemd.service.html">http://www.jinbuguo.com/systemd/systemd.service.html</a></p><p><a href="https://www.junmajinlong.com/linux/systemd/service_1/">https://www.junmajinlong.com/linux/systemd/service_1/</a></p>]]></content>
    
    
      
      
    <summary type="html">&lt;a href=&quot;/2021/08/01/%E5%88%9B%E5%BB%BA%E5%BC%80%E6%9C%BA%E8%87%AA%E5%90%AF%E7%A8%8B%E5%BA%8F%EF%BC%88%E4%B8%8A%E7%AF%87%EF%BC%89/&quot; title=&quot;上</summary>
      
    
    
    
    <category term="linux系统" scheme="https://slions.github.io/categories/linux%E7%B3%BB%E7%BB%9F/"/>
    
    
    <category term="linux" scheme="https://slions.github.io/tags/linux/"/>
    
  </entry>
  
  <entry>
    <title>创建开机自启程序（上篇）</title>
    <link href="https://slions.github.io/2021/08/01/%E5%88%9B%E5%BB%BA%E5%BC%80%E6%9C%BA%E8%87%AA%E5%90%AF%E7%A8%8B%E5%BA%8F%EF%BC%88%E4%B8%8A%E7%AF%87%EF%BC%89/"/>
    <id>https://slions.github.io/2021/08/01/%E5%88%9B%E5%BB%BA%E5%BC%80%E6%9C%BA%E8%87%AA%E5%90%AF%E7%A8%8B%E5%BA%8F%EF%BC%88%E4%B8%8A%E7%AF%87%EF%BC%89/</id>
    <published>2021-08-01T04:58:46.000Z</published>
    <updated>2021-08-12T11:50:18.525Z</updated>
    
    <content type="html"><![CDATA[<p>在我们实现如何创建开机自启程序前，先简单了解下linux的启动过程。</p><h1 id="linux启动过程"><a href="#linux启动过程" class="headerlink" title="linux启动过程"></a>linux启动过程</h1><p>Linux 系统的启动，从计算机开机通电自检开始，一直到登陆系统，需要经历多个过程。</p><h2 id="CentOS-RHEL6"><a href="#CentOS-RHEL6" class="headerlink" title="CentOS/RHEL6"></a>CentOS/RHEL6</h2><ol><li>服务器加电，加载 BIOS 信息，BIOS 进行系统检测。依照 BIOS 设定，找到第一个可以启动的设备（一般是硬盘）；</li><li>读取第一个启动设备的 MBR (主引导记录），加载 MBR 中的 Boot Loader（启动引导程序，最为常见的是 GRUB）。</li><li>依据 Boot Loader 的设置加载内核，内核会再进行一遍系统检测。系统一般会采用内核检测硬件的信息，而不一定采用 Bios 的自检信息。内核在检测硬件的同时，还会通过加载动态模块的形式加载硬件的驱动。</li><li>内核启动系统的第一个进程，也就是 /sbin/init。</li><li>由 /sbin/init 进程调用 /etc/init/rcS.conf 配置文件，通过这个配置文件调用 /etc/rc.d/rc.sysinit 配置文件。而 /etc/rc.d/rc.sysinit 配置文件是用来进行系统初始化的，主要用于配置计算机的初始环境。</li><li>还是通过 /etc/init/rcS.conf 配置文件调用 /etc/inittab 配置文件。通过 /etc/inittab 配置文件来确定系统的默认运行级别。</li><li>确定默认运行级别后，调用 /etc/init/rc.conf 配置文件。</li><li>通过 /etc/init/rc.conf 配置文件调用并执行 /etc/rc.d/rc 脚本，并传入运行级别参数。</li><li>/etc/rc.d/rc 确定传入的运行级别，然后运行相应的运行级别目录 /etc/rc[0-6].d/ 中的脚本。</li><li>/etc/rc[0-6].d/ 目录中的脚本依据设定好的优先级依次启动和关闭。</li><li>最后执行 /etc/rc.d/rc.local 中的程序。</li><li>如果是字符界面启动，就可以看到登录界面了。如果是图形界面启动，就会调用相应的 X Window 接口。</li></ol><p><img src="/doc_picture/2-1Q02310563a22.jpg" alt="img"></p><h2 id="CentOS-RHEL7"><a href="#CentOS-RHEL7" class="headerlink" title="CentOS/RHEL7"></a>CentOS/RHEL7</h2><ol><li><p>服务器加电，加载 BIOS 信息，BIOS 进行系统检测。依照 BIOS 设定，找到第一个可以启动的设备（一般是硬盘）；</p></li><li><p>读取第一个启动设备的 MBR (主引导记录），加载 MBR 中的 OSLoader（启动引导程序GRUB2）。</p></li><li><p>OSLoader 加载其相关配置 , 并显示相关的配置 菜单来引导用户选择相关操作(/etc/grub.d,/etc/default/grub,/boot/grub2/grub.cfg)</p></li><li><p>在用户选择后 ( 或 timeout 后 ),GRUB2 将加载 内核及 initramfs 至内存中 .initramfs 属于一个 img 的虚拟磁盘。</p><p>initramfs 包含了动态的内核模块 , 初始化脚本及非常多的硬件驱动等 , 在 RHEL7 中 initramfs 自身即包含了一个完整的可用系统。（/etc/dracut.conf）</p></li><li><p>GRUB2 将控制系统切换到 kernel, 通过对 GRUB2 的控制可以添加各种 kernel 的选项 . 并将 这些选项同时传递至内存中 , 影响 kernel 及 initramfs 的运行 . (/etc/grub.d,/etc/default/grub,/boot/grub2/gr ub.cfg)</p></li><li><p>kernel 在启动后将初始化所有的硬件 , 通过 initramfs 找到硬件相关的驱动程序 , 而后从 initramfs 中执行 PID 1 的 /sbin/init 命令 ( 最高 进程 ). 在 RHEL 中 init 属 于 /lib/systemd/systemd 的软连接 , 以及一个 udev 进程来自动建立已经存在的硬件的设备。（/etc/fstab）</p></li><li><p>由 initramfs 建立的内存的根分区 /sysroot( 物理 ‘/‘ 分区 ) 在成功挂载之后，将切 换到此根分区上 , 并将 systemd 重新执行安装至真实的根分区中。</p></li><li><p> systemd 开始查找所有的服务 , 开始执行 ( 停止 ) 相关的服务 , 以符合该服务的配置并解决相关的依赖问题。（/etc/systemd/system/default.target）</p></li></ol><blockquote><p>Systemd引入了并行启动的概念，它会为每个需要启动的守护进程建立一个套接字，这些套接字对于使用它们的进程来说是抽象的，这样它们可以允许不同守护进程之间进行交互。Systemd会创建新进程并为每个进程分配一个控制组（cgroup）。处于不同控制组的进程之间可以通过内核来互相通信。</p><p>从 7.x 版本开始，引入了 systemd 来管理服务，/etc/rc.local 是为向前兼容而保留。</p></blockquote><h1 id="开机自启"><a href="#开机自启" class="headerlink" title="开机自启"></a>开机自启</h1><p>我们现在大多都在使用CentOS/RHEL7中，实现开机启动程序主要有两种方法：</p><ul><li><p>在/etc/rc.local脚本文件中编写启动程序的脚本</p></li><li><p>把要启动的程序配置成自定义的systemd服务（推荐）</p></li></ul><h2 id="通过rc-local实现开机自启程序"><a href="#通过rc-local实现开机自启程序" class="headerlink" title="通过rc.local实现开机自启程序"></a>通过rc.local实现开机自启程序</h2><p>/etc/rc.local是/etc/rc.d/rc.local的软链接</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@slions_pc1 home]# ll /etc/rc.local</span><br><span class="line">lrwxrwxrwx. 1 root root 13 5月  21 16:45 /etc/rc.local -&gt; rc.d/rc.local</span><br></pre></td></tr></table></figure><h3 id="rc-local配置文件解读"><a href="#rc-local配置文件解读" class="headerlink" title="rc.local配置文件解读"></a>rc.local配置文件解读</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">[root@slions_pc1 home]# cat /etc/rc.local</span><br><span class="line"><span class="meta">#</span><span class="bash">!/bin/bash</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> THIS FILE IS ADDED FOR COMPATIBILITY PURPOSES</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 添加此文件是为了兼容。</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> It is highly advisable to create own systemd services or udev rules</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> to run scripts during boot instead of using this file.</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 强烈建议创建自己的systemd服务或udev规则，以便在引导期间运行脚本，而不是使用此文件。</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> In contrast to previous versions due to parallel execution during boot</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> this script will NOT be run after all other services.</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 请注意，必须运行<span class="string">&#x27;chmod+x/etc/rc.d/rc.local&#x27;</span>，以确保在引导期间执行此脚本。</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> Please note that you must run <span class="string">&#x27;chmod +x /etc/rc.d/rc.local&#x27;</span> to ensure</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> that this script will be executed during boot.</span></span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash">默认会touch这个文件，每次系统启动时都会touch这个文件，这个文件的修改时间就是系统的启动时间</span></span><br><span class="line">touch /var/lock/subsys/local</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>rc.local本质上是一个shell脚本文件，可以把启动时需要执行的命令写在里面，启动时将按顺序执行。</p><h3 id="rc-local编写注意"><a href="#rc-local编写注意" class="headerlink" title="rc.local编写注意"></a>rc.local编写注意</h3><ul><li>rc.local脚本在操作系统启动时只执行一次</li><li>环境变量的问题<ul><li>在rc.local脚本中执行程序时是没有环境变量的，如果执行的程序需要环境变量，可以在脚本中设置环境变量</li></ul></li><li>命令需写绝对路径</li><li>不要让rc.local挂起<ul><li>rc.local是一个脚本，是按顺序执行的，执行完一个程序后才会执行下一个程序，如果某程序不是后台程序，就应该加&amp;让程序运行在后台，否则rc.local会挂起。</li></ul></li><li>切记chmod+x/etc/rc.d/rc.local赋予执行权限</li></ul><h2 id="通过system实现开机自启程序"><a href="#通过system实现开机自启程序" class="headerlink" title="通过system实现开机自启程序"></a>通过system实现开机自启程序</h2><h3 id="systemd可管理的服务"><a href="#systemd可管理的服务" class="headerlink" title="systemd可管理的服务"></a>systemd可管理的服务</h3><p>操作系统使用systemd后，所有用户进程都是systemd的后代进程。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line">[root@slions_pc1 ~]# pstree -p</span><br><span class="line">systemd(1)─┬─NetworkManager(8829)─┬─&#123;NetworkManager&#125;(8881)</span><br><span class="line">           │                      └─&#123;NetworkManager&#125;(8883)</span><br><span class="line">           ├─VGAuthService(8844)</span><br><span class="line">           ├─abrt-dbus(14768)─┬─&#123;abrt-dbus&#125;(14802)</span><br><span class="line">           │                  └─&#123;abrt-dbus&#125;(14813)</span><br><span class="line">           ├─abrt-watch-log(8832)</span><br><span class="line">           ├─abrtd(8830)</span><br><span class="line">           ├─atd(8850)</span><br><span class="line">           ├─auditd(8793)───&#123;auditd&#125;(8794)</span><br><span class="line">           ├─chronyd(8869)</span><br><span class="line">           ├─crond(8853)</span><br><span class="line">           ├─dbus-daemon(8821)───&#123;dbus-daemon&#125;(8827)</span><br><span class="line">           ├─irqbalance(8834)</span><br><span class="line">           ├─login(8866)───bash(14656)</span><br><span class="line">           ├─lsmd(8849)</span><br><span class="line">           ├─lvmetad(4420)</span><br><span class="line">           ├─master(9397)─┬─pickup(9415)</span><br><span class="line">           │              └─qmgr(9416)</span><br><span class="line">           ├─polkitd(8818)─┬─&#123;polkitd&#125;(8826)</span><br><span class="line">           │               ├─&#123;polkitd&#125;(8828)</span><br><span class="line">           │               ├─&#123;polkitd&#125;(8833)</span><br><span class="line">           │               ├─&#123;polkitd&#125;(8839)</span><br><span class="line">           │               ├─&#123;polkitd&#125;(8840)</span><br><span class="line">           │               └─&#123;polkitd&#125;(8841)</span><br><span class="line">           ├─rngd(8835)</span><br><span class="line">           ├─rpcbind(8824)</span><br><span class="line">           ├─rsyslogd(9174)─┬─&#123;rsyslogd&#125;(9237)</span><br><span class="line">           │                └─&#123;rsyslogd&#125;(9258)</span><br><span class="line">           ├─smartd(8820)</span><br><span class="line">           ├─sshd(9171)─┬─sshd(19034)───bash(19040)───pstree(19161)</span><br><span class="line">           │            └─sshd(19038)───sftp-server(19075)</span><br><span class="line">           ├─systemd-journal(4398)</span><br><span class="line">           ├─systemd-logind(8846)</span><br><span class="line">           ├─systemd-udevd(4427)</span><br><span class="line">           ├─tuned(9172)─┬─&#123;tuned&#125;(9926)</span><br><span class="line">           │             ├─&#123;tuned&#125;(9928)</span><br><span class="line">           │             ├─&#123;tuned&#125;(9984)</span><br><span class="line">           │             └─&#123;tuned&#125;(10117)</span><br><span class="line">           └─vmtoolsd(8845)───&#123;vmtoolsd&#125;(8935)</span><br></pre></td></tr></table></figure><p>虽然从进程树关系来看，所有进程都直接或间接地受到systemd的管理，但是，并非所有systemd的子进程都受Systemd Unit管理单元的管理。只有那些由systemd方式启动的服务进程(比如systemctl命令启动)才受到Systemd Unit管理单元的监控和管理。为了简化描述，后面均直接以『systemd管理』来描述受systemd unit管理单元的管理。</p><p>比如，用户可以通过下面两种方式启动Nginx服务进程：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">nginx                    # (1)</span><br><span class="line">systemctl start nginx    # (2)</span><br></pre></td></tr></table></figure><p>但systemd只能监控、管理第(2)种方式启动的nginx服务。比如第一种方式启动的nginx，无法使用systemctl stop nginx来停止。</p><h3 id="systemd管理服务的命令"><a href="#systemd管理服务的命令" class="headerlink" title="systemd管理服务的命令"></a>systemd管理服务的命令</h3><p><code>systemctl</code>是 Systemd 的主命令，用于管理系统。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 启动、停止服务:</span></span><br><span class="line">systemctl start Service_Name1 Service_Name2</span><br><span class="line">systemctl stop Service_Name</span><br><span class="line"><span class="meta">#</span><span class="bash"> 开机启动服务：</span></span><br><span class="line">systemctl enable Service_Name</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 有时候，该命令可能没有响应，服务停不下来。这时候就不得不<span class="string">&quot;杀进程&quot;</span>了，向正在运行的进程发出<span class="built_in">kill</span>信号。</span></span><br><span class="line">systemctl kill Service_Name</span><br><span class="line"><span class="meta">#</span><span class="bash"> 服务重载、重启相关操作：</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 重载服务：服务未运行时不做任何事</span></span><br><span class="line">systemctl reload Service_Name</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 重启服务：服务已运行时重启之，服务未运行时启动之</span></span><br><span class="line">systemctl restart Service_Name</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 服务已运行时重启之，未运行时不启动之</span></span><br><span class="line">systemctl try-restart Service_Name</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 服务已运行时，如果支持reload，则reload，如果不支持则restart</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 服务未运行时，启动之</span></span><br><span class="line">systemctl reload-or-restart Service_Name</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 服务已运行时，如果支持reload，则reload，如果不支持则restart</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 服务未运行时，不做任何事</span></span><br><span class="line">systemctl reload-or-try-restart Service_Name</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 查看服务状态</span></span><br><span class="line">systemctl status Service_Name</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 检查服务是否active: 服务是否已启动</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 至少一个服务active时，返回0，否则返回非0退出状态码</span></span><br><span class="line">systemctl is-active Service_Name1 Service_Name2</span><br><span class="line">systemctl --quiet is-active Service_Name  # 静默模式</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 检查服务是否failed: 服务启动命令退出状态码非0或启动超时</span></span><br><span class="line">systemctl is-failed Service_Name</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>因为篇幅问题，<a href="/2021/08/01/%E5%88%9B%E5%BB%BA%E5%BC%80%E6%9C%BA%E8%87%AA%E5%90%AF%E7%A8%8B%E5%BA%8F%EF%BC%88%E4%B8%8B%E7%AF%87%EF%BC%89/" title="下篇继续介绍如何编写systemd service">下篇继续介绍如何编写systemd service</a></p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;在我们实现如何创建开机自启程序前，先简单了解下linux的启动过程。&lt;/p&gt;
&lt;h1 id=&quot;linux启动过程&quot;&gt;&lt;a href=&quot;#linux启动过程&quot; class=&quot;headerlink&quot; title=&quot;linux启动过程&quot;&gt;&lt;/a&gt;linux启动过程&lt;/h1&gt;&lt;p&gt;</summary>
      
    
    
    
    <category term="linux系统" scheme="https://slions.github.io/categories/linux%E7%B3%BB%E7%BB%9F/"/>
    
    
    <category term="linux" scheme="https://slions.github.io/tags/linux/"/>
    
  </entry>
  
  <entry>
    <title>alias与rm的最佳实践</title>
    <link href="https://slions.github.io/2021/08/01/alias%E4%B8%8Erm%E7%9A%84%E6%9C%80%E4%BD%B3%E5%AE%9E%E8%B7%B5/"/>
    <id>https://slions.github.io/2021/08/01/alias%E4%B8%8Erm%E7%9A%84%E6%9C%80%E4%BD%B3%E5%AE%9E%E8%B7%B5/</id>
    <published>2021-08-01T03:37:38.000Z</published>
    <updated>2021-08-12T11:50:28.246Z</updated>
    
    <content type="html"><![CDATA[<p>不论是刚入职的运维萌新，还是工作多年的运维老油条，当在服务器上敲下rm时必须要保证命令的准确，在技术贴吧中也常常有人调侃到不想干了就<code>rm -rf /</code>与删库跑路等等的段子，如何让<code>rm</code>的操作更有保障呢，以下会提供一种解题思路。</p><h1 id="alias的用法"><a href="#alias的用法" class="headerlink" title="alias的用法"></a>alias的用法</h1><p>Linux alias命令用于设置指令的别名。</p><p>语法：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">alias[别名]=[指令名称]</span><br></pre></td></tr></table></figure><p>使用不带参数的alias将列出当前shell环境下所有的已定义的别名。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">[root@slions_pc1 home]# alias</span><br><span class="line">alias cp=&#x27;cp -i&#x27;</span><br><span class="line">alias egrep=&#x27;egrep --color=auto&#x27;</span><br><span class="line">alias fgrep=&#x27;fgrep --color=auto&#x27;</span><br><span class="line">alias grep=&#x27;grep --color=auto&#x27;</span><br><span class="line">alias l.=&#x27;ls -d .* --color=auto&#x27;</span><br><span class="line">alias ll=&#x27;ls -l --color=auto&#x27;</span><br><span class="line">alias ls=&#x27;ls --color=auto&#x27;</span><br><span class="line">alias mv=&#x27;mv -i&#x27;</span><br><span class="line">alias rm=&#x27;rm -i&#x27;</span><br><span class="line">alias which=&#x27;alias | /usr/bin/which --tty-only --read-alias --show-dot --show-tilde&#x27;</span><br></pre></td></tr></table></figure><p>另外需要说明的是，当别名和命令同名时，将优先执行别名(否则别名就没有意义了)，这可以从which的结果中看出：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@slions_pc1 home]# which rm</span><br><span class="line">alias rm=&#x27;rm -i&#x27;</span><br><span class="line">        /usr/bin/rm</span><br></pre></td></tr></table></figure><p>如果定义的命名名称和原始命令同名(例如定义的别名 ls=’ls -l’ )，此时如果想要明确使用原始命令，可以删除别名或者使用绝对路径或者使用转义符来还原命令。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[root@slions_pc1 home]# \ls</span><br><span class="line">slions</span><br><span class="line">[root@slions_pc1 home]# /usr/bin/ls</span><br><span class="line">slions</span><br></pre></td></tr></table></figure><p>alias命令是临时定义别名，要定义长久生效的别名就将别名定义语句写入<code>/etc/profile</code>或<code>~/.bash_profile</code>或<code>~/.bashrc</code>，第一个对所有用户有效，后面两个对对应用户有效。修改后记得使用source来重新调取这些配置文件。</p><p>使用unalias可以临时取消别名。</p><h1 id="alias与rm"><a href="#alias与rm" class="headerlink" title="alias与rm"></a>alias与rm</h1><p>如何让alias与rm配合起来能减少误操作引发的影响呢，整体思路就是rm变为mv操作，后续给备份目录增加个时效性机制，保障磁盘空间的可用性。</p><h2 id="alias的bug"><a href="#alias的bug" class="headerlink" title="alias的bug"></a>alias的bug</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">[root@slions_pc1 home]# ls</span><br><span class="line">slions</span><br><span class="line">[root@slions_pc1 home]# touch testfile</span><br><span class="line">[root@slions_pc1 home]# alias rmm=&#x27;cp $@ /tmp/backup;rm $@&#x27;</span><br><span class="line">[root@slions_pc1 home]# rmm testfile</span><br><span class="line">cp: 在&quot;/tmp/backup&quot; 后缺少了要操作的目标文件</span><br><span class="line">Try &#x27;cp --help&#x27; for more information.</span><br><span class="line">rm：是否删除普通空文件 &quot;testfile&quot;？y</span><br><span class="line">[root@slions_pc1 home]# ls</span><br><span class="line">slions</span><br><span class="line">[root@slions_pc1 home]# ls /tmp/backup/</span><br></pre></td></tr></table></figure><p>该别名的目的是删除文件时先备份到<code>/tmp/backup</code>目录下，然后再删除。<strong>按照man bash里的说明，别名rmm只是第一个cp命令的别名，分号后的rm不是别名的一部分，而是紧跟在别名后的下一行命令。当执行别名rmm时，首先读取别名到分号位置处，然后进行别名扩展，执行完别名命令后，再执行分号后的rm命令。</strong></p><p>上面的命令没有达到预期效果，问题出在cp的参数”$@”，该变量本表示提供的所有参数，但由于cp命令后使用分号分隔并定义了另一个命令，这使得执行别名命令时，参数无法传递到cp命令上，而只能传递到最后一个命令rm上，也就是说cp后的”$@”是空值。</p><p>可以测试下：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[root@slions_pc1 home]# touch testfile</span><br><span class="line">[root@slions_pc1 home]# alias rmm=&#x27;echo cp $@ /tmp/backup;echo rm $@&#x27;</span><br><span class="line">[root@slions_pc1 home]# rmm testfile</span><br><span class="line">cp /tmp/backup</span><br><span class="line">rm testfile</span><br></pre></td></tr></table></figure><p>从上面的结果中看到cp后的”$@”根本就没有进行扩展，而是空值。</p><p>那如果别名定义语句中没有使用分号或其他方法定义额外的命令，而是只有一个命令呢？别名一定就能正确工作吗？</p><p>测试会发现也有问题：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@slions_pc1 home]# alias rmm=&#x27;echo cp $@ /tmp/backup&#x27;</span><br><span class="line">[root@slions_pc1 home]# rmm testfile</span><br><span class="line">cp /tmp/backup testfile</span><br></pre></td></tr></table></figure><p>之所以无法正常工作，是因为<code>/tmp/backup</code>也是”$@”的一部分，且是”$@”中最前面的参数。</p><p>从上面的分析可以知道，alias是有其缺陷的，它只适合进行简单的命令和参数替换、补全，想要实现复杂的命令替代有点难度。因此man bash中建议尽量使用<strong>函数</strong>来取代别名。</p><h2 id="alias最佳实践"><a href="#alias最佳实践" class="headerlink" title="alias最佳实践"></a>alias最佳实践</h2><p>例如，为了让rm安全执行，使用以下方法定义别名：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[root@slions_pc1 home]# alias rmm=&#x27;move()&#123; /bin/mv -f $@ /tmp/backup; &#125;;move $@&#x27;</span><br><span class="line">[root@slions_pc1 home]# rmm testfile</span><br><span class="line">[root@slions_pc1 home]# ls</span><br><span class="line">slions</span><br><span class="line">[root@slions_pc1 home]# ls /tmp/backup/</span><br><span class="line">testfile</span><br></pre></td></tr></table></figure><p>因为执行别名时的参数只能传递给最后一个命令即move函数，但”$@”代表的参数可以传递给函数，让函数中的”$@”得到正确的扩展，于是整个别名都能合理且正确地执行。</p><p>或者直接定义一个shell function替代rm。例如向/etc/profile.d/rm.sh文件中写入：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">function rm()&#123; [ -d /tmp/rmbackup ] || mkdir /tmp/rmbackup;/bin/mv -f $@ /tmp/rmbackup; &#125;</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@slions_pc1 home]# chmod +x /etc/profile.d/rm.sh</span><br><span class="line">[root@slions_pc1 home]# source /etc/profile.d/rm.sh</span><br></pre></td></tr></table></figure><p>如此，执行rm命令时，便会执行此处定义的rm函数，使得rm变得更安全。但注意，这样的函数默认无法直接在脚本中使用，除非使用 <code>export -f function_name </code>导出函数，使其可以被子shell继承。</p><p>所以，可在/etc/profile.d/rm.sh文件的尾部加上导出语句：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">function rm()&#123; [ -d /tmp/rmbackup ] || mkdir /tmp/rmbackup;/bin/mv -f $@ /tmp/rmbackup; &#125;</span><br><span class="line">export -f rm</span><br></pre></td></tr></table></figure><p>如果function名和命令名相同，则默认优先执行function，除非使用command明确指定。例如上面定义了rm函数，如果想执行rm命令，除了使用/bin/rm，还可以如下操作：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">command rm testfile</span><br></pre></td></tr></table></figure><p>最后，如果是**在shell脚本里涉及到rm命令，那么更建议在每次rm之前先cd到那个目录下，然后再rm相对路径，这样至少能保证不出现符号”/“**。当然，最重要的是习惯和责任心，切勿忙中出错。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;不论是刚入职的运维萌新，还是工作多年的运维老油条，当在服务器上敲下rm时必须要保证命令的准确，在技术贴吧中也常常有人调侃到不想干了就&lt;code&gt;rm -rf /&lt;/code&gt;与删库跑路等等的段子，如何让&lt;code&gt;rm&lt;/code&gt;的操作更有保障呢，以下会提供一种解题思路。</summary>
      
    
    
    
    <category term="linux系统" scheme="https://slions.github.io/categories/linux%E7%B3%BB%E7%BB%9F/"/>
    
    
    <category term="linux" scheme="https://slions.github.io/tags/linux/"/>
    
  </entry>
  
  <entry>
    <title>linux用户与组管理（下篇）</title>
    <link href="https://slions.github.io/2021/07/31/linux%E7%94%A8%E6%88%B7%E4%B8%8E%E7%BB%84%E7%AE%A1%E7%90%86%EF%BC%88%E4%B8%8B%E7%AF%87%EF%BC%89/"/>
    <id>https://slions.github.io/2021/07/31/linux%E7%94%A8%E6%88%B7%E4%B8%8E%E7%BB%84%E7%AE%A1%E7%90%86%EF%BC%88%E4%B8%8B%E7%AF%87%EF%BC%89/</id>
    <published>2021-07-31T07:27:52.000Z</published>
    <updated>2021-08-12T11:50:35.229Z</updated>
    
    <content type="html"><![CDATA[<h1 id="用户和组管理命令"><a href="#用户和组管理命令" class="headerlink" title="用户和组管理命令"></a>用户和组管理命令</h1><h2 id="useradd-和-adduser"><a href="#useradd-和-adduser" class="headerlink" title="useradd 和 adduser"></a>useradd 和 adduser</h2><p>adduser是useradd的一个软链接。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@slions_pc1 ~]# ls -l /usr/sbin/adduser</span><br><span class="line">lrwxrwxrwx. 1 root root 7 5月  21 16:45 /usr/sbin/adduser -&gt; useradd</span><br></pre></td></tr></table></figure><p>useradd用法：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line">useradd [options] login_name</span><br><span class="line">选项说明：</span><br><span class="line">-b：指定家目录的basedir，默认为/home目录</span><br><span class="line">-d：指定用户家目录，不写时默认为/home/user_name</span><br><span class="line">-m：要创建家目录时，若家目录不存在则自动创建，若不指定该项且/etc/login.defs中的CREATE_HOME未启用时将不会创建家目录</span><br><span class="line">-M：显式指明不要创建家目录，会覆盖/etc/login.defs中的CREATE_HOME设置</span><br><span class="line"> </span><br><span class="line">-g：指定用户主组，要求组已存在</span><br><span class="line">-G：指定用户的辅助组，多个组以逗号分隔</span><br><span class="line">-N：明确指明不要创建和用户名同名的组名</span><br><span class="line">-U：明确指明要创建一个和用户名同名的组，并将用户加入到此组中</span><br><span class="line"></span><br><span class="line">-o：允许创建一个重复UID的用户，只有和-u选项同时使用时才生效</span><br><span class="line">-r：创建一个系统用户。useradd命令不会为此选项的系统用户创建家目录，除非明确使用-m选项</span><br><span class="line">-s：指定用户登录的shell，默认留空。此时将选择/etc/default/useradd中的SHELL变量设置</span><br><span class="line">-u：指定用户uid，默认uid必须唯一，除非使用了-o选项</span><br><span class="line">-c：用户的注释信息 </span><br><span class="line"></span><br><span class="line">-k：指定骨架目录(skeleton)</span><br><span class="line">-K：修改/etc/login.defs文件中有关于用户的配置项，不能修改组相关的配置。设置方式为KEY=VALUE，如-K UID_MIN=100</span><br><span class="line">-D：修改useradd创建用户时的默认选项，就修改/etc/default/useradd文件</span><br><span class="line">-e：帐户过期时间，格式为&quot;YYYY-MM-DD&quot;</span><br><span class="line">-f：密码过期后，该账号还能存活多久才被禁用，设置为0表示密码过期立即禁用帐户，设置为-1表示禁用此功能</span><br><span class="line">-l：不要将用户的信息写入到lastlog和faillog文件中。默认情况下，用户信息会写入到这两个文件中</span><br><span class="line"></span><br><span class="line">useradd -D [options]</span><br><span class="line">修改/etc/default/useradd文件</span><br><span class="line">选项说明：不加任何选项时会列出默认属性</span><br><span class="line">-b, --base-dir BASE_DIR</span><br><span class="line">-e, --expiredate EXPIRE_DATE</span><br><span class="line">-f, --inactive INACTIVE</span><br><span class="line">-g, --gid GROUP</span><br><span class="line">-s, --shell SHELL</span><br></pre></td></tr></table></figure><p>useradd创建用户时，默认会自动创建一个和用户名相同的用户组，这是<code>/etc/login.defs</code>中的USERGROUP_ENAB变量控制的。</p><p>useradd创建普通用户时，不加任何和家目录相关的选项时，是否创建家目录是由<code>/etc/login.defs</code>中的CREATE_HOME变量控制的。</p><h2 id="批量创建用户-newusers"><a href="#批量创建用户-newusers" class="headerlink" title="批量创建用户 newusers"></a>批量创建用户 newusers</h2><p>newusers用于批量创建或修改已有用户信息。在创建用户时，它会读取<code>/etc/login.defs</code>文件中的配置项。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">[root@slions_pc1 ~]# newusers -h</span><br><span class="line">用法：newusers [选项]</span><br><span class="line"></span><br><span class="line">选项：</span><br><span class="line">  -c, --crypt-method 方法        加密方法(NONE DES MD5 SHA256 SHA512 中的一个)</span><br><span class="line">  -h, --help                    显示此帮助信息并推出</span><br><span class="line">  -r, --system                  创建系统帐号</span><br><span class="line">  -R, --root CHROOT_DIR         chroot 到的目录</span><br><span class="line">  -s, --sha-rounds              SHA* 加密算法中的 SHA 旁边的数字</span><br></pre></td></tr></table></figure><p>newusers命令从file中或标准输入中读取要创建或修改用户的信息，文件中每行格式都一样(同passwd格式)，一行代表一个用户。格式如下：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">name:passwd:uid:gid:note:homedir:shell</span><br></pre></td></tr></table></figure><p>newusers首先尝试创建或修改所有指定的用户，然后将信息写入到user和group的文件中。如果尝试创建或修改用户过程中发生错误，则所有动作都将回滚，但如果在写入过程中发生错误，则写入成功的不会回滚，这将可能导致文件的不一致性。要检查用户、组文件的一致性，可以使用showdow-utils包提供的grpck和pwck命令。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">[root@slions_pc1 ~]# cat batch_adduser</span><br><span class="line">s1:123456:7295:7295::/home/s1:/bin/bash</span><br><span class="line">s2:123456:::::/bin/bash</span><br><span class="line">[root@slions_pc1 ~]# newusers -c SHA512 batch_adduser</span><br><span class="line">[root@slions_pc1 ~]# tail -2 /etc/passwd</span><br><span class="line">s1:x:7295:7295::/home/s1:/bin/bash</span><br><span class="line">s2:x:7296:7296:::/bin/bash</span><br><span class="line">[root@slions_pc1 ~]# tail -2 /etc/shadow</span><br><span class="line">s1:$6$OSCVQmJiFP/U4CbD$72ZkAJNKs4ehMgfxJR..tqNuy7yKHINycOiB/.lW4ANBtuIMuIcsphgw8mcfkR7A1tvhKifG6vmPbc8VjmfmV.:18839:0:99999:7:::</span><br><span class="line">s2:$6$/7nes/BRVe5pw7Gw$qV8.mVjf4Mpv9zVwjsKDSmtmx8qCwZwX03hmsbuFM.CEHJ.X76pX6Css8dvIAE87j7GcihAJN8lSn3Lg.KD.Q.:18839:0:99999:7:::</span><br><span class="line"></span><br></pre></td></tr></table></figure><h2 id="创建组-groupadd"><a href="#创建组-groupadd" class="headerlink" title="创建组 groupadd"></a>创建组 groupadd</h2><p>用法：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">[root@slions_pc1 ~]# groupadd -h</span><br><span class="line">用法：groupadd [选项] 组</span><br><span class="line"></span><br><span class="line">选项:</span><br><span class="line">  -f, --force           如果组已经存在则成功退出</span><br><span class="line">                        并且如果 GID 已经存在则取消 -g</span><br><span class="line">  -g, --gid GID                 为新组使用 GID</span><br><span class="line">  -h, --help                    显示此帮助信息并推出</span><br><span class="line">  -K, --key KEY=VALUE           不使用 /etc/login.defs 中的默认值</span><br><span class="line">  -o, --non-unique              允许创建有重复 GID 的组</span><br><span class="line">  -p, --password PASSWORD       为新组使用此加密过的密码</span><br><span class="line">  -r, --system                  创建一个系统账户</span><br><span class="line">  -R, --root CHROOT_DIR         chroot 到的目录</span><br></pre></td></tr></table></figure><h2 id="修改密码-passwd"><a href="#修改密码-passwd" class="headerlink" title="修改密码 passwd"></a>修改密码 passwd</h2><p>修改密码的工具。默认passwd命令不允许为用户创建空密码。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">[root@slions_pc1 ~]# passwd  --help</span><br><span class="line">用法: passwd [选项...] &lt;帐号名称&gt;</span><br><span class="line">  -k, --keep-tokens       保持身份验证令牌不过期</span><br><span class="line">  -d, --delete            删除已命名帐号的密码(只有根用户才能进行此操作)</span><br><span class="line">  -l, --lock              锁定指名帐户的密码(仅限 root 用户)</span><br><span class="line">  -u, --unlock            解锁指名账户的密码(仅限 root 用户)</span><br><span class="line">  -e, --expire            终止指名帐户的密码(仅限 root 用户)</span><br><span class="line">  -f, --force             强制执行操作</span><br><span class="line">  -x, --maximum=DAYS      密码的最长有效时限(只有根用户才能进行此操作)</span><br><span class="line">  -n, --minimum=DAYS      密码的最短有效时限(只有根用户才能进行此操作)</span><br><span class="line">  -w, --warning=DAYS      在密码过期前多少天开始提醒用户(只有根用户才能进行此操作)</span><br><span class="line">  -i, --inactive=DAYS     当密码过期后经过多少天该帐号会被禁用(只有根用户才能进行此操作)</span><br><span class="line">  -S, --status            报告已命名帐号的密码状态(只有根用户才能进行此操作)</span><br><span class="line">  --stdin                 从标准输入读取令牌(只有根用户才能进行此操作)</span><br></pre></td></tr></table></figure><h2 id="批量修改密码-chpasswd"><a href="#批量修改密码-chpasswd" class="headerlink" title="批量修改密码 chpasswd"></a>批量修改密码 chpasswd</h2><p>以批处理模式从标准输入中获取提供的用户和密码来修改用户密码，可以一次修改多个用户密码。也就是说不用交互。适用于一次性创建了多个用户时为他们提供密码。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">[root@slions_pc1 ~]# chpasswd --help</span><br><span class="line">用法：chpasswd [选项]</span><br><span class="line"></span><br><span class="line">选项：</span><br><span class="line">  -c, --crypt-method 方法        加密方法(NONE DES MD5 SHA256 SHA512 中的一个)</span><br><span class="line">  -e, --encrypted               提供的密码已经加密</span><br><span class="line">  -h, --help                    显示此帮助信息并推出</span><br><span class="line">  -m, --md5             使用 MD5 算法加密明文密码</span><br><span class="line">  -R, --root CHROOT_DIR         chroot 到的目录</span><br><span class="line">  -s, --sha-rounds              SHA* 加密算法中的 SHA 旁边的数字</span><br></pre></td></tr></table></figure><p>chpasswd会读取<code>/etc/login.defs</code>中的相关配置，修改成功后会将密码信息写入到密码文件中。</p><p>该命令的修改密码的处理方式是先在内存中修改，如果所有用户的密码都能设置成功，然后才写入到磁盘密码文件中。在内存中修改过程中出错，则所有修改都回滚，但若在写入密码文件过程中出错，则成功的不会回滚。</p><p>修改单个用户密码。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> <span class="built_in">echo</span> <span class="string">&quot;s1:123456&quot;</span> | chpasswd -c SHA512</span></span><br></pre></td></tr></table></figure><p>修改多个用户密码，则提供的每个用户对都要分行。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> <span class="built_in">echo</span>  -e <span class="string">&#x27;s1:123456\ns2:123456&#x27;</span> | chpasswd</span></span><br></pre></td></tr></table></figure><p>更方便的是写入到文件中，每行一个用户密码对。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> cat /tmp/passwdfile</span></span><br><span class="line">s1:123456</span><br><span class="line">s2:123456</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">$</span><span class="bash"> chapasswd -c SHA512 &lt;/tmp/passwdfile</span></span><br></pre></td></tr></table></figure><h2 id="chage"><a href="#chage" class="headerlink" title="chage"></a>chage</h2><p>chage命令主要修改或查看和密码时间相关的内容。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">-l：列出指定用户密码相关信息</span><br><span class="line">-E：指定帐户(不是密码)过期时间，所以是强锁定，如果指定为0，则立即过期，即直接锁定该用户</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">[root@slions_pc1 ~]# chage -l s1|column -t</span><br><span class="line">最近一次密码修改时间            ：7月    31,  2021</span><br><span class="line">密码过期时间                    ：从不</span><br><span class="line">密码失效时间                    ：从不</span><br><span class="line">帐户过期时间                    ：从不</span><br><span class="line">两次改变密码之间相距的最小天数  ：0</span><br><span class="line">两次改变密码之间相距的最大天数  ：99999</span><br><span class="line">在密码过期之前警告的天数        ：7</span><br><span class="line">[root@slions_pc1 ~]# chage -E 0 s1</span><br><span class="line">[root@slions_pc1 ~]# chage -l s1|column -t</span><br><span class="line">最近一次密码修改时间            ：7月    31,  2021</span><br><span class="line">密码过期时间                    ：从不</span><br><span class="line">密码失效时间                    ：从不</span><br><span class="line">帐户过期时间                    ：1月    01,  1970</span><br><span class="line">两次改变密码之间相距的最小天数  ：0</span><br><span class="line">两次改变密码之间相距的最大天数  ：99999</span><br><span class="line">在密码过期之前警告的天数        ：7</span><br><span class="line"></span><br></pre></td></tr></table></figure><h2 id="删除用户-userdel"><a href="#删除用户-userdel" class="headerlink" title="删除用户 userdel"></a>删除用户 userdel</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">userdel [options] login_name</span><br><span class="line">-r：递归删除家目录，默认不删除家目录。</span><br><span class="line">-f：强制删除用户，即使这个用户正处于登录状态。同时也会强制删除家目录。</span><br></pre></td></tr></table></figure><p>一般不直接删除家目录，即不用-r，可以vim /etc/passwd，将不需要的用户直接注释掉。</p><h2 id="删除组-groupdel"><a href="#删除组-groupdel" class="headerlink" title="删除组 groupdel"></a>删除组 groupdel</h2><p>如果要删除的组是某用户的主组，需要先删除主组中的用户。</p><h2 id="修改帐户属性信息-usermod"><a href="#修改帐户属性信息-usermod" class="headerlink" title="修改帐户属性信息 usermod"></a>修改帐户属性信息 usermod</h2><p>修改帐户属性信息。必须要确保在执行该命令的时候，待修改的用户没有在执行进程。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">usermod [options] login</span><br><span class="line">选项说明：</span><br><span class="line">-l：修改用户名，仅仅只是改用户名，其他的一切都不会改动(uid、家目录等)</span><br><span class="line">-u：新的uid，新的uid必须唯一，除非同时使用了-o选项</span><br><span class="line">-g：修改用户主组，可以是以gid或组名。对于那些以旧组为所属组的文件(除原家目录)，需要重新手动修改其所属组</span><br><span class="line">-m：移动家目录内容到新的位置，该选项只在和-d选项一起使用时才生效</span><br><span class="line">-d：修改用户的家目录位置，若不存在则自动创建。默认旧的家目录不会删除</span><br><span class="line">    如果同时指定了-m选项，则旧的家目录中的内容会移到新家目录</span><br><span class="line">    如果当前用户家目录不存在或没有家目录，则也不会创建新的家目录</span><br><span class="line">-o：允许用户使用非唯一的UID</span><br><span class="line">-s：修改用的shell，留空则选择默认shell</span><br><span class="line">-c：修改用户注释信息</span><br><span class="line"></span><br><span class="line">-a：将用户以追加的方式加入到辅助组中，只能和-G选项一起使用</span><br><span class="line">-G：将用户加入指定的辅助组中，若此处未列出某组，而此前该用户又是该组成员，则会删除该组中此成员</span><br><span class="line"></span><br><span class="line">-L：锁定用户的密码，将在/etc/shadow的密码列加上前缀&quot;!&quot;或&quot;!!&quot;</span><br><span class="line">-U：解锁用户的密码，解锁的方式是移除shadow文件密码列的前缀&quot;!&quot;或&quot;!!&quot;</span><br><span class="line">-e：帐户过期时间，时间格式为&quot;YYYY-MM-DD&quot;，如果给一个空的参数，则立即禁用该帐户</span><br><span class="line">-f：密码过期后多少天，帐户才过期被禁用，0表示密码过期帐户立即禁用，-1表示禁用该功能</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>同样，还有groupmod修改组信息，用法非常简单，几乎也用不上，不多说了。</p><h2 id="vipw和vigr"><a href="#vipw和vigr" class="headerlink" title="vipw和vigr"></a>vipw和vigr</h2><p>vipw和vigr是编辑用户和组文件的工具，vipw可以修改/etc/passwd和/etc/shadow，vigr可以修改/etc/group和/etc/gshadow，用这两个工具比较安全，在修改的时候会检查文件的一致性。</p><p>删除用户出错时，提示用户正在被进程占用。可以使用vi编辑/etc/paswd和/etc/shadow文件将该用户对应的行删除掉。也可以使用vipw和vipw -s来分别编辑/etc/paswd和/etc/shadow文件。它们的作用是一样的。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;用户和组管理命令&quot;&gt;&lt;a href=&quot;#用户和组管理命令&quot; class=&quot;headerlink&quot; title=&quot;用户和组管理命令&quot;&gt;&lt;/a&gt;用户和组管理命令&lt;/h1&gt;&lt;h2 id=&quot;useradd-和-adduser&quot;&gt;&lt;a href=&quot;#useradd-和-ad</summary>
      
    
    
    
    <category term="linux系统" scheme="https://slions.github.io/categories/linux%E7%B3%BB%E7%BB%9F/"/>
    
    
    <category term="linux" scheme="https://slions.github.io/tags/linux/"/>
    
  </entry>
  
  <entry>
    <title>linux用户与组管理（上篇）</title>
    <link href="https://slions.github.io/2021/07/30/linux%E7%94%A8%E6%88%B7%E4%B8%8E%E7%BB%84%E7%AE%A1%E7%90%86%EF%BC%88%E4%B8%8A%E7%AF%87%EF%BC%89/"/>
    <id>https://slions.github.io/2021/07/30/linux%E7%94%A8%E6%88%B7%E4%B8%8E%E7%BB%84%E7%AE%A1%E7%90%86%EF%BC%88%E4%B8%8A%E7%AF%87%EF%BC%89/</id>
    <published>2021-07-30T04:34:26.000Z</published>
    <updated>2021-08-12T11:50:42.650Z</updated>
    
    <content type="html"><![CDATA[<h1 id="基本概念"><a href="#基本概念" class="headerlink" title="基本概念"></a>基本概念</h1><p>Linux 继承了UNIX 对用户的优秀支持，其属于多用户的操作系统。在linux中用户与组都是一种身份认证资源。</p><p>每个用户都有用户名、用户的唯一编号uid、所属组及其默认的shell，可能还有密码、家目录、附属组、注释信息等。每个组也有自己的名称、组唯一编号gid。一般来说，gid和uid都是相同的，但可以根据自己的实际需求来设置。组分为主组(primary group)和辅助组(secondary group)两种，用户一定会属于某个主组，也可以同时加入多个辅助组。</p><p>在Linux中，用户按权限来分类，可以分为3类：</p><ul><li><p>超级管理员</p><p>超级管理员是最高权限者，它的uid为0，默认超级管理员用户名为root。</p></li><li><p>系统用户</p><p>由系统或程序自行建立的账户被称为系统用户，特点是他们具有某些特权但又不需要登录操作系统。他们的uid范围从201到999，centos6的uid范围是1到499，出于安全考虑，它们一般不用来登录，所以它们的shell一般是/sbin/nologin，而且大多数时候它们是没有家目录的。</p></li><li><p>普通用户</p><p>普通用户是权限受到限制的用户，默认只能执行/bin、/usr/bin、/usr/local/bin和自身家目录下的命令。它们的uid从1000开始。尽管普通用户权限收到限制，但是它对自身家目录下的文件是有所有权限的。</p></li></ul><p>默认root用户的家目录为/root，其他用户的家目录一般在/home下以用户名命名的目录中。</p><h1 id="用户管理文件"><a href="#用户管理文件" class="headerlink" title="用户管理文件"></a>用户管理文件</h1><h2 id="用户文件"><a href="#用户文件" class="headerlink" title="用户文件"></a>用户文件</h2><p><code>/etc/passwd</code>文件里记录的是操作系统中用户的信息，这里面记录了几行就表示系统中有几个系统用户。它的格式大致如下：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">[root@slions_pc1 ~]# cat /etc/passwd</span><br><span class="line">root:x:0:0:root:/root:/bin/bash</span><br><span class="line">bin:x:1:1:bin:/bin:/sbin/nologin</span><br><span class="line">daemon:x:2:2:daemon:/sbin:/sbin/nologin</span><br><span class="line">adm:x:3:4:adm:/var/adm:/sbin/nologin</span><br><span class="line">lp:x:4:7:lp:/var/spool/lpd:/sbin/nologin</span><br><span class="line">sync:x:5:0:sync:/sbin:/bin/sync</span><br><span class="line">shutdown:x:6:0:shutdown:/sbin:/sbin/shutdown</span><br><span class="line">halt:x:7:0:halt:/sbin:/sbin/halt</span><br><span class="line">mail:x:8:12:mail:/var/spool/mail:/sbin/nologin</span><br><span class="line">operator:x:11:0:operator:/root:/sbin/nologin</span><br><span class="line">games:x:12:100:games:/usr/games:/sbin/nologin</span><br><span class="line">ftp:x:14:50:FTP User:/var/ftp:/sbin/nologin</span><br><span class="line">nobody:x:99:99:Nobody:/:/sbin/nologin</span><br><span class="line">systemd-network:x:192:192:systemd Network Management:/:/sbin/nologin</span><br><span class="line">dbus:x:81:81:System message bus:/:/sbin/nologin</span><br><span class="line">polkitd:x:999:998:User for polkitd:/:/sbin/nologin</span><br><span class="line">libstoragemgmt:x:998:997:daemon account for libstoragemgmt:/var/run/lsm:/sbin/nologin</span><br><span class="line">abrt:x:173:173::/etc/abrt:/sbin/nologin</span><br><span class="line">rpc:x:32:32:Rpcbind Daemon:/var/lib/rpcbind:/sbin/nologin</span><br><span class="line">sshd:x:74:74:Privilege-separated SSH:/var/empty/sshd:/sbin/nologin</span><br><span class="line">postfix:x:89:89::/var/spool/postfix:/sbin/nologin</span><br><span class="line">ntp:x:38:38::/etc/ntp:/sbin/nologin</span><br><span class="line">chrony:x:997:995::/var/lib/chrony:/sbin/nologin</span><br><span class="line">tcpdump:x:72:72::/:/sbin/nologin</span><br></pre></td></tr></table></figure><p>/etc/passwd 内容总共分为 7 个区域 ,以“ :” 作为区域的分隔符。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">用户名:x:uid:gid:用户注释信息:家目录:使用的shell类型</span><br></pre></td></tr></table></figure><p>第一列：用户名。区分大小写，账户名可以以字母 , 数字 , 英文句号 ‘.’, 下 划线 ‘_’, 连字符 ‘-‘ 等连和使用，账户名必须唯一</p><p>第二列：x。在以前老版本的系统上，第二列是存放用户密码的，但是密码和用户信息放在一起不便于管理(密钥要保证其特殊属性)，所以后来将密码单独放在另一个文件/etc/shadow中，这里就都写成x了。</p><p>第三列：uid。UID 号应该唯一，UID 号 0-999 为保留 UID</p><p>第四列：gid。</p><p>第五列：用户注释信息。</p><p>第六列：用户家目录，普通账户主目录默认建立在 /home 下。</p><p>第七列：用户的默认shell，虽然叫shell，但其实可以是任意一个可执行程序或脚本。</p><h2 id="密码文件"><a href="#密码文件" class="headerlink" title="密码文件"></a>密码文件</h2><p><code>/etc/shadow</code>文件管理着用户的密码 , 格式大致为：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">[root@slions_pc1 ~]# cat /etc/shadow</span><br><span class="line">root:$6$JifzPmKZFWt52T4M$2s.UGoczIsSGCFKiqw/KhfWsiLKgwAHqY3dq8jLsHg4/RGZ1NoSKhmgfeEgZiPLJVWJbafyoVLiVRptCi4KIs1::0:99999:7:::</span><br><span class="line">bin:*:17834:0:99999:7:::</span><br><span class="line">daemon:*:17834:0:99999:7:::</span><br><span class="line">adm:*:17834:0:99999:7:::</span><br><span class="line">lp:*:17834:0:99999:7:::</span><br><span class="line">sync:*:17834:0:99999:7:::</span><br><span class="line">shutdown:*:17834:0:99999:7:::</span><br><span class="line">halt:*:17834:0:99999:7:::</span><br><span class="line">mail:*:17834:0:99999:7:::</span><br><span class="line">operator:*:17834:0:99999:7:::</span><br><span class="line">games:*:17834:0:99999:7:::</span><br><span class="line">ftp:*:17834:0:99999:7:::</span><br><span class="line">nobody:*:17834:0:99999:7:::</span><br><span class="line">systemd-network:!!:18768::::::</span><br><span class="line">dbus:!!:18768::::::</span><br><span class="line">polkitd:!!:18768::::::</span><br><span class="line">libstoragemgmt:!!:18768::::::</span><br><span class="line">abrt:!!:18768::::::</span><br><span class="line">rpc:!!:18768:0:99999:7:::</span><br><span class="line">sshd:!!:18768::::::</span><br><span class="line">postfix:!!:18768::::::</span><br><span class="line">ntp:!!:18768::::::</span><br><span class="line">chrony:!!:18768::::::</span><br><span class="line">tcpdump:!!:18768::::::</span><br></pre></td></tr></table></figure><p>其有 9 个区域，每个区域的作用如下 :</p><p>第一列：用户名。( 与 /etc/passwd 一致 )<br>第二列：加密后的密码。但是这一列是有玄机的，有些特殊的字符表示特殊的意义。</p><ul><li><p>①.该列留空，即”::”，表示该用户没有密码。</p></li><li><p>②.该列为”!”，即”:!:”，表示该用户被锁，被锁将无法登陆，但是可能其他的登录方式是不受限制的，如ssh key的方式，su的方式。</p></li><li><p>③.该列为”<em>”，即”:</em>:”，也表示该用户被锁，和”!”效果是一样的。</p></li><li><p>④.该列以”!”或”!!”开头，则也表示该用户被锁。</p></li><li><p>⑤.该列为”!!”，即”:!!:”，表示该用户从来没设置过密码。</p></li><li><p>⑥.如果格式为”$id$salt$hashed”，则表示该用户密码正常。其中$id$的id表示密码的加密算法，$1$表示使用MD5算法，$2a$表示使用Blowfish算法，”$2y$”是另一算法长度的Blowfish,”$5$”表示SHA-256算法，而”$6$”表示SHA-512算法，可见上面的结果中都是使用sha-512算法的。$5$和$6$这两种算法的破解难度远高于MD5。$salt$是加密时使用的salt，$hashed才是真正的密码部分。</p></li></ul><p>第三列：密码自新纪元 (1970-1-1) 起到用户前一次修 改密码的天数。<br>第四列：密码最少使用期限(天数)。密码前次与下一次修改的时间间隔 , 一般为“０”位不设定，可随时修改。<br>第五列：密码最大使用期限(天数)。超过了它不一定密码就失效，可能下一个字段设置了过期后的宽限天数。设置为空时将永不过期，后面设置的提醒和警告将失效。root等一些用户的已经默认设置为了99999，表示永不过期。如果值设置小于最短使用期限，用户将不能修改密码。<br>第六列：密码过期前多少天就开始提醒用户密码将要过期。空或0将不提醒。<br>第七列：密码过期后宽限的天数，在宽限时间内用户无法使用原密码登录，必须改密码或者联系管理员。设置为空表示没有强制的宽限时间，可以过期后的任意时间内修改密码。<br>第八列：帐号过期时间。从1970年1月1日开始计算天数。设置为空帐号将永不过期，不能设置为0。不同于密码过期，密码过期后账户还有效，改密码后还能登录；帐号过期后帐号失效，修改密码重设密码都无法使用该帐号。<br>第九列：保留字段。</p><h2 id="组文件"><a href="#组文件" class="headerlink" title="组文件"></a>组文件</h2><p><code>/etc/group</code>包含了组信息。格式大致为：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line">[root@slions_pc1 ~]# cat /etc/group</span><br><span class="line">root:x:0:</span><br><span class="line">bin:x:1:</span><br><span class="line">daemon:x:2:</span><br><span class="line">sys:x:3:</span><br><span class="line">adm:x:4:</span><br><span class="line">tty:x:5:</span><br><span class="line">disk:x:6:</span><br><span class="line">lp:x:7:</span><br><span class="line">mem:x:8:</span><br><span class="line">kmem:x:9:</span><br><span class="line">wheel:x:10:</span><br><span class="line">cdrom:x:11:</span><br><span class="line">mail:x:12:postfix</span><br><span class="line">man:x:15:</span><br><span class="line">dialout:x:18:</span><br><span class="line">floppy:x:19:</span><br><span class="line">games:x:20:</span><br><span class="line">tape:x:33:</span><br><span class="line">video:x:39:</span><br><span class="line">ftp:x:50:</span><br><span class="line">lock:x:54:</span><br><span class="line">audio:x:63:</span><br><span class="line">nobody:x:99:</span><br><span class="line">users:x:100:</span><br><span class="line">utmp:x:22:</span><br><span class="line">utempter:x:35:</span><br><span class="line">input:x:999:</span><br><span class="line">systemd-journal:x:190:</span><br><span class="line">systemd-network:x:192:</span><br><span class="line">dbus:x:81:</span><br><span class="line">polkitd:x:998:</span><br><span class="line">libstoragemgmt:x:997:</span><br><span class="line">ssh_keys:x:996:</span><br><span class="line">abrt:x:173:</span><br><span class="line">rpc:x:32:</span><br><span class="line">sshd:x:74:</span><br><span class="line">slocate:x:21:</span><br><span class="line">postdrop:x:90:</span><br><span class="line">postfix:x:89:</span><br><span class="line">ntp:x:38:</span><br><span class="line">chrony:x:995:</span><br><span class="line">tcpdump:x:72:</span><br><span class="line">stapusr:x:156:</span><br><span class="line">stapsys:x:157:</span><br><span class="line">stapdev:x:158:</span><br><span class="line">cgred:x:994:</span><br><span class="line">docker:x:993:</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>每行一个组，每一行3个冒号共4列属性：</p><p>第一列：组名。<br>第二列：占位符。<br>第三列：gid。<br>第四列：该组下的user列表，这些user成员以该组做为辅助组，多个成员使用逗号隔开。</p><h2 id="框架目录"><a href="#框架目录" class="headerlink" title="框架目录"></a>框架目录</h2><p><code>/etc/skel</code>框架目录中的文件是每次新建用户时，都会复制到新用户家目录里的文件。默认只有3个环境配置文件，可以修改这里面的内容，或者添加几个文件在骨架目录中，以后新建用户时就会自动获取到这些环境和文件。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[root@slions_pc2 ~]# ls -lA /etc/skel/</span><br><span class="line">总用量 12</span><br><span class="line">-rw-r--r--. 1 root root  18 10月 31 2018 .bash_logout</span><br><span class="line">-rw-r--r--. 1 root root 193 10月 31 2018 .bash_profile</span><br><span class="line">-rw-r--r--. 1 root root 231 10月 31 2018 .bashrc</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>删除家目录下这些文件，会导致某些设置出现问题。例如删除”.bashrc”这个文件，会导致提示符变异的问题(-bash-4.2$)。</p><h2 id="创建用户限制文件"><a href="#创建用户限制文件" class="headerlink" title="创建用户限制文件"></a>创建用户限制文件</h2><p><code>/etc/login.defs</code>设置用户帐号限制的文件。该文件里的配置对root用户无效。</p><p>如果/etc/shadow文件里有相同的选项，则以/etc/shadow里的设置为准，也就是说/etc/shadow的配置优先级高于/etc/login.defs。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br></pre></td><td class="code"><pre><span class="line">[root@slions_pc2 ~]# cat /etc/login.defs</span><br><span class="line"><span class="meta">#</span><span class="bash"></span></span><br><span class="line"><span class="bash"><span class="comment"># Please note that the parameters in this configuration file control the</span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> behavior of the tools from the shadow-utils component. None of these</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> tools uses the PAM mechanism, and the utilities that use PAM (such as the</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> passwd <span class="built_in">command</span>) should therefore be configured elsewhere. Refer to</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> /etc/pam.d/system-auth <span class="keyword">for</span> more information.</span></span><br><span class="line"><span class="meta">#</span><span class="bash"></span></span><br><span class="line"><span class="bash"></span><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> *REQUIRED*</span></span><br><span class="line"><span class="meta">#</span><span class="bash">   Directory <span class="built_in">where</span> mailboxes reside, _or_ name of file, relative to the</span></span><br><span class="line"><span class="meta">#</span><span class="bash">   home directory.  If you _do_ define both, MAIL_DIR takes precedence.</span></span><br><span class="line"><span class="meta">#</span><span class="bash">   QMAIL_DIR is <span class="keyword">for</span> Qmail</span></span><br><span class="line"><span class="meta">#</span><span class="bash"></span></span><br><span class="line"><span class="bash"><span class="comment">#QMAIL_DIR      Maildir           # QMAIL_DIR是Qmail邮件的目录，所以可以不设置它</span></span></span><br><span class="line">MAIL_DIR        /var/spool/mail   # 默认邮件根目录，即信箱</span><br><span class="line"><span class="meta">#</span><span class="bash">MAIL_FILE      .mail             <span class="comment"># mail文件的格式是.mail</span></span></span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> Password aging controls:</span></span><br><span class="line"><span class="meta">#</span><span class="bash"></span></span><br><span class="line"><span class="bash"><span class="comment">#       PASS_MAX_DAYS   Maximum number of days a password may be used.</span></span></span><br><span class="line"><span class="meta">#</span><span class="bash">       PASS_MIN_DAYS   Minimum number of days allowed between password changes.</span></span><br><span class="line"><span class="meta">#</span><span class="bash">       PASS_MIN_LEN    Minimum acceptable password length.</span></span><br><span class="line"><span class="meta">#</span><span class="bash">       PASS_WARN_AGE   Number of days warning given before a password expires.</span></span><br><span class="line"><span class="meta">#</span><span class="bash"></span></span><br><span class="line"><span class="bash"><span class="comment"># Password aging controls:</span></span></span><br><span class="line">PASS_MAX_DAYS   99999         # 密码最大有效期(天)</span><br><span class="line">PASS_MIN_DAYS   0             # 两次密码修改之间最小时间间隔</span><br><span class="line">PASS_MIN_LEN    5             # 密码最短长度</span><br><span class="line">PASS_WARN_AGE   7             # 密码过期前给警告信息的时间</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 控制useradd创建用户时自动选择的uid范围</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> Min/max values <span class="keyword">for</span> automatic uid selection <span class="keyword">in</span> useradd</span></span><br><span class="line"><span class="meta">#</span><span class="bash"></span></span><br><span class="line"><span class="bash">UID_MIN                  1000</span></span><br><span class="line">UID_MAX                 60000</span><br><span class="line"><span class="meta">#</span><span class="bash"> System accounts</span></span><br><span class="line">SYS_UID_MIN               201</span><br><span class="line">SYS_UID_MAX               999</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 控制groupadd创建组时自动选择的gid范围</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> Min/max values <span class="keyword">for</span> automatic gid selection <span class="keyword">in</span> groupadd</span></span><br><span class="line"><span class="meta">#</span><span class="bash"></span></span><br><span class="line"><span class="bash">GID_MIN                  1000</span></span><br><span class="line">GID_MAX                 60000</span><br><span class="line"><span class="meta">#</span><span class="bash"> System accounts</span></span><br><span class="line">SYS_GID_MIN               201</span><br><span class="line">SYS_GID_MAX               999</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"></span></span><br><span class="line"><span class="bash"><span class="comment"># If defined, this command is run when removing a user.</span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> It should remove any at/cron/<span class="built_in">print</span> <span class="built_in">jobs</span> etc. owned by</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> the user to be removed (passed as the first argument).</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 设置此项后，在删除用户时，将自动删除用户拥有的at/cron/<span class="built_in">print</span>等job</span></span><br><span class="line"><span class="meta">#</span><span class="bash">USERDEL_CMD    /usr/sbin/userdel_local</span></span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"></span></span><br><span class="line"><span class="bash"><span class="comment"># If useradd should create home directories for users by default</span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> On RH systems, we <span class="keyword">do</span>. This option is overridden with the -m flag on</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> useradd <span class="built_in">command</span> line.</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 控制useradd添加用户时是否默认创建家目录，useradd -m选项会覆盖此处设置</span></span><br><span class="line">CREATE_HOME     yes</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> The permission mask is initialized to this value. If not specified,</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> the permission mask will be initialized to 022.</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 设置创建家目录时的<span class="built_in">umask</span>值，若不指定则默认为022</span></span><br><span class="line">UMASK           077</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> This enables userdel to remove user groups <span class="keyword">if</span> no members exist.</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 设置此项表示当组中没有成员时自动删除该组</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 且useradd是否同时创建同用户名的主组。</span></span><br><span class="line">USERGROUPS_ENAB yes</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> Use SHA512 to encrypt password.</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 设置用户和组密码的加密算法</span></span><br><span class="line">ENCRYPT_METHOD SHA512</span><br></pre></td></tr></table></figure><p>/etc/login.defs中的设置控制的是shadow-utils包中的组件，也就是说，该组件中的工具执行操作时会读取该文件中的配置。该组件中包含下面的程序：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">/usr/bin/gpasswd      ：administer /etc/group and /etc/gshadow</span><br><span class="line">/usr/bin/newgrp       ：log in to a new group，可用来修改gid，哪怕是正在登陆的会话也可以修改</span><br><span class="line">/usr/bin/sg           ：execute command as different group ID</span><br><span class="line">/usr/sbin/groupadd    ：添加组</span><br><span class="line">/usr/sbin/groupdel    ：删除组</span><br><span class="line">/usr/sbin/groupmems   ：管理当前用户的主组中的成员，root用户则可以指定要管理的组</span><br><span class="line">/usr/sbin/groupmod    ：modify a group definition on the system</span><br><span class="line">/usr/sbin/grpck       ：verify integrity of group files</span><br><span class="line">/usr/sbin/grpconv     ：无视它</span><br><span class="line">/usr/sbin/grpunconv   ：无视它</span><br><span class="line">/usr/sbin/pwconv      ：无视它</span><br><span class="line">/usr/sbin/pwunconv    ：无视它</span><br><span class="line">/usr/sbin/adduser     ：是useradd的一个软链接，添加用户</span><br><span class="line">/usr/sbin/chpasswd    ：update passwords in batch mode</span><br><span class="line">/usr/sbin/newusers    ：update and create new users in batch</span><br><span class="line">/usr/sbin/pwck        ：verify integrity of passsword files</span><br><span class="line">/usr/sbin/useradd     ：添加用户</span><br><span class="line">/usr/sbin/userdel     ：删除用户</span><br><span class="line">/usr/sbin/usermod     ：重定义用户信息</span><br><span class="line">/usr/sbin/vigr        ：edit the group and shadow-group file</span><br><span class="line">/usr/sbin/vipw        ：edit the password and shadow-password file</span><br><span class="line">/usr/bin/lastlog      ：输出所有用户或给定用户最近登录信息</span><br><span class="line"></span><br></pre></td></tr></table></figure><h2 id="创建用户时默认配置文件"><a href="#创建用户时默认配置文件" class="headerlink" title="创建用户时默认配置文件"></a>创建用户时默认配置文件</h2><p><code>/etc/default/useradd</code>。useradd -D修改的就是此文件。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">[root@slions_pc1 ~]# cat /etc/default/useradd</span><br><span class="line">[root@xuexi ~]# cat /etc/default/useradd  </span><br><span class="line"><span class="meta">#</span><span class="bash"> useradd defaults file</span></span><br><span class="line">GROUP=100       # 在useradd使用-N或/etc/login.defs中USERGROUPS_ENAB=no时表示创建</span><br><span class="line">                # 用户时不创建同用户名的主组(primary group)，此时新建的用户将默认以</span><br><span class="line">                # 此组为主组，网上关于该设置的很多说明都是错的，具体可看man useradd</span><br><span class="line">                # 的-g选项或useradd -D的-g选项</span><br><span class="line">HOME=/home      # 把用户的家目录建在/home中</span><br><span class="line">INACTIVE=-1     # 是否启用帐号过期设置(是帐号过期不是密码过期)，-1表示不启用</span><br><span class="line">EXPIRE=         # 帐号过期时间，不设置表示不启用</span><br><span class="line">SHELL=/bin/bash # 新建用户默认的shell类型</span><br><span class="line">SKEL=/etc/skel  # 指定框架目录，前文的/etc/skel就在这里</span><br><span class="line">CREATE_MAIL_SPOOL=yes  # 是否创建用户mail缓冲</span><br><span class="line"></span><br></pre></td></tr></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;基本概念&quot;&gt;&lt;a href=&quot;#基本概念&quot; class=&quot;headerlink&quot; title=&quot;基本概念&quot;&gt;&lt;/a&gt;基本概念&lt;/h1&gt;&lt;p&gt;Linux 继承了UNIX 对用户的优秀支持，其属于多用户的操作系统。在linux中用户与组都是一种身份认证资源。&lt;/p&gt;
</summary>
      
    
    
    
    <category term="linux系统" scheme="https://slions.github.io/categories/linux%E7%B3%BB%E7%BB%9F/"/>
    
    
    <category term="linux" scheme="https://slions.github.io/tags/linux/"/>
    
  </entry>
  
  <entry>
    <title>harbor集群系统负载升高原因分析</title>
    <link href="https://slions.github.io/2021/07/19/harbor%E9%9B%86%E7%BE%A4%E7%B3%BB%E7%BB%9F%E8%B4%9F%E8%BD%BD%E5%8D%87%E9%AB%98%E5%8E%9F%E5%9B%A0%E5%88%86%E6%9E%90/"/>
    <id>https://slions.github.io/2021/07/19/harbor%E9%9B%86%E7%BE%A4%E7%B3%BB%E7%BB%9F%E8%B4%9F%E8%BD%BD%E5%8D%87%E9%AB%98%E5%8E%9F%E5%9B%A0%E5%88%86%E6%9E%90/</id>
    <published>2021-07-19T10:07:15.000Z</published>
    <updated>2021-08-12T11:51:04.060Z</updated>
    
    <content type="html"><![CDATA[<h1 id="问题现象"><a href="#问题现象" class="headerlink" title="问题现象"></a>问题现象</h1><p>接到客户反馈，说是创建不了新的服务了，查看相关的Event日志，发现此服务拉取不了images，后台查看harbor节点的状态，发现问题，harbor集群的VIP丢失，并且在主机上执行命令都比较卡，top查看平均负载发现非常高。</p><p><img src="/doc_picture/harbor-1.png" alt="image-20210719181131471"></p><h1 id="问题分析"><a href="#问题分析" class="headerlink" title="问题分析"></a>问题分析</h1><p>首先我们需要排查什么原因导致harbor节点这个卡顿，负载一直升高。</p><blockquote><p>平均负载是指单位时间内，系统处于可运行状态和不可中断状态的平均进程数，也就是平均活跃进程数。所以，它不仅包括了正在使用CPU的进程，还包括等待CPU和等待I/O的进程。</p></blockquote><p>系统平均负载升高的原因主要有三种：</p><ul><li>CPU密集型进程，使用大量CPU会导致平均负载升高，此时CPU使用率跟平均负载是一致的；</li><li>I/O密集型进程，等待I/O也会导致平均负载升高，但CPU使用率不一定很高；</li><li>大量等待CPU的进程调度也会导致平均负载升高，此时CPU使用率也会比较高。</li></ul><p>首先sar -u 观察CPU情况。</p><p><img src="/doc_picture/harbor-2.png" alt="image-20210719182405728"></p><p>cpu使用率非常低，大部分为idle，说明没有进程在等待cpu资源。</p><p>sar -b 观察IO情况 IO设备的读写tps都几乎为0。</p><p><img src="/doc_picture/harbor-3.png" alt="image-20210719182429006"></p><p>发现并不存在CPU/IO密集型的进程后，执行了下df操作，发现命令hang死，另外开一个终端，通过strace去分析df命令的系统调用及信号情况，可以明显发现df是在系统调用尝试获取目录/harborimages的stat信息时挂起。</p><p><img src="/doc_picture/harbor-4.png" alt="image-20210719182846933"></p><p>通过ps aux抓取系统运行的df进程信息（状态为D+(无法中断的休眠状态)）：</p><p><img src="/doc_picture/harbor-5.png" alt="image-20210719182903747"></p><p>通过ps aux查看内存和cpu占用最多的5个进程发现了问题，没有cpu占用特别大的进程，但是有一个状态为Dl的进程（无法中断的休眠状态/多线程，克隆线程）</p><p><img src="/doc_picture/harbor-6.png" alt="image-20210719182922163"></p><p>我们再查看下此进程的线程状态：</p><p><img src="/doc_picture/harbor-7.png" alt="image-20210719182944057"></p><p>发现有一堆不可中断的线程，而且越来越多，cpu使用率都为0，此时可以定位问题了，有大量进程读写请求一直获取不到资源，从而进程一直是不可中断状态。造成负载很高。平均负载升高导致机器性能降低，内部的keepalived服务心跳机制超时，最后使得VIP丢失。</p><p><img src="/doc_picture/harbor-8.png" alt="image-20210719184236890"></p><p>registry是harbor中负责存储镜像文件的组件，同时负责处理镜像的pull/push命令。此服务会将宿主机的/harborimages作为挂载目录，为了保障可用性我们后端使用了客户现成的nas来挂载到了/harborimages目录，之前执行df hang住的地方正是这个目录。 又在本地创了个测试目录，使用之前的nas地址挂载发现异常。此时回想起来，前一天客户发通知说要进行xxx区机器网络进行调整，大概率是由于这个引起了。联系客户方运维协助排查，最后发现nas服务器因为重启过导致防火墙开启了。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;问题现象&quot;&gt;&lt;a href=&quot;#问题现象&quot; class=&quot;headerlink&quot; title=&quot;问题现象&quot;&gt;&lt;/a&gt;问题现象&lt;/h1&gt;&lt;p&gt;接到客户反馈，说是创建不了新的服务了，查看相关的Event日志，发现此服务拉取不了images，后台查看harbor节点的状</summary>
      
    
    
    
    <category term="镜像仓库" scheme="https://slions.github.io/categories/%E9%95%9C%E5%83%8F%E4%BB%93%E5%BA%93/"/>
    
    
    <category term="harbor" scheme="https://slions.github.io/tags/harbor/"/>
    
  </entry>
  
  <entry>
    <title>glusterfs可用性测试</title>
    <link href="https://slions.github.io/2021/07/18/glusterfs%E5%8F%AF%E7%94%A8%E6%80%A7%E6%B5%8B%E8%AF%95/"/>
    <id>https://slions.github.io/2021/07/18/glusterfs%E5%8F%AF%E7%94%A8%E6%80%A7%E6%B5%8B%E8%AF%95/</id>
    <published>2021-07-18T10:58:16.000Z</published>
    <updated>2021-08-12T11:51:16.613Z</updated>
    
    <content type="html"><![CDATA[<h1 id="环境描述"><a href="#环境描述" class="headerlink" title="环境描述"></a>环境描述</h1><table><thead><tr><th><strong>主机名</strong></th><th>ip地址</th><th><strong>系统环境</strong></th><th><strong>角色</strong></th></tr></thead><tbody><tr><td>k8s-1</td><td>IP:192.168.186.10</td><td>cpu:x2  mem:2GB  disk:/dev/sdb 40GB</td><td>K8s_master,Gluster_master,Heketi_master</td></tr><tr><td>K8s-2</td><td>IP:192.168.186.11</td><td>cpu:x2  mem:2GB  disk:/dev/sdb 40GB</td><td>K8s_node,Gluster_node</td></tr><tr><td>K8s-3</td><td>IP:192.168.186.12</td><td>cpu:x2  mem:2GB  disk:/dev/sdb 40GB</td><td>K8s_master,Gluster_node</td></tr></tbody></table><h1 id="集群扩容"><a href="#集群扩容" class="headerlink" title="集群扩容"></a>集群扩容</h1><h2 id="添加新磁盘"><a href="#添加新磁盘" class="headerlink" title="添加新磁盘"></a>添加新磁盘</h2><blockquote><p>添加设备时，请记住将设备添加为一组。例如，如果创建的卷使用副本为2，则应将device添加到两个节点（每个节点一个device）。如果使用副本3，则将device添加到三个节点。</p></blockquote><h3 id="命令行方式"><a href="#命令行方式" class="headerlink" title="命令行方式"></a>命令行方式</h3><p>假设在k8s-3上增加磁盘，查看k8s-3部署的pod name及IP：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-1 ~]# kubectl get po -o wide -l glusterfs-node</span><br><span class="line">NAME             READY      STATUS    RESTARTS   AGE           IP            NODE</span><br><span class="line">glusterfs-5npwn   1/1       Running   0          20h       192.168.186.10   k8s-1</span><br><span class="line">glusterfs-8zfzq   1/1       Running   0          20h       192.168.186.11   k8s-2</span><br><span class="line">glusterfs-bd5dx   1/1       Running   0          20h       192.168.186.12   k8s-3</span><br></pre></td></tr></table></figure><p>在k8s-3上确认新添加的盘符：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">Disk /dev/sdc: 42.9 GB, 42949672960 bytes, 83886080 sectors</span><br><span class="line">Units = sectors of 1 * 512 = 512 bytes</span><br><span class="line">Sector size (logical/physical): 512 bytes / 512 bytes</span><br><span class="line">I/O size (minimum/optimal): 512 bytes / 512 bytes</span><br></pre></td></tr></table></figure><p>使用heketi-cli查看cluster ID和所有node ID：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-1 ~]# heketi-cli cluster list</span><br><span class="line">Clusters:</span><br><span class="line">Id:5dec5676c731498c2bdf996e110a3e5e [file][block]</span><br><span class="line">[root@k8s-1 ~]# heketi-cli cluster info 5dec5676c731498c2bdf996e110a3e5e</span><br><span class="line">Cluster id: 5dec5676c731498c2bdf996e110a3e5e</span><br><span class="line">Nodes:</span><br><span class="line">0f00835397868d3591f45432e432ba38</span><br><span class="line">d38819746cab7d567ba5f5f4fea45d91</span><br><span class="line">fb181b0cef571e9af7d84d2ecf534585</span><br><span class="line">Volumes:</span><br><span class="line">32146a51be9f980c14bc86c34f67ebd5</span><br><span class="line">56d636b452d31a9d4cb523d752ad0891</span><br><span class="line">828dc2dfaa00b7213e831b91c6213ae4</span><br><span class="line">b9c68075c6f20438b46db892d15ed45a</span><br><span class="line">Block: true</span><br><span class="line">File: true</span><br></pre></td></tr></table></figure><p>找到对应的k8s-3的node ID：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-1 ~]# heketi-cli node info 0f00835397868d3591f45432e432ba38</span><br><span class="line">Node Id: 0f00835397868d3591f45432e432ba38</span><br><span class="line">State: online</span><br><span class="line">Cluster Id: 5dec5676c731498c2bdf996e110a3e5e</span><br><span class="line">Zone: 1</span><br><span class="line">Management Hostname: k8s-node02</span><br><span class="line">Storage Hostname: 192.168.186.12</span><br><span class="line">Devices:</span><br><span class="line">Id:82af8e5f2fb2e1396f7c9e9f7698a178   Name:/dev/sdb            State:online    Size (GiB):39      Used (GiB):25      Free (GiB):14      Bricks:4</span><br></pre></td></tr></table></figure><p>添加磁盘至GFS集群的k8s-3：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-1 ~]# heketi-cli device add --name=/dev/sdc --node=0f00835397868d3591f45432e432ba38</span><br><span class="line">Device added successfully</span><br></pre></td></tr></table></figure><p>查看结果：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-1 ~]# heketi-cli node info 0f00835397868d3591f45432e432ba38</span><br><span class="line">Node Id: 0f00835397868d3591f45432e432ba38</span><br><span class="line">State: online</span><br><span class="line">Cluster Id: 5dec5676c731498c2bdf996e110a3e5e</span><br><span class="line">Zone: 1</span><br><span class="line">Management Hostname: k8s-3</span><br><span class="line">Storage Hostname: 192.168.186.12</span><br><span class="line">Devices:</span><br><span class="line">Id:5539e74bc2955e7c70b3a20e72c04615   Name:/dev/sdc            State:online    Size (GiB):39      Used (GiB):0       Free (GiB):39      Bricks:0       </span><br><span class="line">Id:82af8e5f2fb2e1396f7c9e9f7698a178   Name:/dev/sdb            State:online    Size (GiB):39      Used (GiB):25      Free (GiB):14      Bricks:4</span><br></pre></td></tr></table></figure><h3 id="拓扑文件方式"><a href="#拓扑文件方式" class="headerlink" title="拓扑文件方式"></a>拓扑文件方式</h3><p>当一次添加多个设备的一种更简单的方法是将新设备添加到用于设置群集的拓扑文件(topology.json)中的节点描述中。然后重新运行该命令以加载新拓扑。下面是我们向节点添加新的/dev/sdc磁盘的示例：</p><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br></pre></td><td class="code"><pre><span class="line">cat &gt;topology.json&lt;&lt;EOF</span><br><span class="line">&#123;</span><br><span class="line">  <span class="attr">&quot;clusters&quot;</span>: [</span><br><span class="line">    &#123;</span><br><span class="line">      <span class="attr">&quot;nodes&quot;</span>: [</span><br><span class="line">        &#123;</span><br><span class="line">          <span class="attr">&quot;node&quot;</span>: &#123;</span><br><span class="line">            <span class="attr">&quot;hostnames&quot;</span>: &#123;</span><br><span class="line">              <span class="attr">&quot;manage&quot;</span>: [</span><br><span class="line">                <span class="string">&quot;k8s-1&quot;</span></span><br><span class="line">              ],</span><br><span class="line">              <span class="attr">&quot;storage&quot;</span>: [</span><br><span class="line">                <span class="string">&quot;192.168.186.10&quot;</span></span><br><span class="line">              ]</span><br><span class="line">            &#125;,</span><br><span class="line">            <span class="attr">&quot;zone&quot;</span>: <span class="number">1</span></span><br><span class="line">          &#125;,</span><br><span class="line">          <span class="attr">&quot;devices&quot;</span>: [</span><br><span class="line">            <span class="string">&quot;/dev/sdb&quot;</span>,</span><br><span class="line">            <span class="string">&quot;/dev/sdc&quot;</span></span><br><span class="line">          ]</span><br><span class="line">        &#125;,</span><br><span class="line">        &#123;</span><br><span class="line">          <span class="attr">&quot;node&quot;</span>: &#123;</span><br><span class="line">            <span class="attr">&quot;hostnames&quot;</span>: &#123;</span><br><span class="line">              <span class="attr">&quot;manage&quot;</span>: [</span><br><span class="line">                <span class="string">&quot;k8s-2&quot;</span></span><br><span class="line">              ],</span><br><span class="line">              <span class="attr">&quot;storage&quot;</span>: [</span><br><span class="line">                <span class="string">&quot;192.168.186.11&quot;</span></span><br><span class="line">              ]</span><br><span class="line">            &#125;,</span><br><span class="line">            <span class="attr">&quot;zone&quot;</span>: <span class="number">1</span></span><br><span class="line">          &#125;,</span><br><span class="line">          <span class="attr">&quot;devices&quot;</span>: [</span><br><span class="line">            <span class="string">&quot;/dev/sdb&quot;</span>,</span><br><span class="line">            <span class="string">&quot;/dev/sdc&quot;</span></span><br><span class="line">          ]</span><br><span class="line">        &#125;,</span><br><span class="line">        &#123;</span><br><span class="line">          <span class="attr">&quot;node&quot;</span>: &#123;</span><br><span class="line">            <span class="attr">&quot;hostnames&quot;</span>: &#123;</span><br><span class="line">              <span class="attr">&quot;manage&quot;</span>: [</span><br><span class="line">                <span class="string">&quot;k8s-3&quot;</span></span><br><span class="line">              ],</span><br><span class="line">              <span class="attr">&quot;storage&quot;</span>: [</span><br><span class="line">                <span class="string">&quot;192.168.186.12&quot;</span></span><br><span class="line">              ]</span><br><span class="line">            &#125;,</span><br><span class="line">            <span class="attr">&quot;zone&quot;</span>: <span class="number">1</span></span><br><span class="line">          &#125;,</span><br><span class="line">          <span class="attr">&quot;devices&quot;</span>: [</span><br><span class="line">            <span class="string">&quot;/dev/sdb&quot;</span>,</span><br><span class="line">            <span class="string">&quot;/dev/sdc&quot;</span></span><br><span class="line">          ]                                                                                    </span><br><span class="line">        &#125;</span><br><span class="line">      ]</span><br><span class="line">    &#125;</span><br><span class="line">  ]</span><br><span class="line">&#125;</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure><p>heketi加载拓扑配置：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> heketi-cli topology load --json=topology.json</span></span><br><span class="line">   Creating cluster ... ID: 224a5a6555fa5c0c930691111c63e863</span><br><span class="line">     Allowing file volumes on cluster.</span><br><span class="line">     Allowing block volumes on cluster.</span><br><span class="line">     Creating node 192.168.186.10 ... ID: 7946b917b91a579c619ba51d9129aeb0</span><br><span class="line">Found device /dev/sdb</span><br><span class="line">Adding device /dev/sdc ... OK</span><br><span class="line">     Creating node 192.168.186.11 ... ID: 5d10e593e89c7c61f8712964387f959c</span><br><span class="line">Found device /dev/sdb</span><br><span class="line">            Adding device /dev/sdc ... OK</span><br><span class="line">     Creating node 192.168.186.12 ... ID: de620cb2c313a5461d5e0a6ae234c553</span><br><span class="line">Found device /dev/sdb</span><br><span class="line">            Adding device /dev/sdc ... OK</span><br></pre></td></tr></table></figure><h2 id="添加新节点"><a href="#添加新节点" class="headerlink" title="添加新节点"></a>添加新节点</h2><p>假设将k8s-4，IP为192.168.186.13的加入glusterfs集群，并将该节点的/dev/sdb,/dev/sdc加入到集群。</p><p>先给node加标签，之后会自动创建pod：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-1 kubernetes]# kubectl label node k8s-4 storagenode=glusterfs</span><br><span class="line">node/k8s-4 labeled</span><br><span class="line"></span><br><span class="line">[root@k8s-1 kubernetes]# kubectl  get pod -o wide -l glusterfs-node</span><br><span class="line">NAME        READY     STATUS        RESTARTS   AGE       IP          NODE</span><br><span class="line">glusterfs-5npwn   1/1     Running     0     21h       192.168.186.11        k8s-2</span><br><span class="line">glusterfs-8zfzq   1/1       Running      0    21h      192.168.186.10        k8s-1</span><br><span class="line">glusterfs-96w74   0/1  ContainerCreating   0   2m     192.168.186.13         k8s-4</span><br><span class="line">glusterfs-bd5dx   1/1     Running       0      21h       192.168.186.12     k8s-3</span><br></pre></td></tr></table></figure><p>进入任意节点的gfs服务容器执行peer probe，加入新节点：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-1 kubernetes]# kubectl exec -ti glusterfs-5npwn -- gluster peer probe 192.168.186.13</span><br><span class="line">peer probe: success.</span><br></pre></td></tr></table></figure><p>将新节点纳入heketi数据库统一管理：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-1 kubernetes]# heketi-cli cluster list</span><br><span class="line">Clusters:</span><br><span class="line">Id:5dec5676c731498c2bdf996e110a3e5e [file][block]</span><br><span class="line"></span><br><span class="line">[root@k8s-1 kubernetes]# heketi-cli node add --zone=1 --cluster=5dec5676c731498c2bdf996e110a3e5e --management-host-name=k8s-4 --storage-host-name=192.168.186.13</span><br><span class="line">Node information:</span><br><span class="line">Id: 150bc8c458a70310c6137e840619758c</span><br><span class="line">State: online</span><br><span class="line">Cluster Id: 5dec5676c731498c2bdf996e110a3e5e</span><br><span class="line">Zone: 1</span><br><span class="line">Management Hostname k8s-4</span><br><span class="line">Storage Hostname 192.168.186.13</span><br></pre></td></tr></table></figure><p>将新节点的磁盘加入到集群中，参考上面的两种方式之一即可。</p><h1 id="存储卷扩容"><a href="#存储卷扩容" class="headerlink" title="存储卷扩容"></a>存储卷扩容</h1><h2 id="扩容"><a href="#扩容" class="headerlink" title="扩容"></a>扩容</h2><p>扩容volume可使用命令（单位为G）：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> heketi-cli volume expand --volume=volumeID --expand-size=10</span></span><br></pre></td></tr></table></figure><h1 id="集群缩容"><a href="#集群缩容" class="headerlink" title="集群缩容"></a>集群缩容</h1><p>Heketi也支持降低存储容量。这可以通过删除device，节点和集群来实现。可以使用API或使用heketi-cli执行这些更改。</p><blockquote><p> heketi删除device的前提是device没有被使用（Used为0）</p></blockquote><p>以下是如何从Heketi删除没有device被使用的命令：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> heketi-cli topology info</span></span><br><span class="line">Cluster Id: 6fe4dcffb9e077007db17f737ed999fe </span><br><span class="line">Volumes:</span><br><span class="line"></span><br><span class="line">    Nodes:</span><br><span class="line">  </span><br><span class="line">        Node Id: 61d019bb0f717e04ecddfefa5555bc41</span><br><span class="line">        State: online</span><br><span class="line">        Cluster Id: 6fe4dcffb9e077007db17f737ed999fe</span><br><span class="line">        Zone: 1</span><br><span class="line">        Management Hostname: k8s-3</span><br><span class="line">        Storage Hostname: 192.168.186.12</span><br><span class="line">        Devices:</span><br><span class="line">                Id:e4805400ffa45d6da503da19b26baad6   Name:/dev/sdb            State:online    Size (GiB):40     Used (GiB):0       Free (GiB):40</span><br><span class="line">                        Bricks:</span><br><span class="line">                Id:ecc3c65e4d22abf3980deba4ae90238c   Name:/dev/sdc            State:online    Size (GiB):40     Used (GiB):0       Free (GiB):40</span><br><span class="line">                        Bricks:</span><br><span class="line">  </span><br><span class="line">        Node Id: e97d77d0191c26089376c78202ee2f20</span><br><span class="line">        State: online</span><br><span class="line">        Cluster Id: 6fe4dcffb9e077007db17f737ed999fe</span><br><span class="line">        Zone: 2</span><br><span class="line">        Management Hostname: k8s-4</span><br><span class="line">        Storage Hostname: 192.168.186.13</span><br><span class="line">        Devices:</span><br><span class="line">                Id:3dc3b3f0dfd749e8dc4ee98ed2cc4141   Name:/dev/sdb            State:online    Size (GiB):40     Used (GiB):0       Free (GiB):40</span><br><span class="line">                        Bricks:</span><br><span class="line">                Id:4122bdbbe28017944a44e42b06755b1c   Name:/dev/sdc            State:online    Size (GiB):40     Used (GiB):0       Free (GiB)40</span><br><span class="line">                        Bricks:</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> d=`heketi-cli topology info | grep Size | awk <span class="string">&#x27;&#123;print $1&#125;&#x27;</span> | cut -d: -f 2`</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> <span class="keyword">for</span> i <span class="keyword">in</span> <span class="variable">$d</span> ; <span class="keyword">do</span></span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash"> heketi-cli device delete <span class="variable">$i</span></span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash"> <span class="keyword">done</span></span></span><br><span class="line">Device e4805400ffa45d6da503da19b26baad6 deleted</span><br><span class="line">Device ecc3c65e4d22abf3980deba4ae90238c deleted</span><br><span class="line">Device 3dc3b3f0dfd749e8dc4ee98ed2cc4141 deleted</span><br><span class="line">Device 4122bdbbe28017944a44e42b06755b1c deleted</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> heketi-cli node delete <span class="variable">$node1</span></span></span><br><span class="line">Node 61d019bb0f717e04ecddfefa5555bc41 deleted</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> heketi-cli node delete <span class="variable">$node2</span></span></span><br><span class="line">Node e97d77d0191c26089376c78202ee2f20 deleted</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> heketi-cli cluster delete <span class="variable">$cluster</span></span></span><br><span class="line">Cluster 6fe4dcffb9e077007db17f737ed999fe deleted</span><br></pre></td></tr></table></figure><h1 id="可用性测试"><a href="#可用性测试" class="headerlink" title="可用性测试"></a>可用性测试</h1><h2 id="添加节点"><a href="#添加节点" class="headerlink" title="添加节点"></a>添加节点</h2><p>通过restful api添加一台glusterfs主机，可以正常使用，前提是在添加之前要在新节点安装好glusterfs和lvm，加载 dm_thin_pool 模块，开启相关端口，给新节点打glusterfs的tag(daemonset用)，集群内节点可以互相解析域名。</p><h2 id="关闭节点"><a href="#关闭节点" class="headerlink" title="关闭节点"></a>关闭节点</h2><p>测试高可用中的坑。</p><p>三个glusterfs节点，关闭一台，客户端可读可写。</p><p>三个glusterfs节点，关闭两台，客户端可读不可写。</p><p>关闭虚机后发现heketi中还是显示节点在线，bricks不会同步到新增的虚机上，尝试把那台关了的机器剔除，发现bricks同步到了新增的节点。具体操作如下：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">[root@redhat1 /]# heketi-cli -s http://redhat1:30080 node list</span><br><span class="line">Id:05ac57499e3fbc0f8ec5a3301fac92c7Cluster:43c28e6f4b4e05f58ebd3b6f158982cd</span><br><span class="line">Id:5e10a4c264a2fc13ecdf8d2482ba7281Cluster:43c28e6f4b4e05f58ebd3b6f158982cd</span><br><span class="line">Id:b1e3ba52f6e8c82e8b3e512798348876Cluster:43c28e6f4b4e05f58ebd3b6f158982cd</span><br><span class="line">Id:f1bd8cd0f52b2d91b13e79799a34c2edCluster:43c28e6f4b4e05f58ebd3b6f158982cd</span><br><span class="line">f1bd8cd0f52b2d91b13e79799a34c2ed为已关闭的节点</span><br><span class="line">5e10a4c264a2fc13ecdf8d2482ba7281为新添加的节点</span><br><span class="line"> </span><br><span class="line">[root@redhat1 /]# heketi-cli -s http://redhat1:30080 node disable f1bd8cd0f52b2d91b13e79799a34c2ed</span><br><span class="line">Node f1bd8cd0f52b2d91b13e79799a34c2ed is now offline</span><br><span class="line"></span><br><span class="line">[root@redhat1 /]#heketi-cli-s http://redhat1:30080 node remove f1bd8cd0f52b2d91b13e79799a34c2ed</span><br><span class="line">Node f1bd8cd0f52b2d91b13e79799a34c2ed is now removed</span><br><span class="line"></span><br><span class="line">[root@redhat1 /]#heketi-cli -s http://redhat1:30080 device delete ff257d2350f05f7f5ebaa2853e5815e8</span><br><span class="line">Error: Failed to delete device /dev/sdb with id ff257d2350f05f7f5ebaa2853e5815e8 on host redhat3: error dialing backend: dial tcp 192.168.186.12:10250: connect: no route to host</span><br><span class="line"></span><br><span class="line">[root@redhat1 /]# heketi-cli -s http://redhat1:30080 node delete f1bd8cd0f52b2d91b13e79799a34c2ed</span><br><span class="line">Error: Unable to delete node [f1bd8cd0f52b2d91b13e79799a34c2ed] because it contains devices</span><br></pre></td></tr></table></figure><p>报错是因为关机了连不上节点，此时再查看关闭的哪个节点上bricks已经没了，同步到了新增加的节点上。</p><blockquote><p> 在旧节点从群集中完全清除之前，新增的节点不能和原先关闭的节点共用同样的标识。</p></blockquote><h2 id="硬盘损坏"><a href="#硬盘损坏" class="headerlink" title="硬盘损坏"></a>硬盘损坏</h2><p>本地三台gfs节点，每台挂载一块裸盘，把其中一块盘给删除模拟磁盘损坏，此时heketi中还能看到device，并且正在使用，登录那台节点执行partprobe更新下磁盘后发现lv和vg没有了，pv会存在残留数据，重启此节点后残留数据消失，heketi端还显示device正在使用。</p><p>手动删除device报以下错误</p><p>Error: Failed to remove device, error: No Replacement was found for resource</p><p>此错误是因为存储设备当前没有达到副本数要求的三个。</p><p><strong>解决方案：</strong></p><p>添加device后删除原先的device即可。</p><h2 id="brick损坏"><a href="#brick损坏" class="headerlink" title="brick损坏"></a>brick损坏</h2><p>手动删除gfs节点的一个brick:</p><p><img src="/doc_picture/gfs-test-1.png" alt="image-20210718192918628"></p><p>查看当前volume的状态，此时brick显示离线:</p><p><img src="/doc_picture/gfs-test-2.png" alt="image-20210718192931819"></p><p><strong>解决方案：</strong></p><p>先从volume端删除此brick</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> gluster volume remove-brick &lt;volume-name&gt;  replica &lt;count&gt; force</span></span><br></pre></td></tr></table></figure><blockquote><p>replica  2参数，开始我们创建卷时复制数是3，现在变为2。</p></blockquote><p><img src="/doc_picture/gfs-test-3.jpg" alt="img"> </p><p>此时查看volume状态，brick已经删除，volume变为2副本</p><p><img src="/doc_picture/gfs-test-4.jpg" alt="img"> </p><p>重新添加回此brick:</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> gluster volume add-brick &lt;volume-name&gt;  replica &lt;count&gt; force</span></span><br></pre></td></tr></table></figure><p><img src="/doc_picture/gfs-test-5.jpg" alt="img"> </p><p>查看volume状态，brick已经添加回来了</p><p><img src="/doc_picture/gfs-test-6.jpg" alt="img"> </p><p>查看此brick中数据已恢复。</p><p>上述方法是模拟其中一个brick故障，如果此卷可以重启的话可以快速重启尝试恢复。</p><h2 id="创建大于剩余空间的卷"><a href="#创建大于剩余空间的卷" class="headerlink" title="创建大于剩余空间的卷"></a>创建大于剩余空间的卷</h2><p>创建不出来，报错</p><p><img src="/doc_picture/gfs-test-7.png" alt="image-20210718193341163"></p><h2 id="扩容volume"><a href="#扩容volume" class="headerlink" title="扩容volume"></a>扩容volume</h2><p>提前申请一个1g大小的pvc并且挂载到应用服务，写一些数据，然后将应用服务停掉，扩容pvc到2G大小，再把应用服务开启，测试写，因为扩容volume想当于是在原先子卷的情况下又加了一个子卷，但是就算是新加入的子卷有剩余空间，glusterfs的hash寻址机制也会一直读写老的子卷（找之前的文件hash值），尝试给卷做rebalance操作来触发数据均衡操作，（扩容后会自动进行rebalance），没有效果。</p><p>创建存储盘使应提前预估好使用量大小。</p><p><img src="/doc_picture/gfs-test-8.png" alt="image-20210718193421419"></p><p><img src="/doc_picture/gfs-test-9.png" alt="image-20210718193430193"></p><p><img src="/doc_picture/gfs-test-10.png" alt="image-20210718193443985"></p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;环境描述&quot;&gt;&lt;a href=&quot;#环境描述&quot; class=&quot;headerlink&quot; title=&quot;环境描述&quot;&gt;&lt;/a&gt;环境描述&lt;/h1&gt;&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;&lt;strong&gt;主机名&lt;/strong&gt;&lt;/th&gt;
&lt;th&gt;ip地址&lt;/th&gt;
</summary>
      
    
    
    
    <category term="存储类" scheme="https://slions.github.io/categories/%E5%AD%98%E5%82%A8%E7%B1%BB/"/>
    
    
    <category term="glusterfs" scheme="https://slions.github.io/tags/glusterfs/"/>
    
    <category term="kubernetes storage" scheme="https://slions.github.io/tags/kubernetes-storage/"/>
    
  </entry>
  
  <entry>
    <title>glusterfs回收站功能</title>
    <link href="https://slions.github.io/2021/07/16/glusterfs%E5%9B%9E%E6%94%B6%E7%AB%99%E5%8A%9F%E8%83%BD/"/>
    <id>https://slions.github.io/2021/07/16/glusterfs%E5%9B%9E%E6%94%B6%E7%AB%99%E5%8A%9F%E8%83%BD/</id>
    <published>2021-07-15T16:24:50.000Z</published>
    <updated>2021-08-12T11:51:34.052Z</updated>
    
    <content type="html"><![CDATA[<h1 id="功能简述"><a href="#功能简述" class="headerlink" title="功能简述"></a>功能简述</h1><p>glusterfs有一个类似windows回收站的功能，可以帮助用户获取和恢复临时被删除的数据。每个块都会保留一个隐藏的目录.trash，它将会被用于存放被从各个块删除的文件。这个translator以后还会增强功能来支持被删除文件的恢复。</p><p>回收站的目录名应该是可配置的。trash translator也会被用于内部操作比如自卷的自修复以及再平衡。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> gluster volume <span class="built_in">set</span> &lt;VOLNAME&gt; features.trash &lt;on/off&gt;</span></span><br></pre></td></tr></table></figure><p>此命令可用于启用卷中的Trash translator,如果设置为on，则在卷启动命令期间，将在卷内的每个brick块中创建.trashcan目录。默认情况下，translator在卷启动期间加载，但仍然不起作用。在此选项的帮助下禁用垃圾桶将不会从卷中删除垃圾邮件目录或甚至其内容。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> gluster volume <span class="built_in">set</span> &lt;VOLNAME&gt; features.trash-dir &lt;name&gt;</span></span><br></pre></td></tr></table></figure><p>此命令用于将垃圾目录重新配置为用户指定的名称。参数是有效的目录名称。目录将在这个名字下面的每个brick内创建。如果用户没有指定，translator将创建默认名称为“.trashcan”的垃圾桶目录。只有当Trash translator开启时才可使用。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> gluster volume <span class="built_in">set</span> &lt;VOLNAME&gt; features.trash-max-filesize &lt;size&gt;</span></span><br></pre></td></tr></table></figure><p>此命令可用于根据大小过滤进入垃圾目录的文件。大小超过rash_max_filesize的文件将直接删除/截断。大小值后可以跟乘性后缀，例如KB（= 1024字节），MB（= 1024 * 1024字节）和GB（= 1024 * 1024 * 1024字节）。默认大小设置为5MB。考虑到垃圾目录占用了glusterfs卷空间这一事实，垃圾邮件功能的实现方式是，即使此选项设置为大于1GB的某个值，它也可以直接删除/截断大于1GB的文件。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> gluster volume <span class="built_in">set</span> &lt;VOLNAME&gt; features.trash-internal-op &lt;on/off&gt;</span></span><br></pre></td></tr></table></figure><p>此命令可用于为内部操作（例如自愈和重新平衡）启用垃圾桶。默认设置为关闭。</p><h1 id="简单测试"><a href="#简单测试" class="headerlink" title="简单测试"></a>简单测试</h1><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">[root@gluster1 /]# gluster volume info</span><br><span class="line"> </span><br><span class="line">Volume Name: gv1</span><br><span class="line">Type: Distribute</span><br><span class="line">Volume ID: 58bf037f-5b56-4cf6-8dab-9e9944800b61</span><br><span class="line">Status: Started</span><br><span class="line">Number of Bricks: 2</span><br><span class="line">Transport-type: tcp</span><br><span class="line">Bricks:</span><br><span class="line">Brick1: gluster1:/storage/brick1</span><br><span class="line">Brick2: mystorage2:/storage/brick1</span><br><span class="line">Options Reconfigured:</span><br><span class="line">features.trash: on</span><br><span class="line">performance.readdir-ahead: on</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>启用gv1卷中的Trash translator：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@gluster1 /]# gluster volume set gv1 features.trash on</span><br><span class="line">volume set: success</span><br></pre></td></tr></table></figure><p>进入到挂载目录进行删除操作:</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[root@gluster1 /]# cd /mnt/</span><br><span class="line">[root@gluster1 mnt]# ls</span><br><span class="line">aa  bb  cc  ddd</span><br><span class="line">[root@gluster1 mnt]# rm -rf cc</span><br></pre></td></tr></table></figure><p>查看目录发现有带时间戳的文件:</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">[root@gluster1 mnt]# ls -la</span><br><span class="line">total 12</span><br><span class="line">drwxr-xr-x   4 root root 4096 May 21 06:03 .</span><br><span class="line">dr-xr-xr-x. 23 root root 4096 May 20 17:23 ..</span><br><span class="line">-rwxr-xr-x   1 root root    0 May 20 00:36 aa</span><br><span class="line">-rwxr-xr-x   1 root root    0 May 20 00:36 bb</span><br><span class="line">-rwxr-xr-x   1 root root    0 May 20 00:36 ddd</span><br><span class="line">drwsr-sr-x   3 root root 4096 May 20 23:22 .trashcan</span><br><span class="line">[root@gluster1 mnt]# cd .trashcan/</span><br><span class="line">[root@gluster1 .trashcan]# ls</span><br><span class="line">cc_2019-05-20_151208  internal_op</span><br></pre></td></tr></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;功能简述&quot;&gt;&lt;a href=&quot;#功能简述&quot; class=&quot;headerlink&quot; title=&quot;功能简述&quot;&gt;&lt;/a&gt;功能简述&lt;/h1&gt;&lt;p&gt;glusterfs有一个类似windows回收站的功能，可以帮助用户获取和恢复临时被删除的数据。每个块都会保留一个隐藏的目录</summary>
      
    
    
    
    <category term="存储类" scheme="https://slions.github.io/categories/%E5%AD%98%E5%82%A8%E7%B1%BB/"/>
    
    
    <category term="glusterfs" scheme="https://slions.github.io/tags/glusterfs/"/>
    
  </entry>
  
  <entry>
    <title>k8s-v1.11使用glusterfs</title>
    <link href="https://slions.github.io/2021/07/15/k8s-v1.11%E4%BD%BF%E7%94%A8glusterfs/"/>
    <id>https://slions.github.io/2021/07/15/k8s-v1.11%E4%BD%BF%E7%94%A8glusterfs/</id>
    <published>2021-07-15T15:18:09.000Z</published>
    <updated>2021-08-12T11:49:46.309Z</updated>
    
    <content type="html"><![CDATA[<p>Glusterfs提供底层存储功能，heketi为glusterfs提供restful风格的api，Heketi要求在每个glusterfs节点上配备<strong>裸磁盘</strong>，目前heketi仅支持使用裸磁盘(未格式化)添加为device，不支持文件系统，因为Heketi要用来创建PV和VG方便管理glusterfs。</p><p>集群托管于heketi后，不能使用命令管理存储卷，以免与Heketi数据库中存储的信息不一致。</p><blockquote><p>glusterfs支持k8s的pv的3种访问模式ReadWriteOnce，ReadOnlyMany ，ReadWriteMany。访问模式只是能力描述，并不是强制执行的，对于没有按pvc声明的方式使用pv，存储提供者应该负责访问时的运行错误。例如：如果设置pvc的访问模式为ReadOnlyMany ，pod挂载后依然可写，如果需要真正的不可写，申请pvc是需要指定 readOnly: true 参数。</p></blockquote><h1 id="环境描述"><a href="#环境描述" class="headerlink" title="环境描述"></a>环境描述</h1><table><thead><tr><th><strong>主机名</strong></th><th>ip地址</th><th><strong>系统环境</strong></th><th><strong>角色</strong></th></tr></thead><tbody><tr><td>k8s-1</td><td>192.168.186.10</td><td>cpu:x2  mem:2GB  disk:/dev/sdb 10GB</td><td>K8s_master,Gluster_master,Heketi_master</td></tr><tr><td>K8s-2</td><td>192.168.186.11</td><td>cpu:x2  mem:2GB  disk:/dev/sdb 10GB</td><td>K8s_node,Gluster_node</td></tr><tr><td>K8s-3</td><td>192.168.186.12</td><td>cpu:x2  mem:2GB  disk:/dev/sdb 10GB</td><td>K8s_node,Gluster_node</td></tr></tbody></table><p>如果存在iptable限制，需执行以下命令开通以下port</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">iptables -N heketi</span><br><span class="line">iptables -A heketi -p tcp -m state --state NEW -m tcp --dport 24007 -j ACCEPT</span><br><span class="line">iptables -A heketi -p tcp -m state --state NEW -m tcp --dport 24008 -j ACCEPT</span><br><span class="line">iptables -A heketi -p tcp -m state --state NEW -m tcp --dport 2222 -j ACCEPT</span><br><span class="line">iptables -A heketi -p tcp -m state --state NEW -m multiport --dports 49152:49251 -j ACCEPT</span><br><span class="line">service iptables save</span><br></pre></td></tr></table></figure><h1 id="安装步骤"><a href="#安装步骤" class="headerlink" title="安装步骤"></a>安装步骤</h1><p>下面的测试是采用容器化方式部署GFS，GFS以Daemonset的方式进行部署，保证每台需要部署GFS管理服务的Node上都运行一个GFS管理服务。</p><h2 id="三台节点执行："><a href="#三台节点执行：" class="headerlink" title="三台节点执行："></a>三台节点执行：</h2><p>要求所有node节点存在主机的解析记录，务必配置好/etc/hosts</p><p><img src="/doc_picture/heketi-1.png" alt="image-20210714161502089"></p><p>安装 glusterfs 每节点需要提前加载 <code>dm_thin_pool</code> 模块：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> modprobe dm_thin_pool</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> modprobe dm_snapshot</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> modprobe dm_mirror</span></span><br></pre></td></tr></table></figure><p>配置开启自加载：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> cat &gt;/etc/modules-load.d/glusterfs.conf&lt;&lt;<span class="string">EOF</span></span></span><br><span class="line">dm_thin_pool</span><br><span class="line">dm_snapshot</span><br><span class="line">dm_mirror</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure><p>安装 glusterfs-fuse：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> yum install -y glusterfs-fuse lvm2</span></span><br></pre></td></tr></table></figure><h2 id="第一台节点执行："><a href="#第一台节点执行：" class="headerlink" title="第一台节点执行："></a>第一台节点执行：</h2><h3 id="安装glusterfs与heketi"><a href="#安装glusterfs与heketi" class="headerlink" title="安装glusterfs与heketi"></a>安装glusterfs与heketi</h3><p>安装 heketi client</p><p><a href="https://github.com/heketi/heketi/releases">https://github.com/heketi/heketi/releases</a></p><p>去github下载相关的版本：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-1 ~]# wget https://github.com/heketi/heketi/releases/download/v8.0.0/heketi-client-v9.0.0.linux.amd64.tar.gz</span><br><span class="line">[root@k8s-1 ~]# tar xf heketi-client-v9.0.0.linux.amd64.tar.gz</span><br><span class="line">[root@k8s-1 ~]# cp heketi-client/bin/heketi-cli /usr/local/bin</span><br></pre></td></tr></table></figure><p> 查看版本：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-1 ~]# heketi-cli -v</span><br></pre></td></tr></table></figure><p>之后部署步骤都在如下目录执行：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-1 ~]#cd heketi-client/share/heketi/kubernetes</span><br></pre></td></tr></table></figure><p>在k8s中部署 glusterfs：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-1 kubernetes]# kubectl create -f glusterfs-daemonset.json</span><br></pre></td></tr></table></figure><blockquote><ol><li>此时采用的为默认的挂载方式，可使用其他磁盘当做GFS的工作目录</li><li>此时创建的namespace为默认的default，按需更改</li></ol></blockquote><p>给提供存储 node 节点打 label:</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-1 kubernetes]# kubectl label node k8s-1 k8s-2 k8s-3 storagenode=glusterfs</span><br></pre></td></tr></table></figure><p>查看 glusterfs 状态:</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-1 kubernetes]# kubectl get pods -o wide</span><br></pre></td></tr></table></figure><p>部署 heketi server #配置 heketi server 的权限:</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-1 kubernetes]# kubectl create -f heketi-service-account.json</span><br><span class="line">[root@k8s-1 kubernetes]# kubectl create clusterrolebinding heketi-gluster-admin --clusterrole=edit --serviceaccount=default:heketi-service-account</span><br></pre></td></tr></table></figure><p> 创建 cofig secret:</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-1 kubernetes]# kubectl create secret generic heketi-config-secret --from-file=./heketi.json</span><br></pre></td></tr></table></figure><p> 初始化部署:</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-1 kubernetes]# kubectl create -f heketi-bootstrap.json</span><br></pre></td></tr></table></figure><p># 查看 heketi bootstrap 状态</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-1 kubernetes]# kubectl get pods -o wide</span><br><span class="line"></span><br><span class="line">[root@k8s-1 kubernetes]# kubectl get svc</span><br></pre></td></tr></table></figure><p># 配置端口转发 heketi server</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-1 kubernetes]# HEKETI_BOOTSTRAP_POD=$(kubectl get pods | grep deploy-heketi | awk &#x27;&#123;print $1&#125;&#x27;)</span><br><span class="line"></span><br><span class="line">[root@k8s-1 kubernetes]# kubectl port-forward $HEKETI_BOOTSTRAP_POD 58080:8080</span><br></pre></td></tr></table></figure><p># 测试访问,另起一终端</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-1 ~]#curl http://localhost:58080/hello</span><br></pre></td></tr></table></figure><h3 id="配置-glusterfs"><a href="#配置-glusterfs" class="headerlink" title="配置 glusterfs"></a>配置 glusterfs</h3><blockquote><ol><li>hostnames/manage 字段里必须和 kubectl get node 一致</li><li>hostnames/storage 指定存储网络 ip 本次实验使用与k8s集群同一个ip</li></ol></blockquote><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s<span class="number">-1</span> kubernetes]# cat &gt;topology.json&lt;&lt;EOF</span><br><span class="line">&#123;</span><br><span class="line">  <span class="attr">&quot;clusters&quot;</span>: [</span><br><span class="line">    &#123;</span><br><span class="line">      <span class="attr">&quot;nodes&quot;</span>: [</span><br><span class="line">        &#123;</span><br><span class="line">          <span class="attr">&quot;node&quot;</span>: &#123;</span><br><span class="line">            <span class="attr">&quot;hostnames&quot;</span>: &#123;</span><br><span class="line">              <span class="attr">&quot;manage&quot;</span>: [</span><br><span class="line">                <span class="string">&quot;k8s-1&quot;</span></span><br><span class="line">              ],</span><br><span class="line">              <span class="attr">&quot;storage&quot;</span>: [</span><br><span class="line">                <span class="string">&quot;192.168.186.10&quot;</span></span><br><span class="line">              ]</span><br><span class="line">            &#125;,</span><br><span class="line">            <span class="attr">&quot;zone&quot;</span>: <span class="number">1</span></span><br><span class="line">          &#125;,</span><br><span class="line">          <span class="attr">&quot;devices&quot;</span>: [</span><br><span class="line">            <span class="string">&quot;/dev/sdb&quot;</span></span><br><span class="line">          ]</span><br><span class="line">        &#125;,</span><br><span class="line">        &#123;</span><br><span class="line">          <span class="attr">&quot;node&quot;</span>: &#123;</span><br><span class="line">            <span class="attr">&quot;hostnames&quot;</span>: &#123;</span><br><span class="line">              <span class="attr">&quot;manage&quot;</span>: [</span><br><span class="line">                <span class="string">&quot;k8s-2&quot;</span></span><br><span class="line">              ],</span><br><span class="line">              <span class="attr">&quot;storage&quot;</span>: [</span><br><span class="line">                <span class="string">&quot;192.168.186.11&quot;</span></span><br><span class="line">              ]</span><br><span class="line">            &#125;,</span><br><span class="line">            <span class="attr">&quot;zone&quot;</span>: <span class="number">1</span></span><br><span class="line">          &#125;,</span><br><span class="line">          <span class="attr">&quot;devices&quot;</span>: [</span><br><span class="line">            <span class="string">&quot;/dev/sdb&quot;</span></span><br><span class="line">          ]</span><br><span class="line">        &#125;,</span><br><span class="line">        &#123;</span><br><span class="line">          <span class="attr">&quot;node&quot;</span>: &#123;</span><br><span class="line">            <span class="attr">&quot;hostnames&quot;</span>: &#123;</span><br><span class="line">              <span class="attr">&quot;manage&quot;</span>: [</span><br><span class="line">                <span class="string">&quot;k8s-3&quot;</span></span><br><span class="line">              ],</span><br><span class="line">              <span class="attr">&quot;storage&quot;</span>: [</span><br><span class="line">                <span class="string">&quot;192.168.186.12&quot;</span></span><br><span class="line">              ]</span><br><span class="line">            &#125;,</span><br><span class="line">            <span class="attr">&quot;zone&quot;</span>: <span class="number">1</span></span><br><span class="line">          &#125;,</span><br><span class="line">          <span class="attr">&quot;devices&quot;</span>: [</span><br><span class="line">            <span class="string">&quot;/dev/sdb&quot;</span></span><br><span class="line">          ]                                                                                       </span><br><span class="line">        &#125;</span><br><span class="line">      ]</span><br><span class="line">    &#125;</span><br><span class="line">  ]</span><br><span class="line">&#125;</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure><p>heketi加载配置:</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-1 kubernetes]# export HEKETI_CLI_SERVER=http://localhost:58080</span><br><span class="line"></span><br><span class="line">[root@k8s-1 kubernetes]# heketi-cli topology load --json=topology.json</span><br></pre></td></tr></table></figure><p>使用 Heketi 创建一个用于存储 Heketi 数据库的 volume：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-1 kubernetes]# heketi-cli setup-openshift-heketi-storage</span><br><span class="line">[root@k8s-1 kubernetes]# kubectl create -f heketi-storage.json</span><br></pre></td></tr></table></figure><blockquote><p>heketi-storage.json中：</p><p>创建了heketi-storage-endpoints，（指明了gfs地址和端口，默认端口为1）创建了heketi-storage-copy-job，此job的作用就是复制heketi中的数据文件到 /heketi，而/heketi目录挂载在了卷heketi-storage中，而heketi-storage volume是前面执行”heketi-cli setup-openshift-heketi-storage”时创建好了的。</p></blockquote><p>查看状态,等所有job完成 即状态为 <code>Completed</code>,才能进行如下的步骤：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-1 kubernetes]# kubectl get pods</span><br><span class="line">[root@k8s-1 kubernetes]# kubectl get job</span><br></pre></td></tr></table></figure><p> 删除部署时产生的相关资源：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-1 kubernetes]# kubectl delete all,service,jobs,deployment,secret --selector=&quot;deploy-heketi&quot;</span><br></pre></td></tr></table></figure><p># 部署 heketi server</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-1 kubernetes]# kubectl create -f heketi-deployment.json</span><br></pre></td></tr></table></figure><p># 查看 heketi server 状态</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-1 kubernetes]# kubectl get pods -o wide</span><br><span class="line">[root@k8s-1 kubernetes]# kubectl get svc</span><br></pre></td></tr></table></figure><p># 查看 heketi 状态信息, 配置端口转发 heketi server</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-1 kubernetes]# HEKETI_BOOTSTRAP_POD=$(kubectl get pods | grep heketi | awk &#x27;&#123;print $1&#125;&#x27;)</span><br><span class="line">[root@k8s-1 kubernetes]# kubectl port-forward $HEKETI_BOOTSTRAP_POD 58080:8080</span><br><span class="line">[root@k8s-1 kubernetes]# export HEKETI_CLI_SERVER=http://localhost:58080</span><br><span class="line">[root@k8s-1 kubernetes]# heketi-cli cluster list</span><br><span class="line">[root@k8s-1 kubernetes]# heketi-cli volume list</span><br></pre></td></tr></table></figure><blockquote><p>可以把heketi的service type换成NodePrort,并给glusterfs的daemonset添加spec. template.spec.hostNetwork: true,之后就不用以端口转发映射本地端口的方式访问heketi，直接heketi-cli -s <a href="srv:port">srv:port</a> 即可</p></blockquote><h3 id="创建-StorageClass"><a href="#创建-StorageClass" class="headerlink" title="创建 StorageClass"></a>创建 StorageClass</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-1 kubernetes]# HEKETI_SERVER=$(kubectl get svc | grep heketi | head -1 | awk &#x27;&#123;print $3&#125;&#x27;)</span><br><span class="line">[root@k8s-1 kubernetes]# echo $HEKETI_SERVER</span><br><span class="line">[root@k8s-1 kubernetes]# cat &gt;storageclass-glusterfs.yaml&lt;&lt;EOF</span><br><span class="line">kind: StorageClass</span><br><span class="line">apiVersion: storage.k8s.io/v1</span><br><span class="line">metadata:</span><br><span class="line">  name: gluster-heketi</span><br><span class="line">provisioner: kubernetes.io/glusterfs</span><br><span class="line"><span class="meta">#</span><span class="bash">reclaimPolicy: Retain</span></span><br><span class="line">parameters:</span><br><span class="line">  resturl: &quot;http://$HEKETI_SERVER:8080&quot;</span><br><span class="line">  gidMin: &quot;40000&quot;</span><br><span class="line">  gidMax: &quot;50000&quot;</span><br><span class="line">  volumetype: &quot;replicate:3&quot;</span><br><span class="line"><span class="meta">#</span><span class="bash">允许对pvc扩容</span></span><br><span class="line">allowVolumeExpansion: true</span><br><span class="line">EOF</span><br><span class="line">[root@k8s-1 kubernetes]# kubectl create -f storageclass-glusterfs.yaml</span><br></pre></td></tr></table></figure><blockquote><ol><li>以上创建了一个含有三个副本的gluster的存储类型（storage-class） </li><li>volumetype中的relicate必须大于1，否则创建pvc的时候会报错</li><li>在这里创建的storageclass显示指定reclaimPolicy为Retain(默认情况下是Delete)，删除pvc后pv以及后端的volume、brick(lvm)不会被删除。</li></ol></blockquote><h3 id="创建pvc"><a href="#创建pvc" class="headerlink" title="创建pvc"></a>创建pvc</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-1 kubernetes]# cat &gt;gluster-pvc-test.yaml&lt;&lt;EOF</span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: PersistentVolumeClaim</span><br><span class="line">metadata:</span><br><span class="line"> name: gluster1</span><br><span class="line"> annotations:</span><br><span class="line">   volume.beta.kubernetes.io/storage-class: gluster-heketi</span><br><span class="line">spec:</span><br><span class="line"> accessModes:</span><br><span class="line">  - ReadWriteMany</span><br><span class="line"> resources:</span><br><span class="line">   requests:</span><br><span class="line">     storage: 1Gi</span><br><span class="line">EOF</span><br><span class="line">[root@k8s-1 kubernetes]# kubectl apply -f gluster-pvc-test.yaml</span><br></pre></td></tr></table></figure><p>查看卷状态</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-1 kubernetes]# kubectl get pvc</span><br><span class="line">[root@k8s-1 kubernetes]# kubectl get pv</span><br></pre></td></tr></table></figure><h3 id="创建服务挂载测试"><a href="#创建服务挂载测试" class="headerlink" title="创建服务挂载测试"></a>创建服务挂载测试</h3><figure class="highlight yml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line">[<span class="string">root@k8s-1</span> <span class="string">kubernetes</span>]<span class="comment"># cat &gt;nginx-pod.yaml&lt;&lt;EOF</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">extensions/v1beta1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Deployment</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">nginx-gfs</span></span><br><span class="line">  <span class="attr">labels:</span></span><br><span class="line">    <span class="attr">name:</span> <span class="string">nginx-gfs</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">replicas:</span> <span class="number">2</span></span><br><span class="line">  <span class="attr">selector:</span></span><br><span class="line">    <span class="attr">matchLabels:</span></span><br><span class="line">      <span class="attr">name:</span> <span class="string">nginx-gfs</span></span><br><span class="line">  <span class="attr">template:</span></span><br><span class="line">    <span class="attr">metadata:</span></span><br><span class="line">      <span class="attr">labels:</span></span><br><span class="line">        <span class="attr">name:</span> <span class="string">nginx-gfs</span></span><br><span class="line">    <span class="attr">spec:</span></span><br><span class="line">      <span class="attr">containers:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">nginx-gfs</span></span><br><span class="line">        <span class="attr">image:</span> <span class="string">nginx</span></span><br><span class="line">        <span class="attr">ports:</span></span><br><span class="line">        <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">web</span></span><br><span class="line">          <span class="attr">containerPort:</span> <span class="number">80</span></span><br><span class="line">        <span class="attr">volumeMounts:</span></span><br><span class="line">        <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">gfs</span></span><br><span class="line">          <span class="attr">mountPath:</span> <span class="string">/usr/share/nginx/html</span></span><br><span class="line">      <span class="attr">volumes:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">gfs</span></span><br><span class="line">        <span class="attr">persistentVolumeClaim:</span></span><br><span class="line">          <span class="attr">claimName:</span> <span class="string">gluster1</span></span><br><span class="line"><span class="string">EOF</span></span><br><span class="line">[<span class="string">root@k8s-1</span> <span class="string">kubernetes</span>]<span class="comment"># kubectl apply -f nginx-pod.yaml</span></span><br></pre></td></tr></table></figure><p>查看服务是否正常启动：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-1 kubernetes]# kubectl get pods -o wide</span><br></pre></td></tr></table></figure><h3 id="测试pvc的扩容"><a href="#测试pvc的扩容" class="headerlink" title="测试pvc的扩容"></a>测试pvc的扩容</h3><p>修改pvc/gluster1容量1G改为2G，过一会儿会自动生效，此时查看pv,pvc,和进入容器都已经成了2G（自己机器上测试发现生效时长大概为1min），把容器停掉继续扩容发现也是ok的。</p><h1 id="分析篇"><a href="#分析篇" class="headerlink" title="分析篇"></a>分析篇</h1><h2 id="heketi是怎么对磁盘进行操作的"><a href="#heketi是怎么对磁盘进行操作的" class="headerlink" title="heketi是怎么对磁盘进行操作的"></a>heketi是怎么对磁盘进行操作的</h2><p>回过头来分析下heketi加载gfs配置时进行了什么操作</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">$ heketi-cli topology load --json=topology-sample.json  </span><br><span class="line">   Creating cluster ... ID: 224a5a6555fa5c0c930691111c63e863</span><br><span class="line">     Allowing file volumes on cluster.</span><br><span class="line">     Allowing block volumes on cluster.</span><br><span class="line">     Creating node 192.168.186.10 ... ID: 7946b917b91a579c619ba51d9129aeb0</span><br><span class="line">            Adding device /dev/sdb ... OK</span><br><span class="line">     Creating node 192.168.186.11 ... ID: 5d10e593e89c7c61f8712964387f959c</span><br><span class="line">            Adding device /dev/sdb ... OK</span><br><span class="line">     Creating node 192.168.186.12 ... ID: de620cb2c313a5461d5e0a6ae234c553</span><br><span class="line">            Adding device /dev/sdb ... OK</span><br></pre></td></tr></table></figure><ul><li>进入任意glusterfs Pod内，执行gluster peer status 发现都已把对端加入到了可信存储池(TSP)中。</li><li>在运行了gluster Pod的节点上，自动创建了一个VG，此VG正是由topology-sample.json 文件中的磁盘裸设备创建而来。</li><li>一块磁盘设备创建出一个VG，以后创建的PVC，即从此VG里划分的LV。</li><li>heketi-cli topology info 查看拓扑结构，显示出每个磁盘设备的ID，对应VG的ID，总空间、已用空间、空余空间等信息。</li></ul><h2 id="heketi创建db-volume的流程"><a href="#heketi创建db-volume的流程" class="headerlink" title="heketi创建db volume的流程"></a>heketi创建db volume的流程</h2><p>执行heketi-cli setup-openshift-heketi-storage并观测heketi后台做了什么，可以通过相应日志查看：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br></pre></td><td class="code"><pre><span class="line">[negroni] 2020-03-06T16:45:13Z | 200 |   61.841µs | 192.168.186.10:30080 | GET /clusters</span><br><span class="line">[negroni] 2020-03-06T16:45:13Z | 200 |   159.901µs | 192.168.186.10:30080 | GET /clusters/6749ed08e37290fbb1cc4c881872054d</span><br><span class="line">[heketi] INFO 2020/03/06 16:45:13 Allocating brick set #0</span><br><span class="line">[negroni] 2020-03-06T16:45:13Z | 202 |   46.293298ms | 192.168.186.10:30080 | POST /volumes</span><br><span class="line">[asynchttp] INFO 2020/03/06 16:45:13 asynchttp.go:288: Started job 5ffdc4ab574897e19511ae43afa7e78c</span><br><span class="line">[heketi] INFO 2020/03/06 16:45:13 Started async operation: Create Volume</span><br><span class="line">[heketi] INFO 2020/03/06 16:45:13 Trying Create Volume (attempt #1/5)</span><br><span class="line">[heketi] INFO 2020/03/06 16:45:13 Creating brick b6411ccff63daf1270bc9f354ca484dd</span><br><span class="line">[heketi] INFO 2020/03/06 16:45:13 Creating brick 98700f7b0bce70eb29279fb275763704</span><br><span class="line">[heketi] INFO 2020/03/06 16:45:13 Creating brick 777c447835963ef4db7cbb2392c85e59</span><br><span class="line">[negroni] 2020-03-06T16:45:13Z | 200 |   41.375µs | 192.168.186.10:30080 | GET /queue/5ffdc4ab574897e19511ae43afa7e78c</span><br><span class="line">[kubeexec] DEBUG 2020/03/06 16:45:13 heketi/pkg/remoteexec/kube/exec.go:81:kube.ExecCommands: Ran command [mkdir -p /var/lib/heketi/mounts/vg_ccc135aa56ab4f89868d9755bd531a22/brick_b6411ccff63daf1270bc9f354ca484dd] on [pod:glusterfs-77ghn c:glusterfs ns:glusterfs (from host:ceph2 selector:glusterfs-node)]: Stdout []: Stderr []</span><br><span class="line">[kubeexec] DEBUG 2020/03/06 16:45:13 heketi/pkg/remoteexec/kube/exec.go:81:kube.ExecCommands: Ran command [mkdir -p /var/lib/heketi/mounts/vg_63f644b972ff7a04259395f67c149cf2/brick_777c447835963ef4db7cbb2392c85e59] on [pod:glusterfs-mhsgb c:glusterfs ns:glusterfs (from host:ceph3 selector:glusterfs-node)]: Stdout []: Stderr []</span><br><span class="line">[kubeexec] DEBUG 2020/03/06 16:45:13 heketi/pkg/remoteexec/kube/exec.go:81:kube.ExecCommands: Ran command [mkdir -p /var/lib/heketi/mounts/vg_6199228451001048c7543f41ce6572cb/brick_98700f7b0bce70eb29279fb275763704] on [pod:glusterfs-fflbn c:glusterfs ns:glusterfs (from host:ceph1 selector:glusterfs-node)]: Stdout []: Stderr []</span><br><span class="line">[kubeexec] DEBUG 2020/03/06 16:45:13 heketi/pkg/remoteexec/kube/exec.go:81:kube.ExecCommands: Ran command [lvcreate -qq --autobackup=n --poolmetadatasize 12288K --chunksize 256K --size 2097152K --thin vg_ccc135aa56ab4f89868d9755bd531a22/tp_76e0f4c09fbd75bcd2bfae166fb8a73d --virtualsize 2097152K --name brick_b6411ccff63daf1270bc9f354ca484dd] on [pod:glusterfs-77ghn c:glusterfs ns:glusterfs (from host:ceph2 selector:glusterfs-node)]: Stdout []: Stderr [  WARNING: This metadata update is NOT backed up.</span><br><span class="line">]</span><br><span class="line">[kubeexec] DEBUG 2020/03/06 16:45:14 heketi/pkg/remoteexec/kube/exec.go:81:kube.ExecCommands: Ran command [lvcreate -qq --autobackup=n --poolmetadatasize 12288K --chunksize 256K --size 2097152K --thin vg_63f644b972ff7a04259395f67c149cf2/tp_777c447835963ef4db7cbb2392c85e59 --virtualsize 2097152K --name brick_777c447835963ef4db7cbb2392c85e59] on [pod:glusterfs-mhsgb c:glusterfs ns:glusterfs (from host:ceph3 selector:glusterfs-node)]: Stdout []: Stderr [  WARNING: This metadata update is NOT backed up.</span><br><span class="line">]</span><br><span class="line">[kubeexec] DEBUG 2020/03/06 16:45:14 heketi/pkg/remoteexec/kube/exec.go:81:kube.ExecCommands: Ran command [lvcreate -qq --autobackup=n --poolmetadatasize 12288K --chunksize 256K --size 2097152K --thin vg_6199228451001048c7543f41ce6572cb/tp_98700f7b0bce70eb29279fb275763704 --virtualsize 2097152K --name brick_98700f7b0bce70eb29279fb275763704] on [pod:glusterfs-fflbn c:glusterfs ns:glusterfs (from host:ceph1 selector:glusterfs-node)]: Stdout []: Stderr [  WARNING: This metadata update is NOT backed up.</span><br><span class="line">]</span><br><span class="line">[kubeexec] DEBUG 2020/03/06 16:45:14 heketi/pkg/remoteexec/kube/exec.go:81:kube.ExecCommands: Ran command [mkfs.xfs -i size=512 -n size=8192 /dev/mapper/vg_ccc135aa56ab4f89868d9755bd531a22-brick_b6411ccff63daf1270bc9f354ca484dd] on [pod:glusterfs-77ghn c:glusterfs ns:glusterfs (from host:ceph2 selector:glusterfs-node)]: Stdout [meta-data=/dev/mapper/vg_ccc135aa56ab4f89868d9755bd531a22-brick_b6411ccff63daf1270bc9f354ca484dd isize=512    agcount=8, agsize=65536 blks</span><br><span class="line">         =                       sectsz=512   attr=2, projid32bit=1</span><br><span class="line">         =                       crc=1        finobt=0, sparse=0</span><br><span class="line">data     =                       bsize=4096   blocks=524288, imaxpct=25</span><br><span class="line">         =                       sunit=64     swidth=64 blks</span><br><span class="line">naming   =version 2              bsize=8192   ascii-ci=0 ftype=1</span><br><span class="line">log      =internal log           bsize=4096   blocks=2560, version=2</span><br><span class="line">         =                       sectsz=512   sunit=64 blks, lazy-count=1</span><br><span class="line">realtime =none                   extsz=4096   blocks=0, rtextents=0</span><br><span class="line">]: Stderr []</span><br><span class="line">[kubeexec] DEBUG 2020/03/06 16:45:14 heketi/pkg/remoteexec/kube/exec.go:81:kube.ExecCommands: Ran command [awk &quot;BEGIN &#123;print \&quot;/dev/mapper/vg_ccc135aa56ab4f89868d9755bd531a22-brick_b6411ccff63daf1270bc9f354ca484dd /var/lib/heketi/mounts/vg_ccc135aa56ab4f89868d9755bd531a22/brick_b6411ccff63daf1270bc9f354ca484dd xfs rw,inode64,noatime,nouuid 1 2\&quot; &gt;&gt; \&quot;/var/lib/heketi/fstab\&quot;&#125;&quot;] on [pod:glusterfs-77ghn c:glusterfs ns:glusterfs (from host:ceph2 selector:glusterfs-node)]: Stdout []: Stderr []</span><br><span class="line">[kubeexec] DEBUG 2020/03/06 16:45:14 heketi/pkg/remoteexec/kube/exec.go:81:kube.ExecCommands: Ran command [mkfs.xfs -i size=512 -n size=8192 /dev/mapper/vg_63f644b972ff7a04259395f67c149cf2-brick_777c447835963ef4db7cbb2392c85e59] on [pod:glusterfs-mhsgb c:glusterfs ns:glusterfs (from host:ceph3 selector:glusterfs-node)]: Stdout [meta-data=/dev/mapper/vg_63f644b972ff7a04259395f67c149cf2-brick_777c447835963ef4db7cbb2392c85e59 isize=512    agcount=8, agsize=65536 blks</span><br><span class="line">         =                       sectsz=512   attr=2, projid32bit=1</span><br><span class="line">         =                       crc=1        finobt=0, sparse=0</span><br><span class="line">data     =                       bsize=4096   blocks=524288, imaxpct=25</span><br><span class="line">         =                       sunit=64     swidth=64 blks</span><br><span class="line">naming   =version 2              bsize=8192   ascii-ci=0 ftype=1</span><br><span class="line">log      =internal log           bsize=4096   blocks=2560, version=2</span><br><span class="line">         =                       sectsz=512   sunit=64 blks, lazy-count=1</span><br><span class="line">realtime =none                   extsz=4096   blocks=0, rtextents=0</span><br><span class="line">]: Stderr []</span><br><span class="line">[negroni] 2020-03-06T16:45:14Z | 200 |   34.302µs | 192.168.186.10:30080 | GET /queue/5ffdc4ab574897e19511ae43afa7e78c</span><br><span class="line">[kubeexec] DEBUG 2020/03/06 16:45:14 heketi/pkg/remoteexec/kube/exec.go:81:kube.ExecCommands: Ran command [awk &quot;BEGIN &#123;print \&quot;/dev/mapper/vg_63f644b972ff7a04259395f67c149cf2-brick_777c447835963ef4db7cbb2392c85e59 /var/lib/heketi/mounts/vg_63f644b972ff7a04259395f67c149cf2/brick_777c447835963ef4db7cbb2392c85e59 xfs rw,inode64,noatime,nouuid 1 2\&quot; &gt;&gt; \&quot;/var/lib/heketi/fstab\&quot;&#125;&quot;] on [pod:glusterfs-mhsgb c:glusterfs ns:glusterfs (from host:ceph3 selector:glusterfs-node)]: Stdout []: Stderr []</span><br><span class="line">[kubeexec] DEBUG 2020/03/06 16:45:14 heketi/pkg/remoteexec/kube/exec.go:81:kube.ExecCommands: Ran command [mkfs.xfs -i size=512 -n size=8192 /dev/mapper/vg_6199228451001048c7543f41ce6572cb-brick_98700f7b0bce70eb29279fb275763704] on [pod:glusterfs-fflbn c:glusterfs ns:glusterfs (from host:ceph1 selector:glusterfs-node)]: Stdout [meta-data=/dev/mapper/vg_6199228451001048c7543f41ce6572cb-brick_98700f7b0bce70eb29279fb275763704 isize=512    agcount=8, agsize=65536 blks</span><br><span class="line">         =                       sectsz=512   attr=2, projid32bit=1</span><br><span class="line">         =                       crc=1        finobt=0, sparse=0</span><br><span class="line">data     =                       bsize=4096   blocks=524288, imaxpct=25</span><br><span class="line">         =                       sunit=64     swidth=64 blks</span><br><span class="line">naming   =version 2              bsize=8192   ascii-ci=0 ftype=1</span><br><span class="line">log      =internal log           bsize=4096   blocks=2560, version=2</span><br><span class="line">         =                       sectsz=512   sunit=64 blks, lazy-count=1</span><br><span class="line">realtime =none                   extsz=4096   blocks=0, rtextents=0</span><br><span class="line">]: Stderr []</span><br><span class="line">[kubeexec] DEBUG 2020/03/06 16:45:14 heketi/pkg/remoteexec/kube/exec.go:81:kube.ExecCommands: Ran command [awk &quot;BEGIN &#123;print \&quot;/dev/mapper/vg_6199228451001048c7543f41ce6572cb-brick_98700f7b0bce70eb29279fb275763704 /var/lib/heketi/mounts/vg_6199228451001048c7543f41ce6572cb/brick_98700f7b0bce70eb29279fb275763704 xfs rw,inode64,noatime,nouuid 1 2\&quot; &gt;&gt; \&quot;/var/lib/heketi/fstab\&quot;&#125;&quot;] on [pod:glusterfs-fflbn c:glusterfs ns:glusterfs (from host:ceph1 selector:glusterfs-node)]: Stdout []: Stderr []</span><br><span class="line">[kubeexec] DEBUG 2020/03/06 16:45:14 heketi/pkg/remoteexec/kube/exec.go:81:kube.ExecCommands: Ran command [mount -o rw,inode64,noatime,nouuid /dev/mapper/vg_ccc135aa56ab4f89868d9755bd531a22-brick_b6411ccff63daf1270bc9f354ca484dd /var/lib/heketi/mounts/vg_ccc135aa56ab4f89868d9755bd531a22/brick_b6411ccff63daf1270bc9f354ca484dd] on [pod:glusterfs-77ghn c:glusterfs ns:glusterfs (from host:ceph2 selector:glusterfs-node)]: Stdout []: Stderr []</span><br><span class="line">[kubeexec] DEBUG 2020/03/06 16:45:14 heketi/pkg/remoteexec/kube/exec.go:81:kube.ExecCommands: Ran command [mount -o rw,inode64,noatime,nouuid /dev/mapper/vg_63f644b972ff7a04259395f67c149cf2-brick_777c447835963ef4db7cbb2392c85e59 /var/lib/heketi/mounts/vg_63f644b972ff7a04259395f67c149cf2/brick_777c447835963ef4db7cbb2392c85e59] on [pod:glusterfs-mhsgb c:glusterfs ns:glusterfs (from host:ceph3 selector:glusterfs-node)]: Stdout []: Stderr []</span><br><span class="line">[kubeexec] DEBUG 2020/03/06 16:45:14 heketi/pkg/remoteexec/kube/exec.go:81:kube.ExecCommands: Ran command [mkdir /var/lib/heketi/mounts/vg_ccc135aa56ab4f89868d9755bd531a22/brick_b6411ccff63daf1270bc9f354ca484dd/brick] on [pod:glusterfs-77ghn c:glusterfs ns:glusterfs (from host:ceph2 selector:glusterfs-node)]: Stdout []: Stderr []</span><br><span class="line">[kubeexec] DEBUG 2020/03/06 16:45:14 heketi/pkg/remoteexec/kube/exec.go:81:kube.ExecCommands: Ran command [mkdir /var/lib/heketi/mounts/vg_63f644b972ff7a04259395f67c149cf2/brick_777c447835963ef4db7cbb2392c85e59/brick] on [pod:glusterfs-mhsgb c:glusterfs ns:glusterfs (from host:ceph3 selector:glusterfs-node)]: Stdout []: Stderr []</span><br><span class="line">[kubeexec] DEBUG 2020/03/06 16:45:14 heketi/pkg/remoteexec/kube/exec.go:81:kube.ExecCommands: Ran command [mount -o rw,inode64,noatime,nouuid /dev/mapper/vg_6199228451001048c7543f41ce6572cb-brick_98700f7b0bce70eb29279fb275763704 /var/lib/heketi/mounts/vg_6199228451001048c7543f41ce6572cb/brick_98700f7b0bce70eb29279fb275763704] on [pod:glusterfs-fflbn c:glusterfs ns:glusterfs (from host:ceph1 selector:glusterfs-node)]: Stdout []: Stderr []</span><br><span class="line">[kubeexec] DEBUG 2020/03/06 16:45:14 heketi/pkg/remoteexec/kube/exec.go:81:kube.ExecCommands: Ran command [mkdir /var/lib/heketi/mounts/vg_6199228451001048c7543f41ce6572cb/brick_98700f7b0bce70eb29279fb275763704/brick] on [pod:glusterfs-fflbn c:glusterfs ns:glusterfs (from host:ceph1 selector:glusterfs-node)]: Stdout []: Stderr []</span><br><span class="line">[cmdexec] INFO 2020/03/06 16:45:14 Creating volume heketidbstorage replica 3</span><br><span class="line">[negroni] 2020-03-06T16:45:15Z | 200 |   42.505µs | 192.168.186.10:30080 | GET /queue/5ffdc4ab574897e19511ae43afa7e78c</span><br><span class="line">[kubeexec] DEBUG 2020/03/06 16:45:15 heketi/pkg/remoteexec/kube/exec.go:81:kube.ExecCommands: Ran command [gluster --mode=script --timeout=600 volume create heketidbstorage replica 3 192.168.186.11:/var/lib/heketi/mounts/vg_ccc135aa56ab4f89868d9755bd531a22/brick_b6411ccff63daf1270bc9f354ca484dd/brick 192.168.186.10:/var/lib/heketi/mounts/vg_6199228451001048c7543f41ce6572cb/brick_98700f7b0bce70eb29279fb275763704/brick 192.168.186.12:/var/lib/heketi/mounts/vg_63f644b972ff7a04259395f67c149cf2/brick_777c447835963ef4db7cbb2392c85e59/brick] on [pod:glusterfs-77ghn c:glusterfs ns:glusterfs (from host:ceph2 selector:glusterfs-node)]: Stdout [volume create: heketidbstorage: success: please start the volume to access data</span><br><span class="line">]: Stderr []</span><br><span class="line">[kubeexec] DEBUG 2020/03/06 16:45:15 heketi/pkg/remoteexec/kube/exec.go:81:kube.ExecCommands: Ran command [gluster --mode=script --timeout=600 volume set heketidbstorage user.heketi.id 17a63c6483a00155fb0b48bb353b9c7a] on [pod:glusterfs-77ghn c:glusterfs ns:glusterfs (from host:ceph2 selector:glusterfs-node)]: Stdout [volume set: success</span><br><span class="line">]: Stderr []</span><br><span class="line">[negroni] 2020-03-06T16:45:16Z | 200 |   33.664µs | 192.168.186.10:30080 | GET /queue/5ffdc4ab574897e19511ae43afa7e78c</span><br><span class="line">[negroni] 2020-03-06T16:45:17Z | 200 |   30.885µs | 192.168.186.10:30080 | GET /queue/5ffdc4ab574897e19511ae43afa7e78c</span><br><span class="line">[negroni] 2020-03-06T16:45:18Z | 200 |   101.832µs | 192.168.186.10:30080 | GET /queue/5ffdc4ab574897e19511ae43afa7e78c</span><br><span class="line">[kubeexec] DEBUG 2020/03/06 16:45:19 heketi/pkg/remoteexec/kube/exec.go:81:kube.ExecCommands: Ran command [gluster --mode=script --timeout=600 volume start heketidbstorage] on [pod:glusterfs-77ghn c:glusterfs ns:glusterfs (from host:ceph2 selector:glusterfs-node)]: Stdout [volume start: heketidbstorage: success</span><br><span class="line">]: Stderr []</span><br><span class="line">[asynchttp] INFO 2020/03/06 16:45:19 asynchttp.go:292: Completed job 5ffdc4ab574897e19511ae43afa7e78c in 5.802125511s</span><br><span class="line">[negroni] 2020-03-06T16:45:19Z | 303 |   42.089µs | 192.168.186.10:30080 | GET /queue/5ffdc4ab574897e19511ae43afa7e78c</span><br><span class="line">[negroni] 2020-03-06T16:45:19Z | 200 |   50.078742ms | 192.168.186.10:30080 | GET /volumes/17a63c6483a00155fb0b48bb353b9c7a</span><br><span class="line">[negroni] 2020-03-06T16:45:19Z | 200 |   484.808µs | 192.168.186.10:30080 | GET /backup/db</span><br><span class="line"></span><br></pre></td></tr></table></figure><h2 id="k8s是怎么通过heketi创建pvc的"><a href="#k8s是怎么通过heketi创建pvc的" class="headerlink" title="k8s是怎么通过heketi创建pvc的"></a>k8s是怎么通过heketi创建pvc的</h2><p>storageclass中会指定heketi server端的地址和卷的类型（replica 3），用户通过pvc创建1G的pv,观查heketi服务后台干了啥：</p><p>首先发现heketi接收到请求后起了一个job，创建了3个bricks，在其中三台gfs节点创建了相应的目录，如下图：</p><p><img src="/doc_picture/gfs-heketi-1.png" alt="image-20210716001252519"></p><p>创建lv,添加自动挂载：</p><p><img src="/doc_picture/gfs-heketi-2.png" alt="image-20210716001304334"></p><p>创建brick，设置权限：</p><p><img src="/doc_picture/gfs-heketi-3.png" alt="image-20210716001320020"></p><p>创建volume：</p><p><img src="/doc_picture/gfs-heketi-4.png" alt="image-20210716001344552"></p><h1 id="参考文档"><a href="#参考文档" class="headerlink" title="参考文档"></a>参考文档</h1><p>heketi: <a href="https://github.com/heketi/heketi">https://github.com/heketi/heketi</a></p><p>glusterfs:  <a href="https://github.com/gluster/gluster-kubernetes">https://github.com/gluster/gluster-kubernetes</a></p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;Glusterfs提供底层存储功能，heketi为glusterfs提供restful风格的api，Heketi要求在每个glusterfs节点上配备&lt;strong&gt;裸磁盘&lt;/strong&gt;，目前heketi仅支持使用裸磁盘(未格式化)添加为device，不支持文件系统，因</summary>
      
    
    
    
    <category term="k8s存储" scheme="https://slions.github.io/categories/k8s%E5%AD%98%E5%82%A8/"/>
    
    
    <category term="glusterfs" scheme="https://slions.github.io/tags/glusterfs/"/>
    
    <category term="kubernetes storage" scheme="https://slions.github.io/tags/kubernetes-storage/"/>
    
  </entry>
  
</feed>
