<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Hexo</title>
  
  
  <link href="https://slions.github.io/atom.xml" rel="self"/>
  
  <link href="https://slions.github.io/"/>
  <updated>2021-08-01T11:02:06.354Z</updated>
  <id>https://slions.github.io/</id>
  
  <author>
    <name>Jingyu Shi</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>创建开机自启程序（下篇）</title>
    <link href="https://slions.github.io/2021/08/01/%E5%88%9B%E5%BB%BA%E5%BC%80%E6%9C%BA%E8%87%AA%E5%90%AF%E7%A8%8B%E5%BA%8F%EF%BC%88%E4%B8%8B%E7%AF%87%EF%BC%89/"/>
    <id>https://slions.github.io/2021/08/01/%E5%88%9B%E5%BB%BA%E5%BC%80%E6%9C%BA%E8%87%AA%E5%90%AF%E7%A8%8B%E5%BA%8F%EF%BC%88%E4%B8%8B%E7%AF%87%EF%BC%89/</id>
    <published>2021-08-01T07:27:11.000Z</published>
    <updated>2021-08-01T11:02:06.354Z</updated>
    
    <content type="html"><![CDATA[<a href="/2021/08/01/%E5%88%9B%E5%BB%BA%E5%BC%80%E6%9C%BA%E8%87%AA%E5%90%AF%E7%A8%8B%E5%BA%8F%EF%BC%88%E4%B8%8A%E7%AF%87%EF%BC%89/" title="上篇">上篇</a>讲述了通过rc.local可以实现程序的开机自启，这篇说说centOS7以后的推荐方式`systemd service`。<h1 id="systemd-service"><a href="#systemd-service" class="headerlink" title="systemd service"></a>systemd service</h1><p>systemd Service是systemd提供的用于管理服务启动、停止和相关操作的功能，它极大的简化了服务管理的配置过程，用户只需要配置几项指令即可。</p><p>systemd service是systemd所管理的其中一项内容。实际上，systemd service是Systemd Unit的一种，除了Service，systemd还有其他几种类型的unit，比如service、socket、slice、scope、target等等。在这里，暂时了解两项内容：</p><ul><li><p>Service类型，定义服务程序的启动、停止、重启等操作和进程相关属性</p></li><li><p>Target类型，主要目的是对Service(也可以是其它Unit)进行分组、归类，可以包含一个或多个Service Unit(也可以是其它Unit)</p></li></ul><blockquote><p>systemd管理服务的一些亮点：</p><ol><li>用户可以直接在Service配置文件中定义CGroup相关指令来对该服务程序做资源限制。</li><li>用户可以选择Journal日志而非采用rsyslog，这意味着用户可以不用单独去配置rsyslog，而且可以直接通过systemctl或journalctl命令来查看某服务的日志信息。当然，该功能并不适用于所有情况，比如用户需要管理日志时</li><li>Systemd Service还有其它一些特性，比如可以动态修改服务管理配置文件，比如可以并行启动非依赖的服务，从而加速开机过程等等。</li></ol></blockquote><h1 id="systemd服务配置文件存放路径"><a href="#systemd服务配置文件存放路径" class="headerlink" title="systemd服务配置文件存放路径"></a>systemd服务配置文件存放路径</h1><p>systemd 默认从目录<code>/etc/systemd/system/</code>读取配置文件。里面存放的大部分文件都是符号链接，真正的配置文件存放在<code>/usr/lib/systemd/system/</code>，如果用户需要，可以将服务配置文件手动存放至用户配置目录<code>/etc/systemd/system</code>下。该目录下的服务配置文件可以是普通.service文件，也可以是链接至<code>/usr/lib/systemd/system</code>目录下服务配置文件的软链接。</p><p>位于<code>/usr/lib/systemd/system</code>下的服务配置文件:</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">[root@slions_pc1 ~]# ls -l /usr/lib/systemd/system/*.service|head -10</span><br><span class="line">-rw-r--r--. 1 root root  275 11月 14 2018 /usr/lib/systemd/system/abrt-ccpp.service</span><br><span class="line">-rw-r--r--. 1 root root  380 11月 14 2018 /usr/lib/systemd/system/abrtd.service</span><br><span class="line">-rw-r--r--. 1 root root  361 11月 14 2018 /usr/lib/systemd/system/abrt-oops.service</span><br><span class="line">-rw-r--r--. 1 root root  266 11月 14 2018 /usr/lib/systemd/system/abrt-pstoreoops.service</span><br><span class="line">-rw-r--r--. 1 root root  262 11月 14 2018 /usr/lib/systemd/system/abrt-vmcore.service</span><br><span class="line">-rw-r--r--. 1 root root  311 11月 14 2018 /usr/lib/systemd/system/abrt-xorg.service</span><br><span class="line">-rw-r--r--. 1 root root  275 10月 31 2018 /usr/lib/systemd/system/arp-ethers.service</span><br><span class="line">-rw-r--r--. 1 root root  222 10月 31 2018 /usr/lib/systemd/system/atd.service</span><br><span class="line">-rw-r--r--. 1 root root 1384 8月   8 2019 /usr/lib/systemd/system/auditd.service</span><br><span class="line">lrwxrwxrwx. 1 root root   14 5月  21 16:45 /usr/lib/systemd/system/autovt@.service -&gt; getty@.service</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>下面这些目录(*.target.wants)定义各种类型下需要运行的服务：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">[root@slions_pc1 ~]# ls -1dF /etc/systemd/system/*</span><br><span class="line">/etc/systemd/system/basic.target.wants/</span><br><span class="line">/etc/systemd/system/dbus-org.freedesktop.NetworkManager.service@</span><br><span class="line">/etc/systemd/system/dbus-org.freedesktop.nm-dispatcher.service@</span><br><span class="line">/etc/systemd/system/default.target@</span><br><span class="line">/etc/systemd/system/default.target.wants/</span><br><span class="line">/etc/systemd/system/getty.target.wants/</span><br><span class="line">/etc/systemd/system/local-fs.target.wants/</span><br><span class="line">/etc/systemd/system/multi-user.target.wants/</span><br><span class="line">/etc/systemd/system/network-online.target.wants/</span><br><span class="line">/etc/systemd/system/sockets.target.wants/</span><br><span class="line">/etc/systemd/system/sysinit.target.wants/</span><br><span class="line">/etc/systemd/system/system-update.target.wants/</span><br><span class="line">/etc/systemd/system/vmtoolsd.service.requires/</span><br></pre></td></tr></table></figure><p>/etc/systemd/system/multi-user.target.wants下的服务配置文件，几乎都是软链接</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">[root@slions_pc1 ~]# ls -l /etc/systemd/system/multi-user.target.wants/ | awk &#x27;&#123;print $9,$10,$11&#125;&#x27;</span><br><span class="line">abrt-ccpp.service -&gt; /usr/lib/systemd/system/abrt-ccpp.service</span><br><span class="line">abrtd.service -&gt; /usr/lib/systemd/system/abrtd.service</span><br><span class="line">abrt-oops.service -&gt; /usr/lib/systemd/system/abrt-oops.service</span><br><span class="line">abrt-vmcore.service -&gt; /usr/lib/systemd/system/abrt-vmcore.service</span><br><span class="line">abrt-xorg.service -&gt; /usr/lib/systemd/system/abrt-xorg.service</span><br><span class="line">atd.service -&gt; /usr/lib/systemd/system/atd.service</span><br><span class="line">auditd.service -&gt; /usr/lib/systemd/system/auditd.service</span><br><span class="line">chronyd.service -&gt; /usr/lib/systemd/system/chronyd.service</span><br><span class="line">crond.service -&gt; /usr/lib/systemd/system/crond.service</span><br><span class="line">irqbalance.service -&gt; /usr/lib/systemd/system/irqbalance.service</span><br><span class="line">kdump.service -&gt; /usr/lib/systemd/system/kdump.service</span><br><span class="line">libstoragemgmt.service -&gt; /usr/lib/systemd/system/libstoragemgmt.service</span><br><span class="line">mdmonitor.service -&gt; /usr/lib/systemd/system/mdmonitor.service</span><br></pre></td></tr></table></figure><h1 id="systemd-service文件格式"><a href="#systemd-service文件格式" class="headerlink" title="systemd service文件格式"></a>systemd service文件格式</h1><p>基本的配置文件格式如下：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">[Unit]</span><br><span class="line">Description = some descriptions</span><br><span class="line">Documentation = man:xxx(8) man:xxx_config(5)</span><br><span class="line">Requires = xxx1.target xxx2.target</span><br><span class="line">After = yyy1.target yyy2.target</span><br><span class="line"></span><br><span class="line">[Service]</span><br><span class="line">Type = &lt;TYPE&gt;</span><br><span class="line">ExecStart = &lt;CMD_for_START&gt;</span><br><span class="line">ExecStop = &lt;CMD_for_STOP&gt;</span><br><span class="line">ExecReload = &lt;CMD_for_RELOAD&gt;</span><br><span class="line">Restart = &lt;WHEN_TO_RESTART&gt;</span><br><span class="line">RestartSec = &lt;TIME&gt;</span><br><span class="line"></span><br><span class="line">[Install]</span><br><span class="line">WantedBy = xxx.target yy.target</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>一个.Service配置文件分为三部分：</p><ul><li>Unit：定义该服务作为Unit角色时相关的属性</li><li>Service：定义本服务相关的属性</li><li>Install：定义本服务在设置服务开机自启动时相关的属性。换句话说，只有在创建/移除服务配置文件的软链接时，Install段才会派上用场。这一配置段不是必须的，<strong>当未配置[Install]时，设置开机自启动或禁止开机自启动的操作将无任何效果</strong></li></ul><h1 id="参考文档"><a href="#参考文档" class="headerlink" title="参考文档"></a>参考文档</h1><p><a href="http://www.jinbuguo.com/systemd/systemd.service.html">http://www.jinbuguo.com/systemd/systemd.service.html</a></p><p><a href="https://www.junmajinlong.com/linux/systemd/service_1/">https://www.junmajinlong.com/linux/systemd/service_1/</a></p>]]></content>
    
    
      
      
    <summary type="html">&lt;a href=&quot;/2021/08/01/%E5%88%9B%E5%BB%BA%E5%BC%80%E6%9C%BA%E8%87%AA%E5%90%AF%E7%A8%8B%E5%BA%8F%EF%BC%88%E4%B8%8A%E7%AF%87%EF%BC%89/&quot; title=&quot;上</summary>
      
    
    
    
    
    <category term="linux" scheme="https://slions.github.io/tags/linux/"/>
    
  </entry>
  
  <entry>
    <title>创建开机自启程序（上篇）</title>
    <link href="https://slions.github.io/2021/08/01/%E5%88%9B%E5%BB%BA%E5%BC%80%E6%9C%BA%E8%87%AA%E5%90%AF%E7%A8%8B%E5%BA%8F%EF%BC%88%E4%B8%8A%E7%AF%87%EF%BC%89/"/>
    <id>https://slions.github.io/2021/08/01/%E5%88%9B%E5%BB%BA%E5%BC%80%E6%9C%BA%E8%87%AA%E5%90%AF%E7%A8%8B%E5%BA%8F%EF%BC%88%E4%B8%8A%E7%AF%87%EF%BC%89/</id>
    <published>2021-08-01T04:58:46.000Z</published>
    <updated>2021-08-01T10:34:17.335Z</updated>
    
    <content type="html"><![CDATA[<p>在我们实现如何创建开机自启程序前，先简单了解下linux的启动过程。</p><h1 id="linux启动过程"><a href="#linux启动过程" class="headerlink" title="linux启动过程"></a>linux启动过程</h1><p>Linux 系统的启动，从计算机开机通电自检开始，一直到登陆系统，需要经历多个过程。</p><h2 id="CentOS-RHEL6"><a href="#CentOS-RHEL6" class="headerlink" title="CentOS/RHEL6"></a>CentOS/RHEL6</h2><ol><li>服务器加电，加载 BIOS 信息，BIOS 进行系统检测。依照 BIOS 设定，找到第一个可以启动的设备（一般是硬盘）；</li><li>读取第一个启动设备的 MBR (主引导记录），加载 MBR 中的 Boot Loader（启动引导程序，最为常见的是 GRUB）。</li><li>依据 Boot Loader 的设置加载内核，内核会再进行一遍系统检测。系统一般会采用内核检测硬件的信息，而不一定采用 Bios 的自检信息。内核在检测硬件的同时，还会通过加载动态模块的形式加载硬件的驱动。</li><li>内核启动系统的第一个进程，也就是 /sbin/init。</li><li>由 /sbin/init 进程调用 /etc/init/rcS.conf 配置文件，通过这个配置文件调用 /etc/rc.d/rc.sysinit 配置文件。而 /etc/rc.d/rc.sysinit 配置文件是用来进行系统初始化的，主要用于配置计算机的初始环境。</li><li>还是通过 /etc/init/rcS.conf 配置文件调用 /etc/inittab 配置文件。通过 /etc/inittab 配置文件来确定系统的默认运行级别。</li><li>确定默认运行级别后，调用 /etc/init/rc.conf 配置文件。</li><li>通过 /etc/init/rc.conf 配置文件调用并执行 /etc/rc.d/rc 脚本，并传入运行级别参数。</li><li>/etc/rc.d/rc 确定传入的运行级别，然后运行相应的运行级别目录 /etc/rc[0-6].d/ 中的脚本。</li><li>/etc/rc[0-6].d/ 目录中的脚本依据设定好的优先级依次启动和关闭。</li><li>最后执行 /etc/rc.d/rc.local 中的程序。</li><li>如果是字符界面启动，就可以看到登录界面了。如果是图形界面启动，就会调用相应的 X Window 接口。</li></ol><p><img src="/doc_picture/2-1Q02310563a22.jpg" alt="img"></p><h2 id="CentOS-RHEL7"><a href="#CentOS-RHEL7" class="headerlink" title="CentOS/RHEL7"></a>CentOS/RHEL7</h2><ol><li><p>服务器加电，加载 BIOS 信息，BIOS 进行系统检测。依照 BIOS 设定，找到第一个可以启动的设备（一般是硬盘）；</p></li><li><p>读取第一个启动设备的 MBR (主引导记录），加载 MBR 中的 OSLoader（启动引导程序GRUB2）。</p></li><li><p>OSLoader 加载其相关配置 , 并显示相关的配置 菜单来引导用户选择相关操作(/etc/grub.d,/etc/default/grub,/boot/grub2/grub.cfg)</p></li><li><p>在用户选择后 ( 或 timeout 后 ),GRUB2 将加载 内核及 initramfs 至内存中 .initramfs 属于一个 img 的虚拟磁盘。</p><p>initramfs 包含了动态的内核模块 , 初始化脚本及非常多的硬件驱动等 , 在 RHEL7 中 initramfs 自身即包含了一个完整的可用系统。（/etc/dracut.conf）</p></li><li><p>GRUB2 将控制系统切换到 kernel, 通过对 GRUB2 的控制可以添加各种 kernel 的选项 . 并将 这些选项同时传递至内存中 , 影响 kernel 及 initramfs 的运行 . (/etc/grub.d,/etc/default/grub,/boot/grub2/gr ub.cfg)</p></li><li><p>kernel 在启动后将初始化所有的硬件 , 通过 initramfs 找到硬件相关的驱动程序 , 而后从 initramfs 中执行 PID 1 的 /sbin/init 命令 ( 最高 进程 ). 在 RHEL 中 init 属 于 /lib/systemd/systemd 的软连接 , 以及一个 udev 进程来自动建立已经存在的硬件的设备。（/etc/fstab）</p></li><li><p>由 initramfs 建立的内存的根分区 /sysroot( 物理 ‘/‘ 分区 ) 在成功挂载之后，将切 换到此根分区上 , 并将 systemd 重新执行安装至真实的根分区中。</p></li><li><p> systemd 开始查找所有的服务 , 开始执行 ( 停止 ) 相关的服务 , 以符合该服务的配置并解决相关的依赖问题。（/etc/systemd/system/default.target）</p></li></ol><blockquote><p>Systemd引入了并行启动的概念，它会为每个需要启动的守护进程建立一个套接字，这些套接字对于使用它们的进程来说是抽象的，这样它们可以允许不同守护进程之间进行交互。Systemd会创建新进程并为每个进程分配一个控制组（cgroup）。处于不同控制组的进程之间可以通过内核来互相通信。</p><p>从 7.x 版本开始，引入了 systemd 来管理服务，/etc/rc.local 是为向前兼容而保留。</p></blockquote><h1 id="开机自启"><a href="#开机自启" class="headerlink" title="开机自启"></a>开机自启</h1><p>我们现在大多都在使用CentOS/RHEL7中，实现开机启动程序主要有两种方法：</p><ul><li><p>在/etc/rc.local脚本文件中编写启动程序的脚本</p></li><li><p>把要启动的程序配置成自定义的systemd服务（推荐）</p></li></ul><h2 id="通过rc-local实现开机自启程序"><a href="#通过rc-local实现开机自启程序" class="headerlink" title="通过rc.local实现开机自启程序"></a>通过rc.local实现开机自启程序</h2><p>/etc/rc.local是/etc/rc.d/rc.local的软链接</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@slions_pc1 home]# ll /etc/rc.local</span><br><span class="line">lrwxrwxrwx. 1 root root 13 5月  21 16:45 /etc/rc.local -&gt; rc.d/rc.local</span><br></pre></td></tr></table></figure><h3 id="rc-local配置文件解读"><a href="#rc-local配置文件解读" class="headerlink" title="rc.local配置文件解读"></a>rc.local配置文件解读</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">[root@slions_pc1 home]# cat /etc/rc.local</span><br><span class="line"><span class="meta">#</span><span class="bash">!/bin/bash</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> THIS FILE IS ADDED FOR COMPATIBILITY PURPOSES</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 添加此文件是为了兼容。</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> It is highly advisable to create own systemd services or udev rules</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> to run scripts during boot instead of using this file.</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 强烈建议创建自己的systemd服务或udev规则，以便在引导期间运行脚本，而不是使用此文件。</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> In contrast to previous versions due to parallel execution during boot</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> this script will NOT be run after all other services.</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 请注意，必须运行<span class="string">&#x27;chmod+x/etc/rc.d/rc.local&#x27;</span>，以确保在引导期间执行此脚本。</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> Please note that you must run <span class="string">&#x27;chmod +x /etc/rc.d/rc.local&#x27;</span> to ensure</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> that this script will be executed during boot.</span></span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash">默认会touch这个文件，每次系统启动时都会touch这个文件，这个文件的修改时间就是系统的启动时间</span></span><br><span class="line">touch /var/lock/subsys/local</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>rc.local本质上是一个shell脚本文件，可以把启动时需要执行的命令写在里面，启动时将按顺序执行。</p><h3 id="rc-local编写注意"><a href="#rc-local编写注意" class="headerlink" title="rc.local编写注意"></a>rc.local编写注意</h3><ul><li>rc.local脚本在操作系统启动时只执行一次</li><li>环境变量的问题<ul><li>在rc.local脚本中执行程序时是没有环境变量的，如果执行的程序需要环境变量，可以在脚本中设置环境变量</li></ul></li><li>命令需写绝对路径</li><li>不要让rc.local挂起<ul><li>rc.local是一个脚本，是按顺序执行的，执行完一个程序后才会执行下一个程序，如果某程序不是后台程序，就应该加&amp;让程序运行在后台，否则rc.local会挂起。</li></ul></li><li>切记chmod+x/etc/rc.d/rc.local赋予执行权限</li></ul><h2 id="通过system实现开机自启程序"><a href="#通过system实现开机自启程序" class="headerlink" title="通过system实现开机自启程序"></a>通过system实现开机自启程序</h2><h3 id="systemd可管理的服务"><a href="#systemd可管理的服务" class="headerlink" title="systemd可管理的服务"></a>systemd可管理的服务</h3><p>操作系统使用systemd后，所有用户进程都是systemd的后代进程。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line">[root@slions_pc1 ~]# pstree -p</span><br><span class="line">systemd(1)─┬─NetworkManager(8829)─┬─&#123;NetworkManager&#125;(8881)</span><br><span class="line">           │                      └─&#123;NetworkManager&#125;(8883)</span><br><span class="line">           ├─VGAuthService(8844)</span><br><span class="line">           ├─abrt-dbus(14768)─┬─&#123;abrt-dbus&#125;(14802)</span><br><span class="line">           │                  └─&#123;abrt-dbus&#125;(14813)</span><br><span class="line">           ├─abrt-watch-log(8832)</span><br><span class="line">           ├─abrtd(8830)</span><br><span class="line">           ├─atd(8850)</span><br><span class="line">           ├─auditd(8793)───&#123;auditd&#125;(8794)</span><br><span class="line">           ├─chronyd(8869)</span><br><span class="line">           ├─crond(8853)</span><br><span class="line">           ├─dbus-daemon(8821)───&#123;dbus-daemon&#125;(8827)</span><br><span class="line">           ├─irqbalance(8834)</span><br><span class="line">           ├─login(8866)───bash(14656)</span><br><span class="line">           ├─lsmd(8849)</span><br><span class="line">           ├─lvmetad(4420)</span><br><span class="line">           ├─master(9397)─┬─pickup(9415)</span><br><span class="line">           │              └─qmgr(9416)</span><br><span class="line">           ├─polkitd(8818)─┬─&#123;polkitd&#125;(8826)</span><br><span class="line">           │               ├─&#123;polkitd&#125;(8828)</span><br><span class="line">           │               ├─&#123;polkitd&#125;(8833)</span><br><span class="line">           │               ├─&#123;polkitd&#125;(8839)</span><br><span class="line">           │               ├─&#123;polkitd&#125;(8840)</span><br><span class="line">           │               └─&#123;polkitd&#125;(8841)</span><br><span class="line">           ├─rngd(8835)</span><br><span class="line">           ├─rpcbind(8824)</span><br><span class="line">           ├─rsyslogd(9174)─┬─&#123;rsyslogd&#125;(9237)</span><br><span class="line">           │                └─&#123;rsyslogd&#125;(9258)</span><br><span class="line">           ├─smartd(8820)</span><br><span class="line">           ├─sshd(9171)─┬─sshd(19034)───bash(19040)───pstree(19161)</span><br><span class="line">           │            └─sshd(19038)───sftp-server(19075)</span><br><span class="line">           ├─systemd-journal(4398)</span><br><span class="line">           ├─systemd-logind(8846)</span><br><span class="line">           ├─systemd-udevd(4427)</span><br><span class="line">           ├─tuned(9172)─┬─&#123;tuned&#125;(9926)</span><br><span class="line">           │             ├─&#123;tuned&#125;(9928)</span><br><span class="line">           │             ├─&#123;tuned&#125;(9984)</span><br><span class="line">           │             └─&#123;tuned&#125;(10117)</span><br><span class="line">           └─vmtoolsd(8845)───&#123;vmtoolsd&#125;(8935)</span><br></pre></td></tr></table></figure><p>虽然从进程树关系来看，所有进程都直接或间接地受到systemd的管理，但是，并非所有systemd的子进程都受Systemd Unit管理单元的管理。只有那些由systemd方式启动的服务进程(比如systemctl命令启动)才受到Systemd Unit管理单元的监控和管理。为了简化描述，后面均直接以『systemd管理』来描述受systemd unit管理单元的管理。</p><p>比如，用户可以通过下面两种方式启动Nginx服务进程：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">nginx                    # (1)</span><br><span class="line">systemctl start nginx    # (2)</span><br></pre></td></tr></table></figure><p>但systemd只能监控、管理第(2)种方式启动的nginx服务。比如第一种方式启动的nginx，无法使用systemctl stop nginx来停止。</p><h3 id="systemd管理服务的命令"><a href="#systemd管理服务的命令" class="headerlink" title="systemd管理服务的命令"></a>systemd管理服务的命令</h3><p><code>systemctl</code>是 Systemd 的主命令，用于管理系统。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 启动、停止服务:</span></span><br><span class="line">systemctl start Service_Name1 Service_Name2</span><br><span class="line">systemctl stop Service_Name</span><br><span class="line"><span class="meta">#</span><span class="bash"> 开机启动服务：</span></span><br><span class="line">systemctl enable Service_Name</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 有时候，该命令可能没有响应，服务停不下来。这时候就不得不<span class="string">&quot;杀进程&quot;</span>了，向正在运行的进程发出<span class="built_in">kill</span>信号。</span></span><br><span class="line">systemctl kill Service_Name</span><br><span class="line"><span class="meta">#</span><span class="bash"> 服务重载、重启相关操作：</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 重载服务：服务未运行时不做任何事</span></span><br><span class="line">systemctl reload Service_Name</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 重启服务：服务已运行时重启之，服务未运行时启动之</span></span><br><span class="line">systemctl restart Service_Name</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 服务已运行时重启之，未运行时不启动之</span></span><br><span class="line">systemctl try-restart Service_Name</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 服务已运行时，如果支持reload，则reload，如果不支持则restart</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 服务未运行时，启动之</span></span><br><span class="line">systemctl reload-or-restart Service_Name</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 服务已运行时，如果支持reload，则reload，如果不支持则restart</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 服务未运行时，不做任何事</span></span><br><span class="line">systemctl reload-or-try-restart Service_Name</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 查看服务状态</span></span><br><span class="line">systemctl status Service_Name</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 检查服务是否active: 服务是否已启动</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 至少一个服务active时，返回0，否则返回非0退出状态码</span></span><br><span class="line">systemctl is-active Service_Name1 Service_Name2</span><br><span class="line">systemctl --quiet is-active Service_Name  # 静默模式</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 检查服务是否failed: 服务启动命令退出状态码非0或启动超时</span></span><br><span class="line">systemctl is-failed Service_Name</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>因为篇幅问题，<a href="/2021/08/01/%E5%88%9B%E5%BB%BA%E5%BC%80%E6%9C%BA%E8%87%AA%E5%90%AF%E7%A8%8B%E5%BA%8F%EF%BC%88%E4%B8%8B%E7%AF%87%EF%BC%89/" title="下篇继续介绍如何编写systemd service">下篇继续介绍如何编写systemd service</a></p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;在我们实现如何创建开机自启程序前，先简单了解下linux的启动过程。&lt;/p&gt;
&lt;h1 id=&quot;linux启动过程&quot;&gt;&lt;a href=&quot;#linux启动过程&quot; class=&quot;headerlink&quot; title=&quot;linux启动过程&quot;&gt;&lt;/a&gt;linux启动过程&lt;/h1&gt;&lt;p&gt;</summary>
      
    
    
    
    
    <category term="linux" scheme="https://slions.github.io/tags/linux/"/>
    
  </entry>
  
  <entry>
    <title>alias与rm的最佳实践</title>
    <link href="https://slions.github.io/2021/08/01/alias%E4%B8%8Erm%E7%9A%84%E6%9C%80%E4%BD%B3%E5%AE%9E%E8%B7%B5/"/>
    <id>https://slions.github.io/2021/08/01/alias%E4%B8%8Erm%E7%9A%84%E6%9C%80%E4%BD%B3%E5%AE%9E%E8%B7%B5/</id>
    <published>2021-08-01T03:37:38.000Z</published>
    <updated>2021-08-01T04:56:54.058Z</updated>
    
    <content type="html"><![CDATA[<p>不论是刚入职的运维萌新，还是工作多年的运维老油条，当在服务器上敲下rm时必须要保证命令的准确，在技术贴吧中也常常有人调侃到不想干了就<code>rm -rf /</code>与删库跑路等等的段子，如何让<code>rm</code>的操作更有保障呢，以下会提供一种解题思路。</p><h1 id="alias的用法"><a href="#alias的用法" class="headerlink" title="alias的用法"></a>alias的用法</h1><p>Linux alias命令用于设置指令的别名。</p><p>语法：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">alias[别名]=[指令名称]</span><br></pre></td></tr></table></figure><p>使用不带参数的alias将列出当前shell环境下所有的已定义的别名。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">[root@slions_pc1 home]# alias</span><br><span class="line">alias cp=&#x27;cp -i&#x27;</span><br><span class="line">alias egrep=&#x27;egrep --color=auto&#x27;</span><br><span class="line">alias fgrep=&#x27;fgrep --color=auto&#x27;</span><br><span class="line">alias grep=&#x27;grep --color=auto&#x27;</span><br><span class="line">alias l.=&#x27;ls -d .* --color=auto&#x27;</span><br><span class="line">alias ll=&#x27;ls -l --color=auto&#x27;</span><br><span class="line">alias ls=&#x27;ls --color=auto&#x27;</span><br><span class="line">alias mv=&#x27;mv -i&#x27;</span><br><span class="line">alias rm=&#x27;rm -i&#x27;</span><br><span class="line">alias which=&#x27;alias | /usr/bin/which --tty-only --read-alias --show-dot --show-tilde&#x27;</span><br></pre></td></tr></table></figure><p>另外需要说明的是，当别名和命令同名时，将优先执行别名(否则别名就没有意义了)，这可以从which的结果中看出：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@slions_pc1 home]# which rm</span><br><span class="line">alias rm=&#x27;rm -i&#x27;</span><br><span class="line">        /usr/bin/rm</span><br></pre></td></tr></table></figure><p>如果定义的命名名称和原始命令同名(例如定义的别名 ls=’ls -l’ )，此时如果想要明确使用原始命令，可以删除别名或者使用绝对路径或者使用转义符来还原命令。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[root@slions_pc1 home]# \ls</span><br><span class="line">slions</span><br><span class="line">[root@slions_pc1 home]# /usr/bin/ls</span><br><span class="line">slions</span><br></pre></td></tr></table></figure><p>alias命令是临时定义别名，要定义长久生效的别名就将别名定义语句写入<code>/etc/profile</code>或<code>~/.bash_profile</code>或<code>~/.bashrc</code>，第一个对所有用户有效，后面两个对对应用户有效。修改后记得使用source来重新调取这些配置文件。</p><p>使用unalias可以临时取消别名。</p><h1 id="alias与rm"><a href="#alias与rm" class="headerlink" title="alias与rm"></a>alias与rm</h1><p>如何让alias与rm配合起来能减少误操作引发的影响呢，整体思路就是rm变为mv操作，后续给备份目录增加个时效性机制，保障磁盘空间的可用性。</p><h2 id="alias的bug"><a href="#alias的bug" class="headerlink" title="alias的bug"></a>alias的bug</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">[root@slions_pc1 home]# ls</span><br><span class="line">slions</span><br><span class="line">[root@slions_pc1 home]# touch testfile</span><br><span class="line">[root@slions_pc1 home]# alias rmm=&#x27;cp $@ /tmp/backup;rm $@&#x27;</span><br><span class="line">[root@slions_pc1 home]# rmm testfile</span><br><span class="line">cp: 在&quot;/tmp/backup&quot; 后缺少了要操作的目标文件</span><br><span class="line">Try &#x27;cp --help&#x27; for more information.</span><br><span class="line">rm：是否删除普通空文件 &quot;testfile&quot;？y</span><br><span class="line">[root@slions_pc1 home]# ls</span><br><span class="line">slions</span><br><span class="line">[root@slions_pc1 home]# ls /tmp/backup/</span><br></pre></td></tr></table></figure><p>该别名的目的是删除文件时先备份到<code>/tmp/backup</code>目录下，然后再删除。<strong>按照man bash里的说明，别名rmm只是第一个cp命令的别名，分号后的rm不是别名的一部分，而是紧跟在别名后的下一行命令。当执行别名rmm时，首先读取别名到分号位置处，然后进行别名扩展，执行完别名命令后，再执行分号后的rm命令。</strong></p><p>上面的命令没有达到预期效果，问题出在cp的参数”$@”，该变量本表示提供的所有参数，但由于cp命令后使用分号分隔并定义了另一个命令，这使得执行别名命令时，参数无法传递到cp命令上，而只能传递到最后一个命令rm上，也就是说cp后的”$@”是空值。</p><p>可以测试下：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[root@slions_pc1 home]# touch testfile</span><br><span class="line">[root@slions_pc1 home]# alias rmm=&#x27;echo cp $@ /tmp/backup;echo rm $@&#x27;</span><br><span class="line">[root@slions_pc1 home]# rmm testfile</span><br><span class="line">cp /tmp/backup</span><br><span class="line">rm testfile</span><br></pre></td></tr></table></figure><p>从上面的结果中看到cp后的”$@”根本就没有进行扩展，而是空值。</p><p>那如果别名定义语句中没有使用分号或其他方法定义额外的命令，而是只有一个命令呢？别名一定就能正确工作吗？</p><p>测试会发现也有问题：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@slions_pc1 home]# alias rmm=&#x27;echo cp $@ /tmp/backup&#x27;</span><br><span class="line">[root@slions_pc1 home]# rmm testfile</span><br><span class="line">cp /tmp/backup testfile</span><br></pre></td></tr></table></figure><p>之所以无法正常工作，是因为<code>/tmp/backup</code>也是”$@”的一部分，且是”$@”中最前面的参数。</p><p>从上面的分析可以知道，alias是有其缺陷的，它只适合进行简单的命令和参数替换、补全，想要实现复杂的命令替代有点难度。因此man bash中建议尽量使用<strong>函数</strong>来取代别名。</p><h2 id="alias最佳实践"><a href="#alias最佳实践" class="headerlink" title="alias最佳实践"></a>alias最佳实践</h2><p>例如，为了让rm安全执行，使用以下方法定义别名：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[root@slions_pc1 home]# alias rmm=&#x27;move()&#123; /bin/mv -f $@ /tmp/backup; &#125;;move $@&#x27;</span><br><span class="line">[root@slions_pc1 home]# rmm testfile</span><br><span class="line">[root@slions_pc1 home]# ls</span><br><span class="line">slions</span><br><span class="line">[root@slions_pc1 home]# ls /tmp/backup/</span><br><span class="line">testfile</span><br></pre></td></tr></table></figure><p>因为执行别名时的参数只能传递给最后一个命令即move函数，但”$@”代表的参数可以传递给函数，让函数中的”$@”得到正确的扩展，于是整个别名都能合理且正确地执行。</p><p>或者直接定义一个shell function替代rm。例如向/etc/profile.d/rm.sh文件中写入：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">function rm()&#123; [ -d /tmp/rmbackup ] || mkdir /tmp/rmbackup;/bin/mv -f $@ /tmp/rmbackup; &#125;</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@slions_pc1 home]# chmod +x /etc/profile.d/rm.sh</span><br><span class="line">[root@slions_pc1 home]# source /etc/profile.d/rm.sh</span><br></pre></td></tr></table></figure><p>如此，执行rm命令时，便会执行此处定义的rm函数，使得rm变得更安全。但注意，这样的函数默认无法直接在脚本中使用，除非使用 <code>export -f function_name </code>导出函数，使其可以被子shell继承。</p><p>所以，可在/etc/profile.d/rm.sh文件的尾部加上导出语句：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">function rm()&#123; [ -d /tmp/rmbackup ] || mkdir /tmp/rmbackup;/bin/mv -f $@ /tmp/rmbackup; &#125;</span><br><span class="line">export -f rm</span><br></pre></td></tr></table></figure><p>如果function名和命令名相同，则默认优先执行function，除非使用command明确指定。例如上面定义了rm函数，如果想执行rm命令，除了使用/bin/rm，还可以如下操作：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">command rm testfile</span><br></pre></td></tr></table></figure><p>最后，如果是**在shell脚本里涉及到rm命令，那么更建议在每次rm之前先cd到那个目录下，然后再rm相对路径，这样至少能保证不出现符号”/“**。当然，最重要的是习惯和责任心，切勿忙中出错。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;不论是刚入职的运维萌新，还是工作多年的运维老油条，当在服务器上敲下rm时必须要保证命令的准确，在技术贴吧中也常常有人调侃到不想干了就&lt;code&gt;rm -rf /&lt;/code&gt;与删库跑路等等的段子，如何让&lt;code&gt;rm&lt;/code&gt;的操作更有保障呢，以下会提供一种解题思路。</summary>
      
    
    
    
    
    <category term="linux" scheme="https://slions.github.io/tags/linux/"/>
    
  </entry>
  
  <entry>
    <title>linux用户与组管理（下篇）</title>
    <link href="https://slions.github.io/2021/07/31/linux%E7%94%A8%E6%88%B7%E4%B8%8E%E7%BB%84%E7%AE%A1%E7%90%86%EF%BC%88%E4%B8%8B%E7%AF%87%EF%BC%89/"/>
    <id>https://slions.github.io/2021/07/31/linux%E7%94%A8%E6%88%B7%E4%B8%8E%E7%BB%84%E7%AE%A1%E7%90%86%EF%BC%88%E4%B8%8B%E7%AF%87%EF%BC%89/</id>
    <published>2021-07-31T07:27:52.000Z</published>
    <updated>2021-07-31T08:17:10.390Z</updated>
    
    <content type="html"><![CDATA[<h1 id="用户和组管理命令"><a href="#用户和组管理命令" class="headerlink" title="用户和组管理命令"></a>用户和组管理命令</h1><h2 id="useradd-和-adduser"><a href="#useradd-和-adduser" class="headerlink" title="useradd 和 adduser"></a>useradd 和 adduser</h2><p>adduser是useradd的一个软链接。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@slions_pc1 ~]# ls -l /usr/sbin/adduser</span><br><span class="line">lrwxrwxrwx. 1 root root 7 5月  21 16:45 /usr/sbin/adduser -&gt; useradd</span><br></pre></td></tr></table></figure><p>useradd用法：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line">useradd [options] login_name</span><br><span class="line">选项说明：</span><br><span class="line">-b：指定家目录的basedir，默认为/home目录</span><br><span class="line">-d：指定用户家目录，不写时默认为/home/user_name</span><br><span class="line">-m：要创建家目录时，若家目录不存在则自动创建，若不指定该项且/etc/login.defs中的CREATE_HOME未启用时将不会创建家目录</span><br><span class="line">-M：显式指明不要创建家目录，会覆盖/etc/login.defs中的CREATE_HOME设置</span><br><span class="line"> </span><br><span class="line">-g：指定用户主组，要求组已存在</span><br><span class="line">-G：指定用户的辅助组，多个组以逗号分隔</span><br><span class="line">-N：明确指明不要创建和用户名同名的组名</span><br><span class="line">-U：明确指明要创建一个和用户名同名的组，并将用户加入到此组中</span><br><span class="line"></span><br><span class="line">-o：允许创建一个重复UID的用户，只有和-u选项同时使用时才生效</span><br><span class="line">-r：创建一个系统用户。useradd命令不会为此选项的系统用户创建家目录，除非明确使用-m选项</span><br><span class="line">-s：指定用户登录的shell，默认留空。此时将选择/etc/default/useradd中的SHELL变量设置</span><br><span class="line">-u：指定用户uid，默认uid必须唯一，除非使用了-o选项</span><br><span class="line">-c：用户的注释信息 </span><br><span class="line"></span><br><span class="line">-k：指定骨架目录(skeleton)</span><br><span class="line">-K：修改/etc/login.defs文件中有关于用户的配置项，不能修改组相关的配置。设置方式为KEY=VALUE，如-K UID_MIN=100</span><br><span class="line">-D：修改useradd创建用户时的默认选项，就修改/etc/default/useradd文件</span><br><span class="line">-e：帐户过期时间，格式为&quot;YYYY-MM-DD&quot;</span><br><span class="line">-f：密码过期后，该账号还能存活多久才被禁用，设置为0表示密码过期立即禁用帐户，设置为-1表示禁用此功能</span><br><span class="line">-l：不要将用户的信息写入到lastlog和faillog文件中。默认情况下，用户信息会写入到这两个文件中</span><br><span class="line"></span><br><span class="line">useradd -D [options]</span><br><span class="line">修改/etc/default/useradd文件</span><br><span class="line">选项说明：不加任何选项时会列出默认属性</span><br><span class="line">-b, --base-dir BASE_DIR</span><br><span class="line">-e, --expiredate EXPIRE_DATE</span><br><span class="line">-f, --inactive INACTIVE</span><br><span class="line">-g, --gid GROUP</span><br><span class="line">-s, --shell SHELL</span><br></pre></td></tr></table></figure><p>useradd创建用户时，默认会自动创建一个和用户名相同的用户组，这是<code>/etc/login.defs</code>中的USERGROUP_ENAB变量控制的。</p><p>useradd创建普通用户时，不加任何和家目录相关的选项时，是否创建家目录是由<code>/etc/login.defs</code>中的CREATE_HOME变量控制的。</p><h2 id="批量创建用户-newusers"><a href="#批量创建用户-newusers" class="headerlink" title="批量创建用户 newusers"></a>批量创建用户 newusers</h2><p>newusers用于批量创建或修改已有用户信息。在创建用户时，它会读取<code>/etc/login.defs</code>文件中的配置项。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">[root@slions_pc1 ~]# newusers -h</span><br><span class="line">用法：newusers [选项]</span><br><span class="line"></span><br><span class="line">选项：</span><br><span class="line">  -c, --crypt-method 方法        加密方法(NONE DES MD5 SHA256 SHA512 中的一个)</span><br><span class="line">  -h, --help                    显示此帮助信息并推出</span><br><span class="line">  -r, --system                  创建系统帐号</span><br><span class="line">  -R, --root CHROOT_DIR         chroot 到的目录</span><br><span class="line">  -s, --sha-rounds              SHA* 加密算法中的 SHA 旁边的数字</span><br></pre></td></tr></table></figure><p>newusers命令从file中或标准输入中读取要创建或修改用户的信息，文件中每行格式都一样(同passwd格式)，一行代表一个用户。格式如下：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">name:passwd:uid:gid:note:homedir:shell</span><br></pre></td></tr></table></figure><p>newusers首先尝试创建或修改所有指定的用户，然后将信息写入到user和group的文件中。如果尝试创建或修改用户过程中发生错误，则所有动作都将回滚，但如果在写入过程中发生错误，则写入成功的不会回滚，这将可能导致文件的不一致性。要检查用户、组文件的一致性，可以使用showdow-utils包提供的grpck和pwck命令。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">[root@slions_pc1 ~]# cat batch_adduser</span><br><span class="line">s1:123456:7295:7295::/home/s1:/bin/bash</span><br><span class="line">s2:123456:::::/bin/bash</span><br><span class="line">[root@slions_pc1 ~]# newusers -c SHA512 batch_adduser</span><br><span class="line">[root@slions_pc1 ~]# tail -2 /etc/passwd</span><br><span class="line">s1:x:7295:7295::/home/s1:/bin/bash</span><br><span class="line">s2:x:7296:7296:::/bin/bash</span><br><span class="line">[root@slions_pc1 ~]# tail -2 /etc/shadow</span><br><span class="line">s1:$6$OSCVQmJiFP/U4CbD$72ZkAJNKs4ehMgfxJR..tqNuy7yKHINycOiB/.lW4ANBtuIMuIcsphgw8mcfkR7A1tvhKifG6vmPbc8VjmfmV.:18839:0:99999:7:::</span><br><span class="line">s2:$6$/7nes/BRVe5pw7Gw$qV8.mVjf4Mpv9zVwjsKDSmtmx8qCwZwX03hmsbuFM.CEHJ.X76pX6Css8dvIAE87j7GcihAJN8lSn3Lg.KD.Q.:18839:0:99999:7:::</span><br><span class="line"></span><br></pre></td></tr></table></figure><h2 id="创建组-groupadd"><a href="#创建组-groupadd" class="headerlink" title="创建组 groupadd"></a>创建组 groupadd</h2><p>用法：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">[root@slions_pc1 ~]# groupadd -h</span><br><span class="line">用法：groupadd [选项] 组</span><br><span class="line"></span><br><span class="line">选项:</span><br><span class="line">  -f, --force           如果组已经存在则成功退出</span><br><span class="line">                        并且如果 GID 已经存在则取消 -g</span><br><span class="line">  -g, --gid GID                 为新组使用 GID</span><br><span class="line">  -h, --help                    显示此帮助信息并推出</span><br><span class="line">  -K, --key KEY=VALUE           不使用 /etc/login.defs 中的默认值</span><br><span class="line">  -o, --non-unique              允许创建有重复 GID 的组</span><br><span class="line">  -p, --password PASSWORD       为新组使用此加密过的密码</span><br><span class="line">  -r, --system                  创建一个系统账户</span><br><span class="line">  -R, --root CHROOT_DIR         chroot 到的目录</span><br></pre></td></tr></table></figure><h2 id="修改密码-passwd"><a href="#修改密码-passwd" class="headerlink" title="修改密码 passwd"></a>修改密码 passwd</h2><p>修改密码的工具。默认passwd命令不允许为用户创建空密码。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">[root@slions_pc1 ~]# passwd  --help</span><br><span class="line">用法: passwd [选项...] &lt;帐号名称&gt;</span><br><span class="line">  -k, --keep-tokens       保持身份验证令牌不过期</span><br><span class="line">  -d, --delete            删除已命名帐号的密码(只有根用户才能进行此操作)</span><br><span class="line">  -l, --lock              锁定指名帐户的密码(仅限 root 用户)</span><br><span class="line">  -u, --unlock            解锁指名账户的密码(仅限 root 用户)</span><br><span class="line">  -e, --expire            终止指名帐户的密码(仅限 root 用户)</span><br><span class="line">  -f, --force             强制执行操作</span><br><span class="line">  -x, --maximum=DAYS      密码的最长有效时限(只有根用户才能进行此操作)</span><br><span class="line">  -n, --minimum=DAYS      密码的最短有效时限(只有根用户才能进行此操作)</span><br><span class="line">  -w, --warning=DAYS      在密码过期前多少天开始提醒用户(只有根用户才能进行此操作)</span><br><span class="line">  -i, --inactive=DAYS     当密码过期后经过多少天该帐号会被禁用(只有根用户才能进行此操作)</span><br><span class="line">  -S, --status            报告已命名帐号的密码状态(只有根用户才能进行此操作)</span><br><span class="line">  --stdin                 从标准输入读取令牌(只有根用户才能进行此操作)</span><br></pre></td></tr></table></figure><h2 id="批量修改密码-chpasswd"><a href="#批量修改密码-chpasswd" class="headerlink" title="批量修改密码 chpasswd"></a>批量修改密码 chpasswd</h2><p>以批处理模式从标准输入中获取提供的用户和密码来修改用户密码，可以一次修改多个用户密码。也就是说不用交互。适用于一次性创建了多个用户时为他们提供密码。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">[root@slions_pc1 ~]# chpasswd --help</span><br><span class="line">用法：chpasswd [选项]</span><br><span class="line"></span><br><span class="line">选项：</span><br><span class="line">  -c, --crypt-method 方法        加密方法(NONE DES MD5 SHA256 SHA512 中的一个)</span><br><span class="line">  -e, --encrypted               提供的密码已经加密</span><br><span class="line">  -h, --help                    显示此帮助信息并推出</span><br><span class="line">  -m, --md5             使用 MD5 算法加密明文密码</span><br><span class="line">  -R, --root CHROOT_DIR         chroot 到的目录</span><br><span class="line">  -s, --sha-rounds              SHA* 加密算法中的 SHA 旁边的数字</span><br></pre></td></tr></table></figure><p>chpasswd会读取<code>/etc/login.defs</code>中的相关配置，修改成功后会将密码信息写入到密码文件中。</p><p>该命令的修改密码的处理方式是先在内存中修改，如果所有用户的密码都能设置成功，然后才写入到磁盘密码文件中。在内存中修改过程中出错，则所有修改都回滚，但若在写入密码文件过程中出错，则成功的不会回滚。</p><p>修改单个用户密码。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> <span class="built_in">echo</span> <span class="string">&quot;s1:123456&quot;</span> | chpasswd -c SHA512</span></span><br></pre></td></tr></table></figure><p>修改多个用户密码，则提供的每个用户对都要分行。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> <span class="built_in">echo</span>  -e <span class="string">&#x27;s1:123456\ns2:123456&#x27;</span> | chpasswd</span></span><br></pre></td></tr></table></figure><p>更方便的是写入到文件中，每行一个用户密码对。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> cat /tmp/passwdfile</span></span><br><span class="line">s1:123456</span><br><span class="line">s2:123456</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">$</span><span class="bash"> chapasswd -c SHA512 &lt;/tmp/passwdfile</span></span><br></pre></td></tr></table></figure><h2 id="chage"><a href="#chage" class="headerlink" title="chage"></a>chage</h2><p>chage命令主要修改或查看和密码时间相关的内容。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">-l：列出指定用户密码相关信息</span><br><span class="line">-E：指定帐户(不是密码)过期时间，所以是强锁定，如果指定为0，则立即过期，即直接锁定该用户</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">[root@slions_pc1 ~]# chage -l s1|column -t</span><br><span class="line">最近一次密码修改时间            ：7月    31,  2021</span><br><span class="line">密码过期时间                    ：从不</span><br><span class="line">密码失效时间                    ：从不</span><br><span class="line">帐户过期时间                    ：从不</span><br><span class="line">两次改变密码之间相距的最小天数  ：0</span><br><span class="line">两次改变密码之间相距的最大天数  ：99999</span><br><span class="line">在密码过期之前警告的天数        ：7</span><br><span class="line">[root@slions_pc1 ~]# chage -E 0 s1</span><br><span class="line">[root@slions_pc1 ~]# chage -l s1|column -t</span><br><span class="line">最近一次密码修改时间            ：7月    31,  2021</span><br><span class="line">密码过期时间                    ：从不</span><br><span class="line">密码失效时间                    ：从不</span><br><span class="line">帐户过期时间                    ：1月    01,  1970</span><br><span class="line">两次改变密码之间相距的最小天数  ：0</span><br><span class="line">两次改变密码之间相距的最大天数  ：99999</span><br><span class="line">在密码过期之前警告的天数        ：7</span><br><span class="line"></span><br></pre></td></tr></table></figure><h2 id="删除用户-userdel"><a href="#删除用户-userdel" class="headerlink" title="删除用户 userdel"></a>删除用户 userdel</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">userdel [options] login_name</span><br><span class="line">-r：递归删除家目录，默认不删除家目录。</span><br><span class="line">-f：强制删除用户，即使这个用户正处于登录状态。同时也会强制删除家目录。</span><br></pre></td></tr></table></figure><p>一般不直接删除家目录，即不用-r，可以vim /etc/passwd，将不需要的用户直接注释掉。</p><h2 id="删除组-groupdel"><a href="#删除组-groupdel" class="headerlink" title="删除组 groupdel"></a>删除组 groupdel</h2><p>如果要删除的组是某用户的主组，需要先删除主组中的用户。</p><h2 id="修改帐户属性信息-usermod"><a href="#修改帐户属性信息-usermod" class="headerlink" title="修改帐户属性信息 usermod"></a>修改帐户属性信息 usermod</h2><p>修改帐户属性信息。必须要确保在执行该命令的时候，待修改的用户没有在执行进程。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">usermod [options] login</span><br><span class="line">选项说明：</span><br><span class="line">-l：修改用户名，仅仅只是改用户名，其他的一切都不会改动(uid、家目录等)</span><br><span class="line">-u：新的uid，新的uid必须唯一，除非同时使用了-o选项</span><br><span class="line">-g：修改用户主组，可以是以gid或组名。对于那些以旧组为所属组的文件(除原家目录)，需要重新手动修改其所属组</span><br><span class="line">-m：移动家目录内容到新的位置，该选项只在和-d选项一起使用时才生效</span><br><span class="line">-d：修改用户的家目录位置，若不存在则自动创建。默认旧的家目录不会删除</span><br><span class="line">    如果同时指定了-m选项，则旧的家目录中的内容会移到新家目录</span><br><span class="line">    如果当前用户家目录不存在或没有家目录，则也不会创建新的家目录</span><br><span class="line">-o：允许用户使用非唯一的UID</span><br><span class="line">-s：修改用的shell，留空则选择默认shell</span><br><span class="line">-c：修改用户注释信息</span><br><span class="line"></span><br><span class="line">-a：将用户以追加的方式加入到辅助组中，只能和-G选项一起使用</span><br><span class="line">-G：将用户加入指定的辅助组中，若此处未列出某组，而此前该用户又是该组成员，则会删除该组中此成员</span><br><span class="line"></span><br><span class="line">-L：锁定用户的密码，将在/etc/shadow的密码列加上前缀&quot;!&quot;或&quot;!!&quot;</span><br><span class="line">-U：解锁用户的密码，解锁的方式是移除shadow文件密码列的前缀&quot;!&quot;或&quot;!!&quot;</span><br><span class="line">-e：帐户过期时间，时间格式为&quot;YYYY-MM-DD&quot;，如果给一个空的参数，则立即禁用该帐户</span><br><span class="line">-f：密码过期后多少天，帐户才过期被禁用，0表示密码过期帐户立即禁用，-1表示禁用该功能</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>同样，还有groupmod修改组信息，用法非常简单，几乎也用不上，不多说了。</p><h2 id="vipw和vigr"><a href="#vipw和vigr" class="headerlink" title="vipw和vigr"></a>vipw和vigr</h2><p>vipw和vigr是编辑用户和组文件的工具，vipw可以修改/etc/passwd和/etc/shadow，vigr可以修改/etc/group和/etc/gshadow，用这两个工具比较安全，在修改的时候会检查文件的一致性。</p><p>删除用户出错时，提示用户正在被进程占用。可以使用vi编辑/etc/paswd和/etc/shadow文件将该用户对应的行删除掉。也可以使用vipw和vipw -s来分别编辑/etc/paswd和/etc/shadow文件。它们的作用是一样的。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;用户和组管理命令&quot;&gt;&lt;a href=&quot;#用户和组管理命令&quot; class=&quot;headerlink&quot; title=&quot;用户和组管理命令&quot;&gt;&lt;/a&gt;用户和组管理命令&lt;/h1&gt;&lt;h2 id=&quot;useradd-和-adduser&quot;&gt;&lt;a href=&quot;#useradd-和-ad</summary>
      
    
    
    
    
    <category term="linux" scheme="https://slions.github.io/tags/linux/"/>
    
  </entry>
  
  <entry>
    <title>linux用户与组管理（上篇）</title>
    <link href="https://slions.github.io/2021/07/30/linux%E7%94%A8%E6%88%B7%E4%B8%8E%E7%BB%84%E7%AE%A1%E7%90%86%EF%BC%88%E4%B8%8A%E7%AF%87%EF%BC%89/"/>
    <id>https://slions.github.io/2021/07/30/linux%E7%94%A8%E6%88%B7%E4%B8%8E%E7%BB%84%E7%AE%A1%E7%90%86%EF%BC%88%E4%B8%8A%E7%AF%87%EF%BC%89/</id>
    <published>2021-07-30T04:34:26.000Z</published>
    <updated>2021-07-31T07:32:03.374Z</updated>
    
    <content type="html"><![CDATA[<h1 id="基本概念"><a href="#基本概念" class="headerlink" title="基本概念"></a>基本概念</h1><p>Linux 继承了UNIX 对用户的优秀支持，其属于多用户的操作系统。在linux中用户与组都是一种身份认证资源。</p><p>每个用户都有用户名、用户的唯一编号uid、所属组及其默认的shell，可能还有密码、家目录、附属组、注释信息等。每个组也有自己的名称、组唯一编号gid。一般来说，gid和uid都是相同的，但可以根据自己的实际需求来设置。组分为主组(primary group)和辅助组(secondary group)两种，用户一定会属于某个主组，也可以同时加入多个辅助组。</p><p>在Linux中，用户按权限来分类，可以分为3类：</p><ul><li><p>超级管理员</p><p>超级管理员是最高权限者，它的uid为0，默认超级管理员用户名为root。</p></li><li><p>系统用户</p><p>由系统或程序自行建立的账户被称为系统用户，特点是他们具有某些特权但又不需要登录操作系统。他们的uid范围从201到999，centos6的uid范围是1到499，出于安全考虑，它们一般不用来登录，所以它们的shell一般是/sbin/nologin，而且大多数时候它们是没有家目录的。</p></li><li><p>普通用户</p><p>普通用户是权限受到限制的用户，默认只能执行/bin、/usr/bin、/usr/local/bin和自身家目录下的命令。它们的uid从1000开始。尽管普通用户权限收到限制，但是它对自身家目录下的文件是有所有权限的。</p></li></ul><p>默认root用户的家目录为/root，其他用户的家目录一般在/home下以用户名命名的目录中。</p><h1 id="用户管理文件"><a href="#用户管理文件" class="headerlink" title="用户管理文件"></a>用户管理文件</h1><h2 id="用户文件"><a href="#用户文件" class="headerlink" title="用户文件"></a>用户文件</h2><p><code>/etc/passwd</code>文件里记录的是操作系统中用户的信息，这里面记录了几行就表示系统中有几个系统用户。它的格式大致如下：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">[root@slions_pc1 ~]# cat /etc/passwd</span><br><span class="line">root:x:0:0:root:/root:/bin/bash</span><br><span class="line">bin:x:1:1:bin:/bin:/sbin/nologin</span><br><span class="line">daemon:x:2:2:daemon:/sbin:/sbin/nologin</span><br><span class="line">adm:x:3:4:adm:/var/adm:/sbin/nologin</span><br><span class="line">lp:x:4:7:lp:/var/spool/lpd:/sbin/nologin</span><br><span class="line">sync:x:5:0:sync:/sbin:/bin/sync</span><br><span class="line">shutdown:x:6:0:shutdown:/sbin:/sbin/shutdown</span><br><span class="line">halt:x:7:0:halt:/sbin:/sbin/halt</span><br><span class="line">mail:x:8:12:mail:/var/spool/mail:/sbin/nologin</span><br><span class="line">operator:x:11:0:operator:/root:/sbin/nologin</span><br><span class="line">games:x:12:100:games:/usr/games:/sbin/nologin</span><br><span class="line">ftp:x:14:50:FTP User:/var/ftp:/sbin/nologin</span><br><span class="line">nobody:x:99:99:Nobody:/:/sbin/nologin</span><br><span class="line">systemd-network:x:192:192:systemd Network Management:/:/sbin/nologin</span><br><span class="line">dbus:x:81:81:System message bus:/:/sbin/nologin</span><br><span class="line">polkitd:x:999:998:User for polkitd:/:/sbin/nologin</span><br><span class="line">libstoragemgmt:x:998:997:daemon account for libstoragemgmt:/var/run/lsm:/sbin/nologin</span><br><span class="line">abrt:x:173:173::/etc/abrt:/sbin/nologin</span><br><span class="line">rpc:x:32:32:Rpcbind Daemon:/var/lib/rpcbind:/sbin/nologin</span><br><span class="line">sshd:x:74:74:Privilege-separated SSH:/var/empty/sshd:/sbin/nologin</span><br><span class="line">postfix:x:89:89::/var/spool/postfix:/sbin/nologin</span><br><span class="line">ntp:x:38:38::/etc/ntp:/sbin/nologin</span><br><span class="line">chrony:x:997:995::/var/lib/chrony:/sbin/nologin</span><br><span class="line">tcpdump:x:72:72::/:/sbin/nologin</span><br></pre></td></tr></table></figure><p>/etc/passwd 内容总共分为 7 个区域 ,以“ :” 作为区域的分隔符。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">用户名:x:uid:gid:用户注释信息:家目录:使用的shell类型</span><br></pre></td></tr></table></figure><p>第一列：用户名。区分大小写，账户名可以以字母 , 数字 , 英文句号 ‘.’, 下 划线 ‘_’, 连字符 ‘-‘ 等连和使用，账户名必须唯一</p><p>第二列：x。在以前老版本的系统上，第二列是存放用户密码的，但是密码和用户信息放在一起不便于管理(密钥要保证其特殊属性)，所以后来将密码单独放在另一个文件/etc/shadow中，这里就都写成x了。</p><p>第三列：uid。UID 号应该唯一，UID 号 0-999 为保留 UID</p><p>第四列：gid。</p><p>第五列：用户注释信息。</p><p>第六列：用户家目录，普通账户主目录默认建立在 /home 下。</p><p>第七列：用户的默认shell，虽然叫shell，但其实可以是任意一个可执行程序或脚本。</p><h2 id="密码文件"><a href="#密码文件" class="headerlink" title="密码文件"></a>密码文件</h2><p><code>/etc/shadow</code>文件管理着用户的密码 , 格式大致为：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">[root@slions_pc1 ~]# cat /etc/shadow</span><br><span class="line">root:$6$JifzPmKZFWt52T4M$2s.UGoczIsSGCFKiqw/KhfWsiLKgwAHqY3dq8jLsHg4/RGZ1NoSKhmgfeEgZiPLJVWJbafyoVLiVRptCi4KIs1::0:99999:7:::</span><br><span class="line">bin:*:17834:0:99999:7:::</span><br><span class="line">daemon:*:17834:0:99999:7:::</span><br><span class="line">adm:*:17834:0:99999:7:::</span><br><span class="line">lp:*:17834:0:99999:7:::</span><br><span class="line">sync:*:17834:0:99999:7:::</span><br><span class="line">shutdown:*:17834:0:99999:7:::</span><br><span class="line">halt:*:17834:0:99999:7:::</span><br><span class="line">mail:*:17834:0:99999:7:::</span><br><span class="line">operator:*:17834:0:99999:7:::</span><br><span class="line">games:*:17834:0:99999:7:::</span><br><span class="line">ftp:*:17834:0:99999:7:::</span><br><span class="line">nobody:*:17834:0:99999:7:::</span><br><span class="line">systemd-network:!!:18768::::::</span><br><span class="line">dbus:!!:18768::::::</span><br><span class="line">polkitd:!!:18768::::::</span><br><span class="line">libstoragemgmt:!!:18768::::::</span><br><span class="line">abrt:!!:18768::::::</span><br><span class="line">rpc:!!:18768:0:99999:7:::</span><br><span class="line">sshd:!!:18768::::::</span><br><span class="line">postfix:!!:18768::::::</span><br><span class="line">ntp:!!:18768::::::</span><br><span class="line">chrony:!!:18768::::::</span><br><span class="line">tcpdump:!!:18768::::::</span><br></pre></td></tr></table></figure><p>其有 9 个区域，每个区域的作用如下 :</p><p>第一列：用户名。( 与 /etc/passwd 一致 )<br>第二列：加密后的密码。但是这一列是有玄机的，有些特殊的字符表示特殊的意义。</p><ul><li><p>①.该列留空，即”::”，表示该用户没有密码。</p></li><li><p>②.该列为”!”，即”:!:”，表示该用户被锁，被锁将无法登陆，但是可能其他的登录方式是不受限制的，如ssh key的方式，su的方式。</p></li><li><p>③.该列为”<em>”，即”:</em>:”，也表示该用户被锁，和”!”效果是一样的。</p></li><li><p>④.该列以”!”或”!!”开头，则也表示该用户被锁。</p></li><li><p>⑤.该列为”!!”，即”:!!:”，表示该用户从来没设置过密码。</p></li><li><p>⑥.如果格式为”$id$salt$hashed”，则表示该用户密码正常。其中$id$的id表示密码的加密算法，$1$表示使用MD5算法，$2a$表示使用Blowfish算法，”$2y$”是另一算法长度的Blowfish,”$5$”表示SHA-256算法，而”$6$”表示SHA-512算法，可见上面的结果中都是使用sha-512算法的。$5$和$6$这两种算法的破解难度远高于MD5。$salt$是加密时使用的salt，$hashed才是真正的密码部分。</p></li></ul><p>第三列：密码自新纪元 (1970-1-1) 起到用户前一次修 改密码的天数。<br>第四列：密码最少使用期限(天数)。密码前次与下一次修改的时间间隔 , 一般为“０”位不设定，可随时修改。<br>第五列：密码最大使用期限(天数)。超过了它不一定密码就失效，可能下一个字段设置了过期后的宽限天数。设置为空时将永不过期，后面设置的提醒和警告将失效。root等一些用户的已经默认设置为了99999，表示永不过期。如果值设置小于最短使用期限，用户将不能修改密码。<br>第六列：密码过期前多少天就开始提醒用户密码将要过期。空或0将不提醒。<br>第七列：密码过期后宽限的天数，在宽限时间内用户无法使用原密码登录，必须改密码或者联系管理员。设置为空表示没有强制的宽限时间，可以过期后的任意时间内修改密码。<br>第八列：帐号过期时间。从1970年1月1日开始计算天数。设置为空帐号将永不过期，不能设置为0。不同于密码过期，密码过期后账户还有效，改密码后还能登录；帐号过期后帐号失效，修改密码重设密码都无法使用该帐号。<br>第九列：保留字段。</p><h2 id="组文件"><a href="#组文件" class="headerlink" title="组文件"></a>组文件</h2><p><code>/etc/group</code>包含了组信息。格式大致为：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line">[root@slions_pc1 ~]# cat /etc/group</span><br><span class="line">root:x:0:</span><br><span class="line">bin:x:1:</span><br><span class="line">daemon:x:2:</span><br><span class="line">sys:x:3:</span><br><span class="line">adm:x:4:</span><br><span class="line">tty:x:5:</span><br><span class="line">disk:x:6:</span><br><span class="line">lp:x:7:</span><br><span class="line">mem:x:8:</span><br><span class="line">kmem:x:9:</span><br><span class="line">wheel:x:10:</span><br><span class="line">cdrom:x:11:</span><br><span class="line">mail:x:12:postfix</span><br><span class="line">man:x:15:</span><br><span class="line">dialout:x:18:</span><br><span class="line">floppy:x:19:</span><br><span class="line">games:x:20:</span><br><span class="line">tape:x:33:</span><br><span class="line">video:x:39:</span><br><span class="line">ftp:x:50:</span><br><span class="line">lock:x:54:</span><br><span class="line">audio:x:63:</span><br><span class="line">nobody:x:99:</span><br><span class="line">users:x:100:</span><br><span class="line">utmp:x:22:</span><br><span class="line">utempter:x:35:</span><br><span class="line">input:x:999:</span><br><span class="line">systemd-journal:x:190:</span><br><span class="line">systemd-network:x:192:</span><br><span class="line">dbus:x:81:</span><br><span class="line">polkitd:x:998:</span><br><span class="line">libstoragemgmt:x:997:</span><br><span class="line">ssh_keys:x:996:</span><br><span class="line">abrt:x:173:</span><br><span class="line">rpc:x:32:</span><br><span class="line">sshd:x:74:</span><br><span class="line">slocate:x:21:</span><br><span class="line">postdrop:x:90:</span><br><span class="line">postfix:x:89:</span><br><span class="line">ntp:x:38:</span><br><span class="line">chrony:x:995:</span><br><span class="line">tcpdump:x:72:</span><br><span class="line">stapusr:x:156:</span><br><span class="line">stapsys:x:157:</span><br><span class="line">stapdev:x:158:</span><br><span class="line">cgred:x:994:</span><br><span class="line">docker:x:993:</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>每行一个组，每一行3个冒号共4列属性：</p><p>第一列：组名。<br>第二列：占位符。<br>第三列：gid。<br>第四列：该组下的user列表，这些user成员以该组做为辅助组，多个成员使用逗号隔开。</p><h2 id="框架目录"><a href="#框架目录" class="headerlink" title="框架目录"></a>框架目录</h2><p><code>/etc/skel</code>框架目录中的文件是每次新建用户时，都会复制到新用户家目录里的文件。默认只有3个环境配置文件，可以修改这里面的内容，或者添加几个文件在骨架目录中，以后新建用户时就会自动获取到这些环境和文件。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[root@slions_pc2 ~]# ls -lA /etc/skel/</span><br><span class="line">总用量 12</span><br><span class="line">-rw-r--r--. 1 root root  18 10月 31 2018 .bash_logout</span><br><span class="line">-rw-r--r--. 1 root root 193 10月 31 2018 .bash_profile</span><br><span class="line">-rw-r--r--. 1 root root 231 10月 31 2018 .bashrc</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>删除家目录下这些文件，会导致某些设置出现问题。例如删除”.bashrc”这个文件，会导致提示符变异的问题(-bash-4.2$)。</p><h2 id="创建用户限制文件"><a href="#创建用户限制文件" class="headerlink" title="创建用户限制文件"></a>创建用户限制文件</h2><p><code>/etc/login.defs</code>设置用户帐号限制的文件。该文件里的配置对root用户无效。</p><p>如果/etc/shadow文件里有相同的选项，则以/etc/shadow里的设置为准，也就是说/etc/shadow的配置优先级高于/etc/login.defs。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br></pre></td><td class="code"><pre><span class="line">[root@slions_pc2 ~]# cat /etc/login.defs</span><br><span class="line"><span class="meta">#</span><span class="bash"></span></span><br><span class="line"><span class="bash"><span class="comment"># Please note that the parameters in this configuration file control the</span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> behavior of the tools from the shadow-utils component. None of these</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> tools uses the PAM mechanism, and the utilities that use PAM (such as the</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> passwd <span class="built_in">command</span>) should therefore be configured elsewhere. Refer to</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> /etc/pam.d/system-auth <span class="keyword">for</span> more information.</span></span><br><span class="line"><span class="meta">#</span><span class="bash"></span></span><br><span class="line"><span class="bash"></span><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> *REQUIRED*</span></span><br><span class="line"><span class="meta">#</span><span class="bash">   Directory <span class="built_in">where</span> mailboxes reside, _or_ name of file, relative to the</span></span><br><span class="line"><span class="meta">#</span><span class="bash">   home directory.  If you _do_ define both, MAIL_DIR takes precedence.</span></span><br><span class="line"><span class="meta">#</span><span class="bash">   QMAIL_DIR is <span class="keyword">for</span> Qmail</span></span><br><span class="line"><span class="meta">#</span><span class="bash"></span></span><br><span class="line"><span class="bash"><span class="comment">#QMAIL_DIR      Maildir           # QMAIL_DIR是Qmail邮件的目录，所以可以不设置它</span></span></span><br><span class="line">MAIL_DIR        /var/spool/mail   # 默认邮件根目录，即信箱</span><br><span class="line"><span class="meta">#</span><span class="bash">MAIL_FILE      .mail             <span class="comment"># mail文件的格式是.mail</span></span></span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> Password aging controls:</span></span><br><span class="line"><span class="meta">#</span><span class="bash"></span></span><br><span class="line"><span class="bash"><span class="comment">#       PASS_MAX_DAYS   Maximum number of days a password may be used.</span></span></span><br><span class="line"><span class="meta">#</span><span class="bash">       PASS_MIN_DAYS   Minimum number of days allowed between password changes.</span></span><br><span class="line"><span class="meta">#</span><span class="bash">       PASS_MIN_LEN    Minimum acceptable password length.</span></span><br><span class="line"><span class="meta">#</span><span class="bash">       PASS_WARN_AGE   Number of days warning given before a password expires.</span></span><br><span class="line"><span class="meta">#</span><span class="bash"></span></span><br><span class="line"><span class="bash"><span class="comment"># Password aging controls:</span></span></span><br><span class="line">PASS_MAX_DAYS   99999         # 密码最大有效期(天)</span><br><span class="line">PASS_MIN_DAYS   0             # 两次密码修改之间最小时间间隔</span><br><span class="line">PASS_MIN_LEN    5             # 密码最短长度</span><br><span class="line">PASS_WARN_AGE   7             # 密码过期前给警告信息的时间</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 控制useradd创建用户时自动选择的uid范围</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> Min/max values <span class="keyword">for</span> automatic uid selection <span class="keyword">in</span> useradd</span></span><br><span class="line"><span class="meta">#</span><span class="bash"></span></span><br><span class="line"><span class="bash">UID_MIN                  1000</span></span><br><span class="line">UID_MAX                 60000</span><br><span class="line"><span class="meta">#</span><span class="bash"> System accounts</span></span><br><span class="line">SYS_UID_MIN               201</span><br><span class="line">SYS_UID_MAX               999</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 控制groupadd创建组时自动选择的gid范围</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> Min/max values <span class="keyword">for</span> automatic gid selection <span class="keyword">in</span> groupadd</span></span><br><span class="line"><span class="meta">#</span><span class="bash"></span></span><br><span class="line"><span class="bash">GID_MIN                  1000</span></span><br><span class="line">GID_MAX                 60000</span><br><span class="line"><span class="meta">#</span><span class="bash"> System accounts</span></span><br><span class="line">SYS_GID_MIN               201</span><br><span class="line">SYS_GID_MAX               999</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"></span></span><br><span class="line"><span class="bash"><span class="comment"># If defined, this command is run when removing a user.</span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> It should remove any at/cron/<span class="built_in">print</span> <span class="built_in">jobs</span> etc. owned by</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> the user to be removed (passed as the first argument).</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 设置此项后，在删除用户时，将自动删除用户拥有的at/cron/<span class="built_in">print</span>等job</span></span><br><span class="line"><span class="meta">#</span><span class="bash">USERDEL_CMD    /usr/sbin/userdel_local</span></span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"></span></span><br><span class="line"><span class="bash"><span class="comment"># If useradd should create home directories for users by default</span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> On RH systems, we <span class="keyword">do</span>. This option is overridden with the -m flag on</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> useradd <span class="built_in">command</span> line.</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 控制useradd添加用户时是否默认创建家目录，useradd -m选项会覆盖此处设置</span></span><br><span class="line">CREATE_HOME     yes</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> The permission mask is initialized to this value. If not specified,</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> the permission mask will be initialized to 022.</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 设置创建家目录时的<span class="built_in">umask</span>值，若不指定则默认为022</span></span><br><span class="line">UMASK           077</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> This enables userdel to remove user groups <span class="keyword">if</span> no members exist.</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 设置此项表示当组中没有成员时自动删除该组</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 且useradd是否同时创建同用户名的主组。</span></span><br><span class="line">USERGROUPS_ENAB yes</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> Use SHA512 to encrypt password.</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 设置用户和组密码的加密算法</span></span><br><span class="line">ENCRYPT_METHOD SHA512</span><br></pre></td></tr></table></figure><p>/etc/login.defs中的设置控制的是shadow-utils包中的组件，也就是说，该组件中的工具执行操作时会读取该文件中的配置。该组件中包含下面的程序：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">/usr/bin/gpasswd      ：administer /etc/group and /etc/gshadow</span><br><span class="line">/usr/bin/newgrp       ：log in to a new group，可用来修改gid，哪怕是正在登陆的会话也可以修改</span><br><span class="line">/usr/bin/sg           ：execute command as different group ID</span><br><span class="line">/usr/sbin/groupadd    ：添加组</span><br><span class="line">/usr/sbin/groupdel    ：删除组</span><br><span class="line">/usr/sbin/groupmems   ：管理当前用户的主组中的成员，root用户则可以指定要管理的组</span><br><span class="line">/usr/sbin/groupmod    ：modify a group definition on the system</span><br><span class="line">/usr/sbin/grpck       ：verify integrity of group files</span><br><span class="line">/usr/sbin/grpconv     ：无视它</span><br><span class="line">/usr/sbin/grpunconv   ：无视它</span><br><span class="line">/usr/sbin/pwconv      ：无视它</span><br><span class="line">/usr/sbin/pwunconv    ：无视它</span><br><span class="line">/usr/sbin/adduser     ：是useradd的一个软链接，添加用户</span><br><span class="line">/usr/sbin/chpasswd    ：update passwords in batch mode</span><br><span class="line">/usr/sbin/newusers    ：update and create new users in batch</span><br><span class="line">/usr/sbin/pwck        ：verify integrity of passsword files</span><br><span class="line">/usr/sbin/useradd     ：添加用户</span><br><span class="line">/usr/sbin/userdel     ：删除用户</span><br><span class="line">/usr/sbin/usermod     ：重定义用户信息</span><br><span class="line">/usr/sbin/vigr        ：edit the group and shadow-group file</span><br><span class="line">/usr/sbin/vipw        ：edit the password and shadow-password file</span><br><span class="line">/usr/bin/lastlog      ：输出所有用户或给定用户最近登录信息</span><br><span class="line"></span><br></pre></td></tr></table></figure><h2 id="创建用户时默认配置文件"><a href="#创建用户时默认配置文件" class="headerlink" title="创建用户时默认配置文件"></a>创建用户时默认配置文件</h2><p><code>/etc/default/useradd</code>。useradd -D修改的就是此文件。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">[root@slions_pc1 ~]# cat /etc/default/useradd</span><br><span class="line">[root@xuexi ~]# cat /etc/default/useradd  </span><br><span class="line"><span class="meta">#</span><span class="bash"> useradd defaults file</span></span><br><span class="line">GROUP=100       # 在useradd使用-N或/etc/login.defs中USERGROUPS_ENAB=no时表示创建</span><br><span class="line">                # 用户时不创建同用户名的主组(primary group)，此时新建的用户将默认以</span><br><span class="line">                # 此组为主组，网上关于该设置的很多说明都是错的，具体可看man useradd</span><br><span class="line">                # 的-g选项或useradd -D的-g选项</span><br><span class="line">HOME=/home      # 把用户的家目录建在/home中</span><br><span class="line">INACTIVE=-1     # 是否启用帐号过期设置(是帐号过期不是密码过期)，-1表示不启用</span><br><span class="line">EXPIRE=         # 帐号过期时间，不设置表示不启用</span><br><span class="line">SHELL=/bin/bash # 新建用户默认的shell类型</span><br><span class="line">SKEL=/etc/skel  # 指定框架目录，前文的/etc/skel就在这里</span><br><span class="line">CREATE_MAIL_SPOOL=yes  # 是否创建用户mail缓冲</span><br><span class="line"></span><br></pre></td></tr></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;基本概念&quot;&gt;&lt;a href=&quot;#基本概念&quot; class=&quot;headerlink&quot; title=&quot;基本概念&quot;&gt;&lt;/a&gt;基本概念&lt;/h1&gt;&lt;p&gt;Linux 继承了UNIX 对用户的优秀支持，其属于多用户的操作系统。在linux中用户与组都是一种身份认证资源。&lt;/p&gt;
</summary>
      
    
    
    
    
    <category term="linux" scheme="https://slions.github.io/tags/linux/"/>
    
  </entry>
  
  <entry>
    <title>harbor集群系统负载升高原因分析</title>
    <link href="https://slions.github.io/2021/07/19/harbor%E9%9B%86%E7%BE%A4%E7%B3%BB%E7%BB%9F%E8%B4%9F%E8%BD%BD%E5%8D%87%E9%AB%98%E5%8E%9F%E5%9B%A0%E5%88%86%E6%9E%90/"/>
    <id>https://slions.github.io/2021/07/19/harbor%E9%9B%86%E7%BE%A4%E7%B3%BB%E7%BB%9F%E8%B4%9F%E8%BD%BD%E5%8D%87%E9%AB%98%E5%8E%9F%E5%9B%A0%E5%88%86%E6%9E%90/</id>
    <published>2021-07-19T10:07:15.000Z</published>
    <updated>2021-07-19T11:00:44.903Z</updated>
    
    <content type="html"><![CDATA[<h1 id="问题现象"><a href="#问题现象" class="headerlink" title="问题现象"></a>问题现象</h1><p>接到客户反馈，说是创建不了新的服务了，查看相关的Event日志，发现此服务拉取不了images，后台查看harbor节点的状态，发现问题，harbor集群的VIP丢失，并且在主机上执行命令都比较卡，top查看平均负载发现非常高。</p><p><img src="/doc_picture/harbor-1.png" alt="image-20210719181131471"></p><h1 id="问题分析"><a href="#问题分析" class="headerlink" title="问题分析"></a>问题分析</h1><p>首先我们需要排查什么原因导致harbor节点这个卡顿，负载一直升高。</p><blockquote><p>平均负载是指单位时间内，系统处于可运行状态和不可中断状态的平均进程数，也就是平均活跃进程数。所以，它不仅包括了正在使用CPU的进程，还包括等待CPU和等待I/O的进程。</p></blockquote><p>系统平均负载升高的原因主要有三种：</p><ul><li>CPU密集型进程，使用大量CPU会导致平均负载升高，此时CPU使用率跟平均负载是一致的；</li><li>I/O密集型进程，等待I/O也会导致平均负载升高，但CPU使用率不一定很高；</li><li>大量等待CPU的进程调度也会导致平均负载升高，此时CPU使用率也会比较高。</li></ul><p>首先sar -u 观察CPU情况。</p><p><img src="/doc_picture/harbor-2.png" alt="image-20210719182405728"></p><p>cpu使用率非常低，大部分为idle，说明没有进程在等待cpu资源。</p><p>sar -b 观察IO情况 IO设备的读写tps都几乎为0。</p><p><img src="/doc_picture/harbor-3.png" alt="image-20210719182429006"></p><p>发现并不存在CPU/IO密集型的进程后，执行了下df操作，发现命令hang死，另外开一个终端，通过strace去分析df命令的系统调用及信号情况，可以明显发现df是在系统调用尝试获取目录/harborimages的stat信息时挂起。</p><p><img src="/doc_picture/harbor-4.png" alt="image-20210719182846933"></p><p>通过ps aux抓取系统运行的df进程信息（状态为D+(无法中断的休眠状态)）：</p><p><img src="/doc_picture/harbor-5.png" alt="image-20210719182903747"></p><p>通过ps aux查看内存和cpu占用最多的5个进程发现了问题，没有cpu占用特别大的进程，但是有一个状态为Dl的进程（无法中断的休眠状态/多线程，克隆线程）</p><p><img src="/doc_picture/harbor-6.png" alt="image-20210719182922163"></p><p>我们再查看下此进程的线程状态：</p><p><img src="/doc_picture/harbor-7.png" alt="image-20210719182944057"></p><p>发现有一堆不可中断的线程，而且越来越多，cpu使用率都为0，此时可以定位问题了，有大量进程读写请求一直获取不到资源，从而进程一直是不可中断状态。造成负载很高。平均负载升高导致机器性能降低，内部的keepalived服务心跳机制超时，最后使得VIP丢失。</p><p><img src="/doc_picture/harbor-8.png" alt="image-20210719184236890"></p><p>registry是harbor中负责存储镜像文件的组件，同时负责处理镜像的pull/push命令。此服务会将宿主机的/harborimages作为挂载目录，为了保障可用性我们后端使用了客户现成的nas来挂载到了/harborimages目录，之前执行df hang住的地方正是这个目录。 又在本地创了个测试目录，使用之前的nas地址挂载发现异常。此时回想起来，前一天客户发通知说要进行xxx区机器网络进行调整，大概率是由于这个引起了。联系客户方运维协助排查，最后发现nas服务器因为重启过导致防火墙开启了。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;问题现象&quot;&gt;&lt;a href=&quot;#问题现象&quot; class=&quot;headerlink&quot; title=&quot;问题现象&quot;&gt;&lt;/a&gt;问题现象&lt;/h1&gt;&lt;p&gt;接到客户反馈，说是创建不了新的服务了，查看相关的Event日志，发现此服务拉取不了images，后台查看harbor节点的状</summary>
      
    
    
    
    
    <category term="harbor" scheme="https://slions.github.io/tags/harbor/"/>
    
  </entry>
  
  <entry>
    <title>glusterfs可用性测试</title>
    <link href="https://slions.github.io/2021/07/18/glusterfs%E5%8F%AF%E7%94%A8%E6%80%A7%E6%B5%8B%E8%AF%95/"/>
    <id>https://slions.github.io/2021/07/18/glusterfs%E5%8F%AF%E7%94%A8%E6%80%A7%E6%B5%8B%E8%AF%95/</id>
    <published>2021-07-18T10:58:16.000Z</published>
    <updated>2021-07-18T11:44:07.694Z</updated>
    
    <content type="html"><![CDATA[<h1 id="环境描述"><a href="#环境描述" class="headerlink" title="环境描述"></a>环境描述</h1><table><thead><tr><th><strong>主机名</strong></th><th>ip地址</th><th><strong>系统环境</strong></th><th><strong>角色</strong></th></tr></thead><tbody><tr><td>k8s-1</td><td>IP:192.168.186.10</td><td>cpu:x2  mem:2GB  disk:/dev/sdb 40GB</td><td>K8s_master,Gluster_master,Heketi_master</td></tr><tr><td>K8s-2</td><td>IP:192.168.186.11</td><td>cpu:x2  mem:2GB  disk:/dev/sdb 40GB</td><td>K8s_node,Gluster_node</td></tr><tr><td>K8s-3</td><td>IP:192.168.186.12</td><td>cpu:x2  mem:2GB  disk:/dev/sdb 40GB</td><td>K8s_master,Gluster_node</td></tr></tbody></table><h1 id="集群扩容"><a href="#集群扩容" class="headerlink" title="集群扩容"></a>集群扩容</h1><h2 id="添加新磁盘"><a href="#添加新磁盘" class="headerlink" title="添加新磁盘"></a>添加新磁盘</h2><blockquote><p>添加设备时，请记住将设备添加为一组。例如，如果创建的卷使用副本为2，则应将device添加到两个节点（每个节点一个device）。如果使用副本3，则将device添加到三个节点。</p></blockquote><h3 id="命令行方式"><a href="#命令行方式" class="headerlink" title="命令行方式"></a>命令行方式</h3><p>假设在k8s-3上增加磁盘，查看k8s-3部署的pod name及IP：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-1 ~]# kubectl get po -o wide -l glusterfs-node</span><br><span class="line">NAME             READY      STATUS    RESTARTS   AGE           IP            NODE</span><br><span class="line">glusterfs-5npwn   1/1       Running   0          20h       192.168.186.10   k8s-1</span><br><span class="line">glusterfs-8zfzq   1/1       Running   0          20h       192.168.186.11   k8s-2</span><br><span class="line">glusterfs-bd5dx   1/1       Running   0          20h       192.168.186.12   k8s-3</span><br></pre></td></tr></table></figure><p>在k8s-3上确认新添加的盘符：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">Disk /dev/sdc: 42.9 GB, 42949672960 bytes, 83886080 sectors</span><br><span class="line">Units = sectors of 1 * 512 = 512 bytes</span><br><span class="line">Sector size (logical/physical): 512 bytes / 512 bytes</span><br><span class="line">I/O size (minimum/optimal): 512 bytes / 512 bytes</span><br></pre></td></tr></table></figure><p>使用heketi-cli查看cluster ID和所有node ID：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-1 ~]# heketi-cli cluster list</span><br><span class="line">Clusters:</span><br><span class="line">Id:5dec5676c731498c2bdf996e110a3e5e [file][block]</span><br><span class="line">[root@k8s-1 ~]# heketi-cli cluster info 5dec5676c731498c2bdf996e110a3e5e</span><br><span class="line">Cluster id: 5dec5676c731498c2bdf996e110a3e5e</span><br><span class="line">Nodes:</span><br><span class="line">0f00835397868d3591f45432e432ba38</span><br><span class="line">d38819746cab7d567ba5f5f4fea45d91</span><br><span class="line">fb181b0cef571e9af7d84d2ecf534585</span><br><span class="line">Volumes:</span><br><span class="line">32146a51be9f980c14bc86c34f67ebd5</span><br><span class="line">56d636b452d31a9d4cb523d752ad0891</span><br><span class="line">828dc2dfaa00b7213e831b91c6213ae4</span><br><span class="line">b9c68075c6f20438b46db892d15ed45a</span><br><span class="line">Block: true</span><br><span class="line">File: true</span><br></pre></td></tr></table></figure><p>找到对应的k8s-3的node ID：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-1 ~]# heketi-cli node info 0f00835397868d3591f45432e432ba38</span><br><span class="line">Node Id: 0f00835397868d3591f45432e432ba38</span><br><span class="line">State: online</span><br><span class="line">Cluster Id: 5dec5676c731498c2bdf996e110a3e5e</span><br><span class="line">Zone: 1</span><br><span class="line">Management Hostname: k8s-node02</span><br><span class="line">Storage Hostname: 192.168.186.12</span><br><span class="line">Devices:</span><br><span class="line">Id:82af8e5f2fb2e1396f7c9e9f7698a178   Name:/dev/sdb            State:online    Size (GiB):39      Used (GiB):25      Free (GiB):14      Bricks:4</span><br></pre></td></tr></table></figure><p>添加磁盘至GFS集群的k8s-3：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-1 ~]# heketi-cli device add --name=/dev/sdc --node=0f00835397868d3591f45432e432ba38</span><br><span class="line">Device added successfully</span><br></pre></td></tr></table></figure><p>查看结果：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-1 ~]# heketi-cli node info 0f00835397868d3591f45432e432ba38</span><br><span class="line">Node Id: 0f00835397868d3591f45432e432ba38</span><br><span class="line">State: online</span><br><span class="line">Cluster Id: 5dec5676c731498c2bdf996e110a3e5e</span><br><span class="line">Zone: 1</span><br><span class="line">Management Hostname: k8s-3</span><br><span class="line">Storage Hostname: 192.168.186.12</span><br><span class="line">Devices:</span><br><span class="line">Id:5539e74bc2955e7c70b3a20e72c04615   Name:/dev/sdc            State:online    Size (GiB):39      Used (GiB):0       Free (GiB):39      Bricks:0       </span><br><span class="line">Id:82af8e5f2fb2e1396f7c9e9f7698a178   Name:/dev/sdb            State:online    Size (GiB):39      Used (GiB):25      Free (GiB):14      Bricks:4</span><br></pre></td></tr></table></figure><h3 id="拓扑文件方式"><a href="#拓扑文件方式" class="headerlink" title="拓扑文件方式"></a>拓扑文件方式</h3><p>当一次添加多个设备的一种更简单的方法是将新设备添加到用于设置群集的拓扑文件(topology.json)中的节点描述中。然后重新运行该命令以加载新拓扑。下面是我们向节点添加新的/dev/sdc磁盘的示例：</p><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br></pre></td><td class="code"><pre><span class="line">cat &gt;topology.json&lt;&lt;EOF</span><br><span class="line">&#123;</span><br><span class="line">  <span class="attr">&quot;clusters&quot;</span>: [</span><br><span class="line">    &#123;</span><br><span class="line">      <span class="attr">&quot;nodes&quot;</span>: [</span><br><span class="line">        &#123;</span><br><span class="line">          <span class="attr">&quot;node&quot;</span>: &#123;</span><br><span class="line">            <span class="attr">&quot;hostnames&quot;</span>: &#123;</span><br><span class="line">              <span class="attr">&quot;manage&quot;</span>: [</span><br><span class="line">                <span class="string">&quot;k8s-1&quot;</span></span><br><span class="line">              ],</span><br><span class="line">              <span class="attr">&quot;storage&quot;</span>: [</span><br><span class="line">                <span class="string">&quot;192.168.186.10&quot;</span></span><br><span class="line">              ]</span><br><span class="line">            &#125;,</span><br><span class="line">            <span class="attr">&quot;zone&quot;</span>: <span class="number">1</span></span><br><span class="line">          &#125;,</span><br><span class="line">          <span class="attr">&quot;devices&quot;</span>: [</span><br><span class="line">            <span class="string">&quot;/dev/sdb&quot;</span>,</span><br><span class="line">            <span class="string">&quot;/dev/sdc&quot;</span></span><br><span class="line">          ]</span><br><span class="line">        &#125;,</span><br><span class="line">        &#123;</span><br><span class="line">          <span class="attr">&quot;node&quot;</span>: &#123;</span><br><span class="line">            <span class="attr">&quot;hostnames&quot;</span>: &#123;</span><br><span class="line">              <span class="attr">&quot;manage&quot;</span>: [</span><br><span class="line">                <span class="string">&quot;k8s-2&quot;</span></span><br><span class="line">              ],</span><br><span class="line">              <span class="attr">&quot;storage&quot;</span>: [</span><br><span class="line">                <span class="string">&quot;192.168.186.11&quot;</span></span><br><span class="line">              ]</span><br><span class="line">            &#125;,</span><br><span class="line">            <span class="attr">&quot;zone&quot;</span>: <span class="number">1</span></span><br><span class="line">          &#125;,</span><br><span class="line">          <span class="attr">&quot;devices&quot;</span>: [</span><br><span class="line">            <span class="string">&quot;/dev/sdb&quot;</span>,</span><br><span class="line">            <span class="string">&quot;/dev/sdc&quot;</span></span><br><span class="line">          ]</span><br><span class="line">        &#125;,</span><br><span class="line">        &#123;</span><br><span class="line">          <span class="attr">&quot;node&quot;</span>: &#123;</span><br><span class="line">            <span class="attr">&quot;hostnames&quot;</span>: &#123;</span><br><span class="line">              <span class="attr">&quot;manage&quot;</span>: [</span><br><span class="line">                <span class="string">&quot;k8s-3&quot;</span></span><br><span class="line">              ],</span><br><span class="line">              <span class="attr">&quot;storage&quot;</span>: [</span><br><span class="line">                <span class="string">&quot;192.168.186.12&quot;</span></span><br><span class="line">              ]</span><br><span class="line">            &#125;,</span><br><span class="line">            <span class="attr">&quot;zone&quot;</span>: <span class="number">1</span></span><br><span class="line">          &#125;,</span><br><span class="line">          <span class="attr">&quot;devices&quot;</span>: [</span><br><span class="line">            <span class="string">&quot;/dev/sdb&quot;</span>,</span><br><span class="line">            <span class="string">&quot;/dev/sdc&quot;</span></span><br><span class="line">          ]                                                                                    </span><br><span class="line">        &#125;</span><br><span class="line">      ]</span><br><span class="line">    &#125;</span><br><span class="line">  ]</span><br><span class="line">&#125;</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure><p>heketi加载拓扑配置：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> heketi-cli topology load --json=topology.json</span></span><br><span class="line">   Creating cluster ... ID: 224a5a6555fa5c0c930691111c63e863</span><br><span class="line">     Allowing file volumes on cluster.</span><br><span class="line">     Allowing block volumes on cluster.</span><br><span class="line">     Creating node 192.168.186.10 ... ID: 7946b917b91a579c619ba51d9129aeb0</span><br><span class="line">Found device /dev/sdb</span><br><span class="line">Adding device /dev/sdc ... OK</span><br><span class="line">     Creating node 192.168.186.11 ... ID: 5d10e593e89c7c61f8712964387f959c</span><br><span class="line">Found device /dev/sdb</span><br><span class="line">            Adding device /dev/sdc ... OK</span><br><span class="line">     Creating node 192.168.186.12 ... ID: de620cb2c313a5461d5e0a6ae234c553</span><br><span class="line">Found device /dev/sdb</span><br><span class="line">            Adding device /dev/sdc ... OK</span><br></pre></td></tr></table></figure><h2 id="添加新节点"><a href="#添加新节点" class="headerlink" title="添加新节点"></a>添加新节点</h2><p>假设将k8s-4，IP为192.168.186.13的加入glusterfs集群，并将该节点的/dev/sdb,/dev/sdc加入到集群。</p><p>先给node加标签，之后会自动创建pod：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-1 kubernetes]# kubectl label node k8s-4 storagenode=glusterfs</span><br><span class="line">node/k8s-4 labeled</span><br><span class="line"></span><br><span class="line">[root@k8s-1 kubernetes]# kubectl  get pod -o wide -l glusterfs-node</span><br><span class="line">NAME        READY     STATUS        RESTARTS   AGE       IP          NODE</span><br><span class="line">glusterfs-5npwn   1/1     Running     0     21h       192.168.186.11        k8s-2</span><br><span class="line">glusterfs-8zfzq   1/1       Running      0    21h      192.168.186.10        k8s-1</span><br><span class="line">glusterfs-96w74   0/1  ContainerCreating   0   2m     192.168.186.13         k8s-4</span><br><span class="line">glusterfs-bd5dx   1/1     Running       0      21h       192.168.186.12     k8s-3</span><br></pre></td></tr></table></figure><p>进入任意节点的gfs服务容器执行peer probe，加入新节点：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-1 kubernetes]# kubectl exec -ti glusterfs-5npwn -- gluster peer probe 192.168.186.13</span><br><span class="line">peer probe: success.</span><br></pre></td></tr></table></figure><p>将新节点纳入heketi数据库统一管理：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-1 kubernetes]# heketi-cli cluster list</span><br><span class="line">Clusters:</span><br><span class="line">Id:5dec5676c731498c2bdf996e110a3e5e [file][block]</span><br><span class="line"></span><br><span class="line">[root@k8s-1 kubernetes]# heketi-cli node add --zone=1 --cluster=5dec5676c731498c2bdf996e110a3e5e --management-host-name=k8s-4 --storage-host-name=192.168.186.13</span><br><span class="line">Node information:</span><br><span class="line">Id: 150bc8c458a70310c6137e840619758c</span><br><span class="line">State: online</span><br><span class="line">Cluster Id: 5dec5676c731498c2bdf996e110a3e5e</span><br><span class="line">Zone: 1</span><br><span class="line">Management Hostname k8s-4</span><br><span class="line">Storage Hostname 192.168.186.13</span><br></pre></td></tr></table></figure><p>将新节点的磁盘加入到集群中，参考上面的两种方式之一即可。</p><h1 id="存储卷扩容"><a href="#存储卷扩容" class="headerlink" title="存储卷扩容"></a>存储卷扩容</h1><h2 id="扩容"><a href="#扩容" class="headerlink" title="扩容"></a>扩容</h2><p>扩容volume可使用命令（单位为G）：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> heketi-cli volume expand --volume=volumeID --expand-size=10</span></span><br></pre></td></tr></table></figure><h1 id="集群缩容"><a href="#集群缩容" class="headerlink" title="集群缩容"></a>集群缩容</h1><p>Heketi也支持降低存储容量。这可以通过删除device，节点和集群来实现。可以使用API或使用heketi-cli执行这些更改。</p><blockquote><p> heketi删除device的前提是device没有被使用（Used为0）</p></blockquote><p>以下是如何从Heketi删除没有device被使用的命令：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> heketi-cli topology info</span></span><br><span class="line">Cluster Id: 6fe4dcffb9e077007db17f737ed999fe </span><br><span class="line">Volumes:</span><br><span class="line"></span><br><span class="line">    Nodes:</span><br><span class="line">  </span><br><span class="line">        Node Id: 61d019bb0f717e04ecddfefa5555bc41</span><br><span class="line">        State: online</span><br><span class="line">        Cluster Id: 6fe4dcffb9e077007db17f737ed999fe</span><br><span class="line">        Zone: 1</span><br><span class="line">        Management Hostname: k8s-3</span><br><span class="line">        Storage Hostname: 192.168.186.12</span><br><span class="line">        Devices:</span><br><span class="line">                Id:e4805400ffa45d6da503da19b26baad6   Name:/dev/sdb            State:online    Size (GiB):40     Used (GiB):0       Free (GiB):40</span><br><span class="line">                        Bricks:</span><br><span class="line">                Id:ecc3c65e4d22abf3980deba4ae90238c   Name:/dev/sdc            State:online    Size (GiB):40     Used (GiB):0       Free (GiB):40</span><br><span class="line">                        Bricks:</span><br><span class="line">  </span><br><span class="line">        Node Id: e97d77d0191c26089376c78202ee2f20</span><br><span class="line">        State: online</span><br><span class="line">        Cluster Id: 6fe4dcffb9e077007db17f737ed999fe</span><br><span class="line">        Zone: 2</span><br><span class="line">        Management Hostname: k8s-4</span><br><span class="line">        Storage Hostname: 192.168.186.13</span><br><span class="line">        Devices:</span><br><span class="line">                Id:3dc3b3f0dfd749e8dc4ee98ed2cc4141   Name:/dev/sdb            State:online    Size (GiB):40     Used (GiB):0       Free (GiB):40</span><br><span class="line">                        Bricks:</span><br><span class="line">                Id:4122bdbbe28017944a44e42b06755b1c   Name:/dev/sdc            State:online    Size (GiB):40     Used (GiB):0       Free (GiB)40</span><br><span class="line">                        Bricks:</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> d=`heketi-cli topology info | grep Size | awk <span class="string">&#x27;&#123;print $1&#125;&#x27;</span> | cut -d: -f 2`</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> <span class="keyword">for</span> i <span class="keyword">in</span> <span class="variable">$d</span> ; <span class="keyword">do</span></span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash"> heketi-cli device delete <span class="variable">$i</span></span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash"> <span class="keyword">done</span></span></span><br><span class="line">Device e4805400ffa45d6da503da19b26baad6 deleted</span><br><span class="line">Device ecc3c65e4d22abf3980deba4ae90238c deleted</span><br><span class="line">Device 3dc3b3f0dfd749e8dc4ee98ed2cc4141 deleted</span><br><span class="line">Device 4122bdbbe28017944a44e42b06755b1c deleted</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> heketi-cli node delete <span class="variable">$node1</span></span></span><br><span class="line">Node 61d019bb0f717e04ecddfefa5555bc41 deleted</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> heketi-cli node delete <span class="variable">$node2</span></span></span><br><span class="line">Node e97d77d0191c26089376c78202ee2f20 deleted</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> heketi-cli cluster delete <span class="variable">$cluster</span></span></span><br><span class="line">Cluster 6fe4dcffb9e077007db17f737ed999fe deleted</span><br></pre></td></tr></table></figure><h1 id="可用性测试"><a href="#可用性测试" class="headerlink" title="可用性测试"></a>可用性测试</h1><h2 id="添加节点"><a href="#添加节点" class="headerlink" title="添加节点"></a>添加节点</h2><p>通过restful api添加一台glusterfs主机，可以正常使用，前提是在添加之前要在新节点安装好glusterfs和lvm，加载 dm_thin_pool 模块，开启相关端口，给新节点打glusterfs的tag(daemonset用)，集群内节点可以互相解析域名。</p><h2 id="关闭节点"><a href="#关闭节点" class="headerlink" title="关闭节点"></a>关闭节点</h2><p>测试高可用中的坑。</p><p>三个glusterfs节点，关闭一台，客户端可读可写。</p><p>三个glusterfs节点，关闭两台，客户端可读不可写。</p><p>关闭虚机后发现heketi中还是显示节点在线，bricks不会同步到新增的虚机上，尝试把那台关了的机器剔除，发现bricks同步到了新增的节点。具体操作如下：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">[root@redhat1 /]# heketi-cli -s http://redhat1:30080 node list</span><br><span class="line">Id:05ac57499e3fbc0f8ec5a3301fac92c7Cluster:43c28e6f4b4e05f58ebd3b6f158982cd</span><br><span class="line">Id:5e10a4c264a2fc13ecdf8d2482ba7281Cluster:43c28e6f4b4e05f58ebd3b6f158982cd</span><br><span class="line">Id:b1e3ba52f6e8c82e8b3e512798348876Cluster:43c28e6f4b4e05f58ebd3b6f158982cd</span><br><span class="line">Id:f1bd8cd0f52b2d91b13e79799a34c2edCluster:43c28e6f4b4e05f58ebd3b6f158982cd</span><br><span class="line">f1bd8cd0f52b2d91b13e79799a34c2ed为已关闭的节点</span><br><span class="line">5e10a4c264a2fc13ecdf8d2482ba7281为新添加的节点</span><br><span class="line"> </span><br><span class="line">[root@redhat1 /]# heketi-cli -s http://redhat1:30080 node disable f1bd8cd0f52b2d91b13e79799a34c2ed</span><br><span class="line">Node f1bd8cd0f52b2d91b13e79799a34c2ed is now offline</span><br><span class="line"></span><br><span class="line">[root@redhat1 /]#heketi-cli-s http://redhat1:30080 node remove f1bd8cd0f52b2d91b13e79799a34c2ed</span><br><span class="line">Node f1bd8cd0f52b2d91b13e79799a34c2ed is now removed</span><br><span class="line"></span><br><span class="line">[root@redhat1 /]#heketi-cli -s http://redhat1:30080 device delete ff257d2350f05f7f5ebaa2853e5815e8</span><br><span class="line">Error: Failed to delete device /dev/sdb with id ff257d2350f05f7f5ebaa2853e5815e8 on host redhat3: error dialing backend: dial tcp 192.168.186.12:10250: connect: no route to host</span><br><span class="line"></span><br><span class="line">[root@redhat1 /]# heketi-cli -s http://redhat1:30080 node delete f1bd8cd0f52b2d91b13e79799a34c2ed</span><br><span class="line">Error: Unable to delete node [f1bd8cd0f52b2d91b13e79799a34c2ed] because it contains devices</span><br></pre></td></tr></table></figure><p>报错是因为关机了连不上节点，此时再查看关闭的哪个节点上bricks已经没了，同步到了新增加的节点上。</p><blockquote><p> 在旧节点从群集中完全清除之前，新增的节点不能和原先关闭的节点共用同样的标识。</p></blockquote><h2 id="硬盘损坏"><a href="#硬盘损坏" class="headerlink" title="硬盘损坏"></a>硬盘损坏</h2><p>本地三台gfs节点，每台挂载一块裸盘，把其中一块盘给删除模拟磁盘损坏，此时heketi中还能看到device，并且正在使用，登录那台节点执行partprobe更新下磁盘后发现lv和vg没有了，pv会存在残留数据，重启此节点后残留数据消失，heketi端还显示device正在使用。</p><p>手动删除device报以下错误</p><p>Error: Failed to remove device, error: No Replacement was found for resource</p><p>此错误是因为存储设备当前没有达到副本数要求的三个。</p><p><strong>解决方案：</strong></p><p>添加device后删除原先的device即可。</p><h2 id="brick损坏"><a href="#brick损坏" class="headerlink" title="brick损坏"></a>brick损坏</h2><p>手动删除gfs节点的一个brick:</p><p><img src="/doc_picture/gfs-test-1.png" alt="image-20210718192918628"></p><p>查看当前volume的状态，此时brick显示离线:</p><p><img src="/doc_picture/gfs-test-2.png" alt="image-20210718192931819"></p><p><strong>解决方案：</strong></p><p>先从volume端删除此brick</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> gluster volume remove-brick &lt;volume-name&gt;  replica &lt;count&gt; force</span></span><br></pre></td></tr></table></figure><blockquote><p>replica  2参数，开始我们创建卷时复制数是3，现在变为2。</p></blockquote><p><img src="/doc_picture/gfs-test-3.jpg" alt="img"> </p><p>此时查看volume状态，brick已经删除，volume变为2副本</p><p><img src="/doc_picture/gfs-test-4.jpg" alt="img"> </p><p>重新添加回此brick:</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> gluster volume add-brick &lt;volume-name&gt;  replica &lt;count&gt; force</span></span><br></pre></td></tr></table></figure><p><img src="/doc_picture/gfs-test-5.jpg" alt="img"> </p><p>查看volume状态，brick已经添加回来了</p><p><img src="/doc_picture/gfs-test-6.jpg" alt="img"> </p><p>查看此brick中数据已恢复。</p><p>上述方法是模拟其中一个brick故障，如果此卷可以重启的话可以快速重启尝试恢复。</p><h2 id="创建大于剩余空间的卷"><a href="#创建大于剩余空间的卷" class="headerlink" title="创建大于剩余空间的卷"></a>创建大于剩余空间的卷</h2><p>创建不出来，报错</p><p><img src="/doc_picture/gfs-test-7.png" alt="image-20210718193341163"></p><h2 id="扩容volume"><a href="#扩容volume" class="headerlink" title="扩容volume"></a>扩容volume</h2><p>提前申请一个1g大小的pvc并且挂载到应用服务，写一些数据，然后将应用服务停掉，扩容pvc到2G大小，再把应用服务开启，测试写，因为扩容volume想当于是在原先子卷的情况下又加了一个子卷，但是就算是新加入的子卷有剩余空间，glusterfs的hash寻址机制也会一直读写老的子卷（找之前的文件hash值），尝试给卷做rebalance操作来触发数据均衡操作，（扩容后会自动进行rebalance），没有效果。</p><p>创建存储盘使应提前预估好使用量大小。</p><p><img src="/doc_picture/gfs-test-8.png" alt="image-20210718193421419"></p><p><img src="/doc_picture/gfs-test-9.png" alt="image-20210718193430193"></p><p><img src="/doc_picture/gfs-test-10.png" alt="image-20210718193443985"></p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;环境描述&quot;&gt;&lt;a href=&quot;#环境描述&quot; class=&quot;headerlink&quot; title=&quot;环境描述&quot;&gt;&lt;/a&gt;环境描述&lt;/h1&gt;&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;&lt;strong&gt;主机名&lt;/strong&gt;&lt;/th&gt;
&lt;th&gt;ip地址&lt;/th&gt;
</summary>
      
    
    
    
    
    <category term="glusterfs" scheme="https://slions.github.io/tags/glusterfs/"/>
    
    <category term="kubernetes storage" scheme="https://slions.github.io/tags/kubernetes-storage/"/>
    
  </entry>
  
  <entry>
    <title>glusterfs回收站功能</title>
    <link href="https://slions.github.io/2021/07/16/glusterfs%E5%9B%9E%E6%94%B6%E7%AB%99%E5%8A%9F%E8%83%BD/"/>
    <id>https://slions.github.io/2021/07/16/glusterfs%E5%9B%9E%E6%94%B6%E7%AB%99%E5%8A%9F%E8%83%BD/</id>
    <published>2021-07-15T16:24:50.000Z</published>
    <updated>2021-07-15T16:33:07.228Z</updated>
    
    <content type="html"><![CDATA[<h1 id="功能简述"><a href="#功能简述" class="headerlink" title="功能简述"></a>功能简述</h1><p>glusterfs有一个类似windows回收站的功能，可以帮助用户获取和恢复临时被删除的数据。每个块都会保留一个隐藏的目录.trash，它将会被用于存放被从各个块删除的文件。这个translator以后还会增强功能来支持被删除文件的恢复。</p><p>回收站的目录名应该是可配置的。trash translator也会被用于内部操作比如自卷的自修复以及再平衡。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> gluster volume <span class="built_in">set</span> &lt;VOLNAME&gt; features.trash &lt;on/off&gt;</span></span><br></pre></td></tr></table></figure><p>此命令可用于启用卷中的Trash translator,如果设置为on，则在卷启动命令期间，将在卷内的每个brick块中创建.trashcan目录。默认情况下，translator在卷启动期间加载，但仍然不起作用。在此选项的帮助下禁用垃圾桶将不会从卷中删除垃圾邮件目录或甚至其内容。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> gluster volume <span class="built_in">set</span> &lt;VOLNAME&gt; features.trash-dir &lt;name&gt;</span></span><br></pre></td></tr></table></figure><p>此命令用于将垃圾目录重新配置为用户指定的名称。参数是有效的目录名称。目录将在这个名字下面的每个brick内创建。如果用户没有指定，translator将创建默认名称为“.trashcan”的垃圾桶目录。只有当Trash translator开启时才可使用。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> gluster volume <span class="built_in">set</span> &lt;VOLNAME&gt; features.trash-max-filesize &lt;size&gt;</span></span><br></pre></td></tr></table></figure><p>此命令可用于根据大小过滤进入垃圾目录的文件。大小超过rash_max_filesize的文件将直接删除/截断。大小值后可以跟乘性后缀，例如KB（= 1024字节），MB（= 1024 * 1024字节）和GB（= 1024 * 1024 * 1024字节）。默认大小设置为5MB。考虑到垃圾目录占用了glusterfs卷空间这一事实，垃圾邮件功能的实现方式是，即使此选项设置为大于1GB的某个值，它也可以直接删除/截断大于1GB的文件。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> gluster volume <span class="built_in">set</span> &lt;VOLNAME&gt; features.trash-internal-op &lt;on/off&gt;</span></span><br></pre></td></tr></table></figure><p>此命令可用于为内部操作（例如自愈和重新平衡）启用垃圾桶。默认设置为关闭。</p><h1 id="简单测试"><a href="#简单测试" class="headerlink" title="简单测试"></a>简单测试</h1><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">[root@gluster1 /]# gluster volume info</span><br><span class="line"> </span><br><span class="line">Volume Name: gv1</span><br><span class="line">Type: Distribute</span><br><span class="line">Volume ID: 58bf037f-5b56-4cf6-8dab-9e9944800b61</span><br><span class="line">Status: Started</span><br><span class="line">Number of Bricks: 2</span><br><span class="line">Transport-type: tcp</span><br><span class="line">Bricks:</span><br><span class="line">Brick1: gluster1:/storage/brick1</span><br><span class="line">Brick2: mystorage2:/storage/brick1</span><br><span class="line">Options Reconfigured:</span><br><span class="line">features.trash: on</span><br><span class="line">performance.readdir-ahead: on</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>启用gv1卷中的Trash translator：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@gluster1 /]# gluster volume set gv1 features.trash on</span><br><span class="line">volume set: success</span><br></pre></td></tr></table></figure><p>进入到挂载目录进行删除操作:</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[root@gluster1 /]# cd /mnt/</span><br><span class="line">[root@gluster1 mnt]# ls</span><br><span class="line">aa  bb  cc  ddd</span><br><span class="line">[root@gluster1 mnt]# rm -rf cc</span><br></pre></td></tr></table></figure><p>查看目录发现有带时间戳的文件:</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">[root@gluster1 mnt]# ls -la</span><br><span class="line">total 12</span><br><span class="line">drwxr-xr-x   4 root root 4096 May 21 06:03 .</span><br><span class="line">dr-xr-xr-x. 23 root root 4096 May 20 17:23 ..</span><br><span class="line">-rwxr-xr-x   1 root root    0 May 20 00:36 aa</span><br><span class="line">-rwxr-xr-x   1 root root    0 May 20 00:36 bb</span><br><span class="line">-rwxr-xr-x   1 root root    0 May 20 00:36 ddd</span><br><span class="line">drwsr-sr-x   3 root root 4096 May 20 23:22 .trashcan</span><br><span class="line">[root@gluster1 mnt]# cd .trashcan/</span><br><span class="line">[root@gluster1 .trashcan]# ls</span><br><span class="line">cc_2019-05-20_151208  internal_op</span><br></pre></td></tr></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;功能简述&quot;&gt;&lt;a href=&quot;#功能简述&quot; class=&quot;headerlink&quot; title=&quot;功能简述&quot;&gt;&lt;/a&gt;功能简述&lt;/h1&gt;&lt;p&gt;glusterfs有一个类似windows回收站的功能，可以帮助用户获取和恢复临时被删除的数据。每个块都会保留一个隐藏的目录</summary>
      
    
    
    
    
    <category term="glusterfs" scheme="https://slions.github.io/tags/glusterfs/"/>
    
  </entry>
  
  <entry>
    <title>k8s-v1.11使用glusterfs</title>
    <link href="https://slions.github.io/2021/07/15/k8s-v1.11%E4%BD%BF%E7%94%A8glusterfs/"/>
    <id>https://slions.github.io/2021/07/15/k8s-v1.11%E4%BD%BF%E7%94%A8glusterfs/</id>
    <published>2021-07-15T15:18:09.000Z</published>
    <updated>2021-07-15T16:21:50.292Z</updated>
    
    <content type="html"><![CDATA[<p>Glusterfs提供底层存储功能，heketi为glusterfs提供restful风格的api，Heketi要求在每个glusterfs节点上配备<strong>裸磁盘</strong>，目前heketi仅支持使用裸磁盘(未格式化)添加为device，不支持文件系统，因为Heketi要用来创建PV和VG方便管理glusterfs。</p><p>集群托管于heketi后，不能使用命令管理存储卷，以免与Heketi数据库中存储的信息不一致。</p><blockquote><p>glusterfs支持k8s的pv的3种访问模式ReadWriteOnce，ReadOnlyMany ，ReadWriteMany。访问模式只是能力描述，并不是强制执行的，对于没有按pvc声明的方式使用pv，存储提供者应该负责访问时的运行错误。例如：如果设置pvc的访问模式为ReadOnlyMany ，pod挂载后依然可写，如果需要真正的不可写，申请pvc是需要指定 readOnly: true 参数。</p></blockquote><h1 id="环境描述"><a href="#环境描述" class="headerlink" title="环境描述"></a>环境描述</h1><table><thead><tr><th><strong>主机名</strong></th><th>ip地址</th><th><strong>系统环境</strong></th><th><strong>角色</strong></th></tr></thead><tbody><tr><td>k8s-1</td><td>IP:192.168.186.10</td><td>cpu:x2  mem:2GB  disk:/dev/sdb 10GB</td><td>K8s_master,Gluster_master,Heketi_master</td></tr><tr><td>K8s-2</td><td>IP:192.168.186.11</td><td>cpu:x2  mem:2GB  disk:/dev/sdb 10GB</td><td>K8s_node,Gluster_node</td></tr><tr><td>K8s-3</td><td>IP:192.168.186.12</td><td>cpu:x2  mem:2GB  disk:/dev/sdb 10GB</td><td>K8s_master,Gluster_node</td></tr></tbody></table><p>如果存在iptable限制，需执行以下命令开通以下port</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">iptables -N heketi</span><br><span class="line">iptables -A heketi -p tcp -m state --state NEW -m tcp --dport 24007 -j ACCEPT</span><br><span class="line">iptables -A heketi -p tcp -m state --state NEW -m tcp --dport 24008 -j ACCEPT</span><br><span class="line">iptables -A heketi -p tcp -m state --state NEW -m tcp --dport 2222 -j ACCEPT</span><br><span class="line">iptables -A heketi -p tcp -m state --state NEW -m multiport --dports 49152:49251 -j ACCEPT</span><br><span class="line">service iptables save</span><br></pre></td></tr></table></figure><h1 id="安装步骤"><a href="#安装步骤" class="headerlink" title="安装步骤"></a>安装步骤</h1><p>下面的测试是采用容器化方式部署GFS，GFS以Daemonset的方式进行部署，保证每台需要部署GFS管理服务的Node上都运行一个GFS管理服务。</p><h2 id="三台节点执行："><a href="#三台节点执行：" class="headerlink" title="三台节点执行："></a>三台节点执行：</h2><p>要求所有node节点存在主机的解析记录，务必配置好/etc/hosts</p><p><img src="/doc_picture/heketi-1.png" alt="image-20210714161502089"></p><p>安装 glusterfs 每节点需要提前加载 <code>dm_thin_pool</code> 模块：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> modprobe dm_thin_pool</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> modprobe dm_snapshot</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> modprobe dm_mirror</span></span><br></pre></td></tr></table></figure><p>配置开启自加载：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> cat &gt;/etc/modules-load.d/glusterfs.conf&lt;&lt;<span class="string">EOF</span></span></span><br><span class="line">dm_thin_pool</span><br><span class="line">dm_snapshot</span><br><span class="line">dm_mirror</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure><p>安装 glusterfs-fuse：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> yum install -y glusterfs-fuse lvm2</span></span><br></pre></td></tr></table></figure><h2 id="第一台节点执行："><a href="#第一台节点执行：" class="headerlink" title="第一台节点执行："></a>第一台节点执行：</h2><h3 id="安装glusterfs与heketi"><a href="#安装glusterfs与heketi" class="headerlink" title="安装glusterfs与heketi"></a>安装glusterfs与heketi</h3><p>安装 heketi client</p><p><a href="https://github.com/heketi/heketi/releases">https://github.com/heketi/heketi/releases</a></p><p>去github下载相关的版本：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-1 ~]# wget https://github.com/heketi/heketi/releases/download/v8.0.0/heketi-client-v9.0.0.linux.amd64.tar.gz</span><br><span class="line">[root@k8s-1 ~]# tar xf heketi-client-v9.0.0.linux.amd64.tar.gz</span><br><span class="line">[root@k8s-1 ~]# cp heketi-client/bin/heketi-cli /usr/local/bin</span><br></pre></td></tr></table></figure><p> 查看版本：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-1 ~]# heketi-cli -v</span><br></pre></td></tr></table></figure><p>之后部署步骤都在如下目录执行：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-1 ~]#cd heketi-client/share/heketi/kubernetes</span><br></pre></td></tr></table></figure><p>在k8s中部署 glusterfs：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-1 kubernetes]# kubectl create -f glusterfs-daemonset.json</span><br></pre></td></tr></table></figure><blockquote><ol><li>此时采用的为默认的挂载方式，可使用其他磁盘当做GFS的工作目录</li><li>此时创建的namespace为默认的default，按需更改</li></ol></blockquote><p>给提供存储 node 节点打 label:</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-1 kubernetes]# kubectl label node k8s-1 k8s-2 k8s-3 storagenode=glusterfs</span><br></pre></td></tr></table></figure><p>查看 glusterfs 状态:</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-1 kubernetes]# kubectl get pods -o wide</span><br></pre></td></tr></table></figure><p>部署 heketi server #配置 heketi server 的权限:</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-1 kubernetes]# kubectl create -f heketi-service-account.json</span><br><span class="line">[root@k8s-1 kubernetes]# kubectl create clusterrolebinding heketi-gluster-admin --clusterrole=edit --serviceaccount=default:heketi-service-account</span><br></pre></td></tr></table></figure><p> 创建 cofig secret:</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-1 kubernetes]# kubectl create secret generic heketi-config-secret --from-file=./heketi.json</span><br></pre></td></tr></table></figure><p> 初始化部署:</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-1 kubernetes]# kubectl create -f heketi-bootstrap.json</span><br></pre></td></tr></table></figure><p># 查看 heketi bootstrap 状态</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-1 kubernetes]# kubectl get pods -o wide</span><br><span class="line"></span><br><span class="line">[root@k8s-1 kubernetes]# kubectl get svc</span><br></pre></td></tr></table></figure><p># 配置端口转发 heketi server</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-1 kubernetes]# HEKETI_BOOTSTRAP_POD=$(kubectl get pods | grep deploy-heketi | awk &#x27;&#123;print $1&#125;&#x27;)</span><br><span class="line"></span><br><span class="line">[root@k8s-1 kubernetes]# kubectl port-forward $HEKETI_BOOTSTRAP_POD 58080:8080</span><br></pre></td></tr></table></figure><p># 测试访问,另起一终端</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-1 ~]#curl http://localhost:58080/hello</span><br></pre></td></tr></table></figure><h3 id="配置-glusterfs"><a href="#配置-glusterfs" class="headerlink" title="配置 glusterfs"></a>配置 glusterfs</h3><blockquote><ol><li>hostnames/manage 字段里必须和 kubectl get node 一致</li><li>hostnames/storage 指定存储网络 ip 本次实验使用与k8s集群同一个ip</li></ol></blockquote><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s<span class="number">-1</span> kubernetes]# cat &gt;topology.json&lt;&lt;EOF</span><br><span class="line">&#123;</span><br><span class="line">  <span class="attr">&quot;clusters&quot;</span>: [</span><br><span class="line">    &#123;</span><br><span class="line">      <span class="attr">&quot;nodes&quot;</span>: [</span><br><span class="line">        &#123;</span><br><span class="line">          <span class="attr">&quot;node&quot;</span>: &#123;</span><br><span class="line">            <span class="attr">&quot;hostnames&quot;</span>: &#123;</span><br><span class="line">              <span class="attr">&quot;manage&quot;</span>: [</span><br><span class="line">                <span class="string">&quot;k8s-1&quot;</span></span><br><span class="line">              ],</span><br><span class="line">              <span class="attr">&quot;storage&quot;</span>: [</span><br><span class="line">                <span class="string">&quot;192.168.186.10&quot;</span></span><br><span class="line">              ]</span><br><span class="line">            &#125;,</span><br><span class="line">            <span class="attr">&quot;zone&quot;</span>: <span class="number">1</span></span><br><span class="line">          &#125;,</span><br><span class="line">          <span class="attr">&quot;devices&quot;</span>: [</span><br><span class="line">            <span class="string">&quot;/dev/sdb&quot;</span></span><br><span class="line">          ]</span><br><span class="line">        &#125;,</span><br><span class="line">        &#123;</span><br><span class="line">          <span class="attr">&quot;node&quot;</span>: &#123;</span><br><span class="line">            <span class="attr">&quot;hostnames&quot;</span>: &#123;</span><br><span class="line">              <span class="attr">&quot;manage&quot;</span>: [</span><br><span class="line">                <span class="string">&quot;k8s-2&quot;</span></span><br><span class="line">              ],</span><br><span class="line">              <span class="attr">&quot;storage&quot;</span>: [</span><br><span class="line">                <span class="string">&quot;192.168.186.11&quot;</span></span><br><span class="line">              ]</span><br><span class="line">            &#125;,</span><br><span class="line">            <span class="attr">&quot;zone&quot;</span>: <span class="number">1</span></span><br><span class="line">          &#125;,</span><br><span class="line">          <span class="attr">&quot;devices&quot;</span>: [</span><br><span class="line">            <span class="string">&quot;/dev/sdb&quot;</span></span><br><span class="line">          ]</span><br><span class="line">        &#125;,</span><br><span class="line">        &#123;</span><br><span class="line">          <span class="attr">&quot;node&quot;</span>: &#123;</span><br><span class="line">            <span class="attr">&quot;hostnames&quot;</span>: &#123;</span><br><span class="line">              <span class="attr">&quot;manage&quot;</span>: [</span><br><span class="line">                <span class="string">&quot;k8s-3&quot;</span></span><br><span class="line">              ],</span><br><span class="line">              <span class="attr">&quot;storage&quot;</span>: [</span><br><span class="line">                <span class="string">&quot;192.168.186.12&quot;</span></span><br><span class="line">              ]</span><br><span class="line">            &#125;,</span><br><span class="line">            <span class="attr">&quot;zone&quot;</span>: <span class="number">1</span></span><br><span class="line">          &#125;,</span><br><span class="line">          <span class="attr">&quot;devices&quot;</span>: [</span><br><span class="line">            <span class="string">&quot;/dev/sdb&quot;</span></span><br><span class="line">          ]                                                                                       </span><br><span class="line">        &#125;</span><br><span class="line">      ]</span><br><span class="line">    &#125;</span><br><span class="line">  ]</span><br><span class="line">&#125;</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure><p>heketi加载配置:</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-1 kubernetes]# export HEKETI_CLI_SERVER=http://localhost:58080</span><br><span class="line"></span><br><span class="line">[root@k8s-1 kubernetes]# heketi-cli topology load --json=topology.json</span><br></pre></td></tr></table></figure><p>使用 Heketi 创建一个用于存储 Heketi 数据库的 volume：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-1 kubernetes]# heketi-cli setup-openshift-heketi-storage</span><br><span class="line">[root@k8s-1 kubernetes]# kubectl create -f heketi-storage.json</span><br></pre></td></tr></table></figure><blockquote><p>heketi-storage.json中：</p><p>创建了heketi-storage-endpoints，（指明了gfs地址和端口，默认端口为1）创建了heketi-storage-copy-job，此job的作用就是复制heketi中的数据文件到 /heketi，而/heketi目录挂载在了卷heketi-storage中，而heketi-storage volume是前面执行”heketi-cli setup-openshift-heketi-storage”时创建好了的。</p></blockquote><p>查看状态,等所有job完成 即状态为 <code>Completed</code>,才能进行如下的步骤：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-1 kubernetes]# kubectl get pods</span><br><span class="line">[root@k8s-1 kubernetes]# kubectl get job</span><br></pre></td></tr></table></figure><p> 删除部署时产生的相关资源：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-1 kubernetes]# kubectl delete all,service,jobs,deployment,secret --selector=&quot;deploy-heketi&quot;</span><br></pre></td></tr></table></figure><p># 部署 heketi server</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-1 kubernetes]# kubectl create -f heketi-deployment.json</span><br></pre></td></tr></table></figure><p># 查看 heketi server 状态</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-1 kubernetes]# kubectl get pods -o wide</span><br><span class="line">[root@k8s-1 kubernetes]# kubectl get svc</span><br></pre></td></tr></table></figure><p># 查看 heketi 状态信息, 配置端口转发 heketi server</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-1 kubernetes]# HEKETI_BOOTSTRAP_POD=$(kubectl get pods | grep heketi | awk &#x27;&#123;print $1&#125;&#x27;)</span><br><span class="line">[root@k8s-1 kubernetes]# kubectl port-forward $HEKETI_BOOTSTRAP_POD 58080:8080</span><br><span class="line">[root@k8s-1 kubernetes]# export HEKETI_CLI_SERVER=http://localhost:58080</span><br><span class="line">[root@k8s-1 kubernetes]# heketi-cli cluster list</span><br><span class="line">[root@k8s-1 kubernetes]# heketi-cli volume list</span><br></pre></td></tr></table></figure><blockquote><p>可以把heketi的service type换成NodePrort,并给glusterfs的daemonset添加spec. template.spec.hostNetwork: true,之后就不用以端口转发映射本地端口的方式访问heketi，直接heketi-cli -s <a href="srv:port">srv:port</a> 即可</p></blockquote><h3 id="创建-StorageClass"><a href="#创建-StorageClass" class="headerlink" title="创建 StorageClass"></a>创建 StorageClass</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-1 kubernetes]# HEKETI_SERVER=$(kubectl get svc | grep heketi | head -1 | awk &#x27;&#123;print $3&#125;&#x27;)</span><br><span class="line">[root@k8s-1 kubernetes]# echo $HEKETI_SERVER</span><br><span class="line">[root@k8s-1 kubernetes]# cat &gt;storageclass-glusterfs.yaml&lt;&lt;EOF</span><br><span class="line">kind: StorageClass</span><br><span class="line">apiVersion: storage.k8s.io/v1</span><br><span class="line">metadata:</span><br><span class="line">  name: gluster-heketi</span><br><span class="line">provisioner: kubernetes.io/glusterfs</span><br><span class="line"><span class="meta">#</span><span class="bash">reclaimPolicy: Retain</span></span><br><span class="line">parameters:</span><br><span class="line">  resturl: &quot;http://$HEKETI_SERVER:8080&quot;</span><br><span class="line">  gidMin: &quot;40000&quot;</span><br><span class="line">  gidMax: &quot;50000&quot;</span><br><span class="line">  volumetype: &quot;replicate:3&quot;</span><br><span class="line"><span class="meta">#</span><span class="bash">允许对pvc扩容</span></span><br><span class="line">allowVolumeExpansion: true</span><br><span class="line">EOF</span><br><span class="line">[root@k8s-1 kubernetes]# kubectl create -f storageclass-glusterfs.yaml</span><br></pre></td></tr></table></figure><blockquote><ol><li>以上创建了一个含有三个副本的gluster的存储类型（storage-class） </li><li>volumetype中的relicate必须大于1，否则创建pvc的时候会报错</li><li>在这里创建的storageclass显示指定reclaimPolicy为Retain(默认情况下是Delete)，删除pvc后pv以及后端的volume、brick(lvm)不会被删除。</li></ol></blockquote><h3 id="创建pvc"><a href="#创建pvc" class="headerlink" title="创建pvc"></a>创建pvc</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-1 kubernetes]# cat &gt;gluster-pvc-test.yaml&lt;&lt;EOF</span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: PersistentVolumeClaim</span><br><span class="line">metadata:</span><br><span class="line"> name: gluster1</span><br><span class="line"> annotations:</span><br><span class="line">   volume.beta.kubernetes.io/storage-class: gluster-heketi</span><br><span class="line">spec:</span><br><span class="line"> accessModes:</span><br><span class="line">  - ReadWriteMany</span><br><span class="line"> resources:</span><br><span class="line">   requests:</span><br><span class="line">     storage: 1Gi</span><br><span class="line">EOF</span><br><span class="line">[root@k8s-1 kubernetes]# kubectl apply -f gluster-pvc-test.yaml</span><br></pre></td></tr></table></figure><p>查看卷状态</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-1 kubernetes]# kubectl get pvc</span><br><span class="line">[root@k8s-1 kubernetes]# kubectl get pv</span><br></pre></td></tr></table></figure><h3 id="创建服务挂载测试"><a href="#创建服务挂载测试" class="headerlink" title="创建服务挂载测试"></a>创建服务挂载测试</h3><figure class="highlight yml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line">[<span class="string">root@k8s-1</span> <span class="string">kubernetes</span>]<span class="comment"># cat &gt;nginx-pod.yaml&lt;&lt;EOF</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">extensions/v1beta1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Deployment</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">nginx-gfs</span></span><br><span class="line">  <span class="attr">labels:</span></span><br><span class="line">    <span class="attr">name:</span> <span class="string">nginx-gfs</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">replicas:</span> <span class="number">2</span></span><br><span class="line">  <span class="attr">selector:</span></span><br><span class="line">    <span class="attr">matchLabels:</span></span><br><span class="line">      <span class="attr">name:</span> <span class="string">nginx-gfs</span></span><br><span class="line">  <span class="attr">template:</span></span><br><span class="line">    <span class="attr">metadata:</span></span><br><span class="line">      <span class="attr">labels:</span></span><br><span class="line">        <span class="attr">name:</span> <span class="string">nginx-gfs</span></span><br><span class="line">    <span class="attr">spec:</span></span><br><span class="line">      <span class="attr">containers:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">nginx-gfs</span></span><br><span class="line">        <span class="attr">image:</span> <span class="string">nginx</span></span><br><span class="line">        <span class="attr">ports:</span></span><br><span class="line">        <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">web</span></span><br><span class="line">          <span class="attr">containerPort:</span> <span class="number">80</span></span><br><span class="line">        <span class="attr">volumeMounts:</span></span><br><span class="line">        <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">gfs</span></span><br><span class="line">          <span class="attr">mountPath:</span> <span class="string">/usr/share/nginx/html</span></span><br><span class="line">      <span class="attr">volumes:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">gfs</span></span><br><span class="line">        <span class="attr">persistentVolumeClaim:</span></span><br><span class="line">          <span class="attr">claimName:</span> <span class="string">gluster1</span></span><br><span class="line"><span class="string">EOF</span></span><br><span class="line">[<span class="string">root@k8s-1</span> <span class="string">kubernetes</span>]<span class="comment"># kubectl apply -f nginx-pod.yaml</span></span><br></pre></td></tr></table></figure><p>查看服务是否正常启动：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-1 kubernetes]# kubectl get pods -o wide</span><br></pre></td></tr></table></figure><h3 id="测试pvc的扩容"><a href="#测试pvc的扩容" class="headerlink" title="测试pvc的扩容"></a>测试pvc的扩容</h3><p>修改pvc/gluster1容量1G改为2G，过一会儿会自动生效，此时查看pv,pvc,和进入容器都已经成了2G（自己机器上测试发现生效时长大概为1min），把容器停掉继续扩容发现也是ok的。</p><h1 id="分析篇"><a href="#分析篇" class="headerlink" title="分析篇"></a>分析篇</h1><h2 id="heketi是怎么对磁盘进行操作的"><a href="#heketi是怎么对磁盘进行操作的" class="headerlink" title="heketi是怎么对磁盘进行操作的"></a>heketi是怎么对磁盘进行操作的</h2><p>回过头来分析下heketi加载gfs配置时进行了什么操作</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">$ heketi-cli topology load --json=topology-sample.json  </span><br><span class="line">   Creating cluster ... ID: 224a5a6555fa5c0c930691111c63e863</span><br><span class="line">     Allowing file volumes on cluster.</span><br><span class="line">     Allowing block volumes on cluster.</span><br><span class="line">     Creating node 192.168.186.10 ... ID: 7946b917b91a579c619ba51d9129aeb0</span><br><span class="line">            Adding device /dev/sdb ... OK</span><br><span class="line">     Creating node 192.168.186.11 ... ID: 5d10e593e89c7c61f8712964387f959c</span><br><span class="line">            Adding device /dev/sdb ... OK</span><br><span class="line">     Creating node 192.168.186.12 ... ID: de620cb2c313a5461d5e0a6ae234c553</span><br><span class="line">            Adding device /dev/sdb ... OK</span><br></pre></td></tr></table></figure><ul><li>进入任意glusterfs Pod内，执行gluster peer status 发现都已把对端加入到了可信存储池(TSP)中。</li><li>在运行了gluster Pod的节点上，自动创建了一个VG，此VG正是由topology-sample.json 文件中的磁盘裸设备创建而来。</li><li>一块磁盘设备创建出一个VG，以后创建的PVC，即从此VG里划分的LV。</li><li>heketi-cli topology info 查看拓扑结构，显示出每个磁盘设备的ID，对应VG的ID，总空间、已用空间、空余空间等信息。</li></ul><h2 id="heketi创建db-volume的流程"><a href="#heketi创建db-volume的流程" class="headerlink" title="heketi创建db volume的流程"></a>heketi创建db volume的流程</h2><p>执行heketi-cli setup-openshift-heketi-storage并观测heketi后台做了什么，可以通过相应日志查看：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br></pre></td><td class="code"><pre><span class="line">[negroni] 2020-03-06T16:45:13Z | 200 |   61.841µs | 192.168.186.10:30080 | GET /clusters</span><br><span class="line">[negroni] 2020-03-06T16:45:13Z | 200 |   159.901µs | 192.168.186.10:30080 | GET /clusters/6749ed08e37290fbb1cc4c881872054d</span><br><span class="line">[heketi] INFO 2020/03/06 16:45:13 Allocating brick set #0</span><br><span class="line">[negroni] 2020-03-06T16:45:13Z | 202 |   46.293298ms | 192.168.186.10:30080 | POST /volumes</span><br><span class="line">[asynchttp] INFO 2020/03/06 16:45:13 asynchttp.go:288: Started job 5ffdc4ab574897e19511ae43afa7e78c</span><br><span class="line">[heketi] INFO 2020/03/06 16:45:13 Started async operation: Create Volume</span><br><span class="line">[heketi] INFO 2020/03/06 16:45:13 Trying Create Volume (attempt #1/5)</span><br><span class="line">[heketi] INFO 2020/03/06 16:45:13 Creating brick b6411ccff63daf1270bc9f354ca484dd</span><br><span class="line">[heketi] INFO 2020/03/06 16:45:13 Creating brick 98700f7b0bce70eb29279fb275763704</span><br><span class="line">[heketi] INFO 2020/03/06 16:45:13 Creating brick 777c447835963ef4db7cbb2392c85e59</span><br><span class="line">[negroni] 2020-03-06T16:45:13Z | 200 |   41.375µs | 192.168.186.10:30080 | GET /queue/5ffdc4ab574897e19511ae43afa7e78c</span><br><span class="line">[kubeexec] DEBUG 2020/03/06 16:45:13 heketi/pkg/remoteexec/kube/exec.go:81:kube.ExecCommands: Ran command [mkdir -p /var/lib/heketi/mounts/vg_ccc135aa56ab4f89868d9755bd531a22/brick_b6411ccff63daf1270bc9f354ca484dd] on [pod:glusterfs-77ghn c:glusterfs ns:glusterfs (from host:ceph2 selector:glusterfs-node)]: Stdout []: Stderr []</span><br><span class="line">[kubeexec] DEBUG 2020/03/06 16:45:13 heketi/pkg/remoteexec/kube/exec.go:81:kube.ExecCommands: Ran command [mkdir -p /var/lib/heketi/mounts/vg_63f644b972ff7a04259395f67c149cf2/brick_777c447835963ef4db7cbb2392c85e59] on [pod:glusterfs-mhsgb c:glusterfs ns:glusterfs (from host:ceph3 selector:glusterfs-node)]: Stdout []: Stderr []</span><br><span class="line">[kubeexec] DEBUG 2020/03/06 16:45:13 heketi/pkg/remoteexec/kube/exec.go:81:kube.ExecCommands: Ran command [mkdir -p /var/lib/heketi/mounts/vg_6199228451001048c7543f41ce6572cb/brick_98700f7b0bce70eb29279fb275763704] on [pod:glusterfs-fflbn c:glusterfs ns:glusterfs (from host:ceph1 selector:glusterfs-node)]: Stdout []: Stderr []</span><br><span class="line">[kubeexec] DEBUG 2020/03/06 16:45:13 heketi/pkg/remoteexec/kube/exec.go:81:kube.ExecCommands: Ran command [lvcreate -qq --autobackup=n --poolmetadatasize 12288K --chunksize 256K --size 2097152K --thin vg_ccc135aa56ab4f89868d9755bd531a22/tp_76e0f4c09fbd75bcd2bfae166fb8a73d --virtualsize 2097152K --name brick_b6411ccff63daf1270bc9f354ca484dd] on [pod:glusterfs-77ghn c:glusterfs ns:glusterfs (from host:ceph2 selector:glusterfs-node)]: Stdout []: Stderr [  WARNING: This metadata update is NOT backed up.</span><br><span class="line">]</span><br><span class="line">[kubeexec] DEBUG 2020/03/06 16:45:14 heketi/pkg/remoteexec/kube/exec.go:81:kube.ExecCommands: Ran command [lvcreate -qq --autobackup=n --poolmetadatasize 12288K --chunksize 256K --size 2097152K --thin vg_63f644b972ff7a04259395f67c149cf2/tp_777c447835963ef4db7cbb2392c85e59 --virtualsize 2097152K --name brick_777c447835963ef4db7cbb2392c85e59] on [pod:glusterfs-mhsgb c:glusterfs ns:glusterfs (from host:ceph3 selector:glusterfs-node)]: Stdout []: Stderr [  WARNING: This metadata update is NOT backed up.</span><br><span class="line">]</span><br><span class="line">[kubeexec] DEBUG 2020/03/06 16:45:14 heketi/pkg/remoteexec/kube/exec.go:81:kube.ExecCommands: Ran command [lvcreate -qq --autobackup=n --poolmetadatasize 12288K --chunksize 256K --size 2097152K --thin vg_6199228451001048c7543f41ce6572cb/tp_98700f7b0bce70eb29279fb275763704 --virtualsize 2097152K --name brick_98700f7b0bce70eb29279fb275763704] on [pod:glusterfs-fflbn c:glusterfs ns:glusterfs (from host:ceph1 selector:glusterfs-node)]: Stdout []: Stderr [  WARNING: This metadata update is NOT backed up.</span><br><span class="line">]</span><br><span class="line">[kubeexec] DEBUG 2020/03/06 16:45:14 heketi/pkg/remoteexec/kube/exec.go:81:kube.ExecCommands: Ran command [mkfs.xfs -i size=512 -n size=8192 /dev/mapper/vg_ccc135aa56ab4f89868d9755bd531a22-brick_b6411ccff63daf1270bc9f354ca484dd] on [pod:glusterfs-77ghn c:glusterfs ns:glusterfs (from host:ceph2 selector:glusterfs-node)]: Stdout [meta-data=/dev/mapper/vg_ccc135aa56ab4f89868d9755bd531a22-brick_b6411ccff63daf1270bc9f354ca484dd isize=512    agcount=8, agsize=65536 blks</span><br><span class="line">         =                       sectsz=512   attr=2, projid32bit=1</span><br><span class="line">         =                       crc=1        finobt=0, sparse=0</span><br><span class="line">data     =                       bsize=4096   blocks=524288, imaxpct=25</span><br><span class="line">         =                       sunit=64     swidth=64 blks</span><br><span class="line">naming   =version 2              bsize=8192   ascii-ci=0 ftype=1</span><br><span class="line">log      =internal log           bsize=4096   blocks=2560, version=2</span><br><span class="line">         =                       sectsz=512   sunit=64 blks, lazy-count=1</span><br><span class="line">realtime =none                   extsz=4096   blocks=0, rtextents=0</span><br><span class="line">]: Stderr []</span><br><span class="line">[kubeexec] DEBUG 2020/03/06 16:45:14 heketi/pkg/remoteexec/kube/exec.go:81:kube.ExecCommands: Ran command [awk &quot;BEGIN &#123;print \&quot;/dev/mapper/vg_ccc135aa56ab4f89868d9755bd531a22-brick_b6411ccff63daf1270bc9f354ca484dd /var/lib/heketi/mounts/vg_ccc135aa56ab4f89868d9755bd531a22/brick_b6411ccff63daf1270bc9f354ca484dd xfs rw,inode64,noatime,nouuid 1 2\&quot; &gt;&gt; \&quot;/var/lib/heketi/fstab\&quot;&#125;&quot;] on [pod:glusterfs-77ghn c:glusterfs ns:glusterfs (from host:ceph2 selector:glusterfs-node)]: Stdout []: Stderr []</span><br><span class="line">[kubeexec] DEBUG 2020/03/06 16:45:14 heketi/pkg/remoteexec/kube/exec.go:81:kube.ExecCommands: Ran command [mkfs.xfs -i size=512 -n size=8192 /dev/mapper/vg_63f644b972ff7a04259395f67c149cf2-brick_777c447835963ef4db7cbb2392c85e59] on [pod:glusterfs-mhsgb c:glusterfs ns:glusterfs (from host:ceph3 selector:glusterfs-node)]: Stdout [meta-data=/dev/mapper/vg_63f644b972ff7a04259395f67c149cf2-brick_777c447835963ef4db7cbb2392c85e59 isize=512    agcount=8, agsize=65536 blks</span><br><span class="line">         =                       sectsz=512   attr=2, projid32bit=1</span><br><span class="line">         =                       crc=1        finobt=0, sparse=0</span><br><span class="line">data     =                       bsize=4096   blocks=524288, imaxpct=25</span><br><span class="line">         =                       sunit=64     swidth=64 blks</span><br><span class="line">naming   =version 2              bsize=8192   ascii-ci=0 ftype=1</span><br><span class="line">log      =internal log           bsize=4096   blocks=2560, version=2</span><br><span class="line">         =                       sectsz=512   sunit=64 blks, lazy-count=1</span><br><span class="line">realtime =none                   extsz=4096   blocks=0, rtextents=0</span><br><span class="line">]: Stderr []</span><br><span class="line">[negroni] 2020-03-06T16:45:14Z | 200 |   34.302µs | 192.168.186.10:30080 | GET /queue/5ffdc4ab574897e19511ae43afa7e78c</span><br><span class="line">[kubeexec] DEBUG 2020/03/06 16:45:14 heketi/pkg/remoteexec/kube/exec.go:81:kube.ExecCommands: Ran command [awk &quot;BEGIN &#123;print \&quot;/dev/mapper/vg_63f644b972ff7a04259395f67c149cf2-brick_777c447835963ef4db7cbb2392c85e59 /var/lib/heketi/mounts/vg_63f644b972ff7a04259395f67c149cf2/brick_777c447835963ef4db7cbb2392c85e59 xfs rw,inode64,noatime,nouuid 1 2\&quot; &gt;&gt; \&quot;/var/lib/heketi/fstab\&quot;&#125;&quot;] on [pod:glusterfs-mhsgb c:glusterfs ns:glusterfs (from host:ceph3 selector:glusterfs-node)]: Stdout []: Stderr []</span><br><span class="line">[kubeexec] DEBUG 2020/03/06 16:45:14 heketi/pkg/remoteexec/kube/exec.go:81:kube.ExecCommands: Ran command [mkfs.xfs -i size=512 -n size=8192 /dev/mapper/vg_6199228451001048c7543f41ce6572cb-brick_98700f7b0bce70eb29279fb275763704] on [pod:glusterfs-fflbn c:glusterfs ns:glusterfs (from host:ceph1 selector:glusterfs-node)]: Stdout [meta-data=/dev/mapper/vg_6199228451001048c7543f41ce6572cb-brick_98700f7b0bce70eb29279fb275763704 isize=512    agcount=8, agsize=65536 blks</span><br><span class="line">         =                       sectsz=512   attr=2, projid32bit=1</span><br><span class="line">         =                       crc=1        finobt=0, sparse=0</span><br><span class="line">data     =                       bsize=4096   blocks=524288, imaxpct=25</span><br><span class="line">         =                       sunit=64     swidth=64 blks</span><br><span class="line">naming   =version 2              bsize=8192   ascii-ci=0 ftype=1</span><br><span class="line">log      =internal log           bsize=4096   blocks=2560, version=2</span><br><span class="line">         =                       sectsz=512   sunit=64 blks, lazy-count=1</span><br><span class="line">realtime =none                   extsz=4096   blocks=0, rtextents=0</span><br><span class="line">]: Stderr []</span><br><span class="line">[kubeexec] DEBUG 2020/03/06 16:45:14 heketi/pkg/remoteexec/kube/exec.go:81:kube.ExecCommands: Ran command [awk &quot;BEGIN &#123;print \&quot;/dev/mapper/vg_6199228451001048c7543f41ce6572cb-brick_98700f7b0bce70eb29279fb275763704 /var/lib/heketi/mounts/vg_6199228451001048c7543f41ce6572cb/brick_98700f7b0bce70eb29279fb275763704 xfs rw,inode64,noatime,nouuid 1 2\&quot; &gt;&gt; \&quot;/var/lib/heketi/fstab\&quot;&#125;&quot;] on [pod:glusterfs-fflbn c:glusterfs ns:glusterfs (from host:ceph1 selector:glusterfs-node)]: Stdout []: Stderr []</span><br><span class="line">[kubeexec] DEBUG 2020/03/06 16:45:14 heketi/pkg/remoteexec/kube/exec.go:81:kube.ExecCommands: Ran command [mount -o rw,inode64,noatime,nouuid /dev/mapper/vg_ccc135aa56ab4f89868d9755bd531a22-brick_b6411ccff63daf1270bc9f354ca484dd /var/lib/heketi/mounts/vg_ccc135aa56ab4f89868d9755bd531a22/brick_b6411ccff63daf1270bc9f354ca484dd] on [pod:glusterfs-77ghn c:glusterfs ns:glusterfs (from host:ceph2 selector:glusterfs-node)]: Stdout []: Stderr []</span><br><span class="line">[kubeexec] DEBUG 2020/03/06 16:45:14 heketi/pkg/remoteexec/kube/exec.go:81:kube.ExecCommands: Ran command [mount -o rw,inode64,noatime,nouuid /dev/mapper/vg_63f644b972ff7a04259395f67c149cf2-brick_777c447835963ef4db7cbb2392c85e59 /var/lib/heketi/mounts/vg_63f644b972ff7a04259395f67c149cf2/brick_777c447835963ef4db7cbb2392c85e59] on [pod:glusterfs-mhsgb c:glusterfs ns:glusterfs (from host:ceph3 selector:glusterfs-node)]: Stdout []: Stderr []</span><br><span class="line">[kubeexec] DEBUG 2020/03/06 16:45:14 heketi/pkg/remoteexec/kube/exec.go:81:kube.ExecCommands: Ran command [mkdir /var/lib/heketi/mounts/vg_ccc135aa56ab4f89868d9755bd531a22/brick_b6411ccff63daf1270bc9f354ca484dd/brick] on [pod:glusterfs-77ghn c:glusterfs ns:glusterfs (from host:ceph2 selector:glusterfs-node)]: Stdout []: Stderr []</span><br><span class="line">[kubeexec] DEBUG 2020/03/06 16:45:14 heketi/pkg/remoteexec/kube/exec.go:81:kube.ExecCommands: Ran command [mkdir /var/lib/heketi/mounts/vg_63f644b972ff7a04259395f67c149cf2/brick_777c447835963ef4db7cbb2392c85e59/brick] on [pod:glusterfs-mhsgb c:glusterfs ns:glusterfs (from host:ceph3 selector:glusterfs-node)]: Stdout []: Stderr []</span><br><span class="line">[kubeexec] DEBUG 2020/03/06 16:45:14 heketi/pkg/remoteexec/kube/exec.go:81:kube.ExecCommands: Ran command [mount -o rw,inode64,noatime,nouuid /dev/mapper/vg_6199228451001048c7543f41ce6572cb-brick_98700f7b0bce70eb29279fb275763704 /var/lib/heketi/mounts/vg_6199228451001048c7543f41ce6572cb/brick_98700f7b0bce70eb29279fb275763704] on [pod:glusterfs-fflbn c:glusterfs ns:glusterfs (from host:ceph1 selector:glusterfs-node)]: Stdout []: Stderr []</span><br><span class="line">[kubeexec] DEBUG 2020/03/06 16:45:14 heketi/pkg/remoteexec/kube/exec.go:81:kube.ExecCommands: Ran command [mkdir /var/lib/heketi/mounts/vg_6199228451001048c7543f41ce6572cb/brick_98700f7b0bce70eb29279fb275763704/brick] on [pod:glusterfs-fflbn c:glusterfs ns:glusterfs (from host:ceph1 selector:glusterfs-node)]: Stdout []: Stderr []</span><br><span class="line">[cmdexec] INFO 2020/03/06 16:45:14 Creating volume heketidbstorage replica 3</span><br><span class="line">[negroni] 2020-03-06T16:45:15Z | 200 |   42.505µs | 192.168.186.10:30080 | GET /queue/5ffdc4ab574897e19511ae43afa7e78c</span><br><span class="line">[kubeexec] DEBUG 2020/03/06 16:45:15 heketi/pkg/remoteexec/kube/exec.go:81:kube.ExecCommands: Ran command [gluster --mode=script --timeout=600 volume create heketidbstorage replica 3 192.168.186.11:/var/lib/heketi/mounts/vg_ccc135aa56ab4f89868d9755bd531a22/brick_b6411ccff63daf1270bc9f354ca484dd/brick 192.168.186.10:/var/lib/heketi/mounts/vg_6199228451001048c7543f41ce6572cb/brick_98700f7b0bce70eb29279fb275763704/brick 192.168.186.12:/var/lib/heketi/mounts/vg_63f644b972ff7a04259395f67c149cf2/brick_777c447835963ef4db7cbb2392c85e59/brick] on [pod:glusterfs-77ghn c:glusterfs ns:glusterfs (from host:ceph2 selector:glusterfs-node)]: Stdout [volume create: heketidbstorage: success: please start the volume to access data</span><br><span class="line">]: Stderr []</span><br><span class="line">[kubeexec] DEBUG 2020/03/06 16:45:15 heketi/pkg/remoteexec/kube/exec.go:81:kube.ExecCommands: Ran command [gluster --mode=script --timeout=600 volume set heketidbstorage user.heketi.id 17a63c6483a00155fb0b48bb353b9c7a] on [pod:glusterfs-77ghn c:glusterfs ns:glusterfs (from host:ceph2 selector:glusterfs-node)]: Stdout [volume set: success</span><br><span class="line">]: Stderr []</span><br><span class="line">[negroni] 2020-03-06T16:45:16Z | 200 |   33.664µs | 192.168.186.10:30080 | GET /queue/5ffdc4ab574897e19511ae43afa7e78c</span><br><span class="line">[negroni] 2020-03-06T16:45:17Z | 200 |   30.885µs | 192.168.186.10:30080 | GET /queue/5ffdc4ab574897e19511ae43afa7e78c</span><br><span class="line">[negroni] 2020-03-06T16:45:18Z | 200 |   101.832µs | 192.168.186.10:30080 | GET /queue/5ffdc4ab574897e19511ae43afa7e78c</span><br><span class="line">[kubeexec] DEBUG 2020/03/06 16:45:19 heketi/pkg/remoteexec/kube/exec.go:81:kube.ExecCommands: Ran command [gluster --mode=script --timeout=600 volume start heketidbstorage] on [pod:glusterfs-77ghn c:glusterfs ns:glusterfs (from host:ceph2 selector:glusterfs-node)]: Stdout [volume start: heketidbstorage: success</span><br><span class="line">]: Stderr []</span><br><span class="line">[asynchttp] INFO 2020/03/06 16:45:19 asynchttp.go:292: Completed job 5ffdc4ab574897e19511ae43afa7e78c in 5.802125511s</span><br><span class="line">[negroni] 2020-03-06T16:45:19Z | 303 |   42.089µs | 192.168.186.10:30080 | GET /queue/5ffdc4ab574897e19511ae43afa7e78c</span><br><span class="line">[negroni] 2020-03-06T16:45:19Z | 200 |   50.078742ms | 192.168.186.10:30080 | GET /volumes/17a63c6483a00155fb0b48bb353b9c7a</span><br><span class="line">[negroni] 2020-03-06T16:45:19Z | 200 |   484.808µs | 192.168.186.10:30080 | GET /backup/db</span><br><span class="line"></span><br></pre></td></tr></table></figure><h2 id="k8s是怎么通过heketi创建pvc的"><a href="#k8s是怎么通过heketi创建pvc的" class="headerlink" title="k8s是怎么通过heketi创建pvc的"></a>k8s是怎么通过heketi创建pvc的</h2><p>storageclass中会指定heketi server端的地址和卷的类型（replica 3），用户通过pvc创建1G的pv,观查heketi服务后台干了啥：</p><p>首先发现heketi接收到请求后起了一个job，创建了3个bricks，在其中三台gfs节点创建了相应的目录，如下图：</p><p><img src="/doc_picture/gfs-heketi-1.png" alt="image-20210716001252519"></p><p>创建lv,添加自动挂载：</p><p><img src="/doc_picture/gfs-heketi-2.png" alt="image-20210716001304334"></p><p>创建brick，设置权限：</p><p><img src="/doc_picture/gfs-heketi-3.png" alt="image-20210716001320020"></p><p>创建volume：</p><p><img src="/doc_picture/gfs-heketi-4.png" alt="image-20210716001344552"></p><h1 id="参考文档"><a href="#参考文档" class="headerlink" title="参考文档"></a>参考文档</h1><p>heketi: <a href="https://github.com/heketi/heketi">https://github.com/heketi/heketi</a></p><p>glusterfs:  <a href="https://github.com/gluster/gluster-kubernetes">https://github.com/gluster/gluster-kubernetes</a></p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;Glusterfs提供底层存储功能，heketi为glusterfs提供restful风格的api，Heketi要求在每个glusterfs节点上配备&lt;strong&gt;裸磁盘&lt;/strong&gt;，目前heketi仅支持使用裸磁盘(未格式化)添加为device，不支持文件系统，因</summary>
      
    
    
    
    
    <category term="glusterfs" scheme="https://slions.github.io/tags/glusterfs/"/>
    
    <category term="kubernetes storage" scheme="https://slions.github.io/tags/kubernetes-storage/"/>
    
  </entry>
  
  <entry>
    <title>linux添加新硬盘无法识别解决方法</title>
    <link href="https://slions.github.io/2021/07/15/linux%E6%B7%BB%E5%8A%A0%E6%96%B0%E7%A1%AC%E7%9B%98%E6%97%A0%E6%B3%95%E8%AF%86%E5%88%AB%E8%A7%A3%E5%86%B3%E6%96%B9%E6%B3%95/"/>
    <id>https://slions.github.io/2021/07/15/linux%E6%B7%BB%E5%8A%A0%E6%96%B0%E7%A1%AC%E7%9B%98%E6%97%A0%E6%B3%95%E8%AF%86%E5%88%AB%E8%A7%A3%E5%86%B3%E6%96%B9%E6%B3%95/</id>
    <published>2021-07-14T16:18:03.000Z</published>
    <updated>2021-07-14T16:34:35.454Z</updated>
    
    <content type="html"><![CDATA[<p>我们经常会遇到当主机新添加磁盘后，进入宿主机查看并没有显示多出来的那块磁盘，是因为Linux目前缺乏允许动态SCSI通道重配的命令。</p><p>重启主机是检测新添加磁盘设备的可靠方式，但是会造成上面运行的应用服务中断，有没有什么方法是能优雅的解决此问题呢。执行以下脚本就可以了。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> <span class="keyword">for</span> i <span class="keyword">in</span> /sys/class/scsi_host/host*/scan;<span class="keyword">do</span> <span class="built_in">echo</span> <span class="string">&quot;- - -&quot;</span> &gt;<span class="variable">$i</span>;<span class="keyword">done</span></span></span><br></pre></td></tr></table></figure><blockquote><p>其中‘- - -’代表channel，target和LUN编号。以上命令会使系统重新扫描所有channel，target以及可见LUN。</p></blockquote>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;我们经常会遇到当主机新添加磁盘后，进入宿主机查看并没有显示多出来的那块磁盘，是因为Linux目前缺乏允许动态SCSI通道重配的命令。&lt;/p&gt;
&lt;p&gt;重启主机是检测新添加磁盘设备的可靠方式，但是会造成上面运行的应用服务中断，有没有什么方法是能优雅的解决此问题呢。执行以下脚本就</summary>
      
    
    
    
    
    <category term="linux" scheme="https://slions.github.io/tags/linux/"/>
    
  </entry>
  
  <entry>
    <title>手工安装heketi管理glusterfs</title>
    <link href="https://slions.github.io/2021/07/14/%E6%89%8B%E5%B7%A5%E5%AE%89%E8%A3%85heketi%E7%AE%A1%E7%90%86glusterfs/"/>
    <id>https://slions.github.io/2021/07/14/%E6%89%8B%E5%B7%A5%E5%AE%89%E8%A3%85heketi%E7%AE%A1%E7%90%86glusterfs/</id>
    <published>2021-07-14T08:06:32.000Z</published>
    <updated>2021-07-14T08:51:24.816Z</updated>
    
    <content type="html"><![CDATA[<p>GlusterFS（gfs）由此名字也可看出是文件系统存储相关的软件，它是一个开源的分布式文件系统，具有强大的横向扩展能力。Heketi是一个GlusterFs管理软件，可以管理glusterFS集群的卷创建、删除等操作。Glusterfs作为kubernetes支持的多种卷类型之一，可以为上层应用提供多种挂载形式。</p><p>heketi + glusterfs提供两种部署形式：</p><ul><li>容器化</li><li>传统服务</li></ul><p>以下我会先介绍以传统服务的部署形态对接kubernetes。</p><blockquote><ol><li>本文的原稿是我在19年初编写的，以下版本可能过于老旧，同学们进行搭建测试时建议下载较新的版本。</li><li>阅读本文前希望您能对glusterfs,kubernetes,storageclass,pv有所了解。</li></ol></blockquote><h1 id="环境描述"><a href="#环境描述" class="headerlink" title="环境描述"></a>环境描述</h1><table><thead><tr><th><strong>主机名</strong></th><th>ip地址</th><th><strong>系统环境</strong></th><th><strong>角色</strong></th></tr></thead><tbody><tr><td>k8s-1</td><td>IP:192.168.186.10</td><td>cpu:x2  mem:2GB  disk:/dev/sdb 10GB</td><td>K8s_master,Gluster_master,Heketi_master</td></tr><tr><td>K8s-2</td><td>IP:192.168.186.11</td><td>cpu:x2  mem:2GB  disk:/dev/sdb 10GB</td><td>K8s_node,Gluster_node</td></tr><tr><td>K8s-3</td><td>IP:192.168.186.12</td><td>cpu:x2  mem:2GB  disk:/dev/sdb 10GB</td><td>K8s_master,Gluster_node</td></tr></tbody></table><p>如果存在iptable限制，需执行以下命令开通以下port</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">iptables -N heketi</span><br><span class="line">iptables -A heketi -p tcp -m state --state NEW -m tcp --dport 24007 -j ACCEPT</span><br><span class="line">iptables -A heketi -p tcp -m state --state NEW -m tcp --dport 24008 -j ACCEPT</span><br><span class="line">iptables -A heketi -p tcp -m state --state NEW -m tcp --dport 2222 -j ACCEPT</span><br><span class="line">iptables -A heketi -p tcp -m state --state NEW -m multiport --dports 49152:49251 -j ACCEPT</span><br><span class="line">service iptables save</span><br></pre></td></tr></table></figure><h1 id="安装配置gfs"><a href="#安装配置gfs" class="headerlink" title="安装配置gfs"></a>安装配置gfs</h1><p>三台机器都要安装gfs软件并启动服务</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-1 ~]# yum -y install centos-release-gluster</span><br><span class="line">[root@k8s-1 ~]# yum -y install glusterfs-server</span><br><span class="line">[root@k8s-1 ~]# systemctl enable glusterd</span><br><span class="line">[root@k8s-1 ~]# systemctl start glusterd</span><br></pre></td></tr></table></figure><p>配置/etc/hosts,IP和主机名都一一对应</p><p><img src="/doc_picture/heketi-1.png" alt="image-20210714161502089"></p><p>安装glusterfs client客户端命令</p><p><img src="/doc_picture/heketi-2.png" alt="image-20210714161514389"></p><p>为存储池添加节点Node:（k8s-1操作，不用添加自己）</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-1 ~]# gluster peer probe k8s-2</span><br><span class="line">[root@k8s-1 ~]# gluster peer probe k8s-3</span><br></pre></td></tr></table></figure><h1 id="安装配置heketi"><a href="#安装配置heketi" class="headerlink" title="安装配置heketi"></a>安装配置heketi</h1><p>Heketi使用SSH来配置GlusterFS的所有节点。创建SSH密钥对:</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-1 ~]# mkdir /etc/heketi</span><br><span class="line">ssh-keygen -f /etc/heketi/heketi_key -t rsa -N ‘’</span><br><span class="line">[root@k8s-1 ~]# chown heketi:heketi /etc/heketi/heketi_key*</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>ssh公钥传递，这里只以一个节点为例：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-1 ~]# ssh-copy-id -i /etc/heketi/heketi_key.pub root@192.168.186.11</span><br></pre></td></tr></table></figure><p>制作完成后会在当前目录下生成heketi_key、heketi_key.pub，将公钥heketi_key.pub拷贝到所有glusterfs节点上/etc/heketi/keketi_key.pub（包括你登陆的第一个节点）</p><p>安装heketi（在k8s-1操作）：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-1 ~]#  yum install -y https://mirrors.aliyun.com/centos/7.6.1810/storage/x86_64/gluster-5/heketi-8.0.0-1.el7.x86_64.rpm</span><br><span class="line">[root@k8s-1 ~]#  yum install -y https://mirrors.aliyun.com/centos/7.6.1810/storage/x86_64/gluster-5/heketi-client-8.0.0-1.el7.x86_64.rpm</span><br></pre></td></tr></table></figure><p>创建存储db的文件夹：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-1 ~]#  mkdir /dcos/heketi</span><br><span class="line">[root@k8s-1 ~]#  chown -R heketi:heketi /dcos/heketi</span><br></pre></td></tr></table></figure><p>配置 heketi.json：</p><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">  <span class="attr">&quot;_port_comment&quot;</span>: <span class="string">&quot;Heketi Server Port Number&quot;</span>,</span><br><span class="line">  <span class="attr">&quot;port&quot;</span>: <span class="string">&quot;8088&quot;</span>,</span><br><span class="line"></span><br><span class="line">  <span class="attr">&quot;_use_auth&quot;</span>: <span class="string">&quot;Enable JWT authorization. Please enable for deployment&quot;</span>,</span><br><span class="line">  <span class="attr">&quot;use_auth&quot;</span>: <span class="literal">false</span>,</span><br><span class="line"></span><br><span class="line">  <span class="attr">&quot;_jwt&quot;</span>: <span class="string">&quot;Private keys for access&quot;</span>,</span><br><span class="line">  <span class="attr">&quot;jwt&quot;</span>: &#123;</span><br><span class="line">    <span class="attr">&quot;_admin&quot;</span>: <span class="string">&quot;Admin has access to all APIs&quot;</span>,</span><br><span class="line">    <span class="attr">&quot;admin&quot;</span>: &#123;</span><br><span class="line">      <span class="attr">&quot;key&quot;</span>: <span class="string">&quot;123456&quot;</span></span><br><span class="line">    &#125;,</span><br><span class="line">    <span class="attr">&quot;_user&quot;</span>: <span class="string">&quot;User only has access to /volumes endpoint&quot;</span>,</span><br><span class="line">    <span class="attr">&quot;user&quot;</span>: &#123;</span><br><span class="line">      <span class="attr">&quot;key&quot;</span>: <span class="string">&quot;123456&quot;</span></span><br><span class="line">    &#125;</span><br><span class="line">  &#125;,</span><br><span class="line"></span><br><span class="line">  <span class="attr">&quot;_glusterfs_comment&quot;</span>: <span class="string">&quot;GlusterFS Configuration&quot;</span>,</span><br><span class="line">  <span class="attr">&quot;glusterfs&quot;</span>: &#123;</span><br><span class="line">    <span class="attr">&quot;_executor_comment&quot;</span>: [</span><br><span class="line">      <span class="string">&quot;Execute plugin. Possible choices: mock, ssh&quot;</span>,</span><br><span class="line">      <span class="string">&quot;mock: This setting is used for testing and development.&quot;</span>,</span><br><span class="line">      <span class="string">&quot;      It will not send commands to any node.&quot;</span>,</span><br><span class="line">      <span class="string">&quot;ssh:  This setting will notify Heketi to ssh to the nodes.&quot;</span>,</span><br><span class="line">      <span class="string">&quot;      It will need the values in sshexec to be configured.&quot;</span>,</span><br><span class="line">      <span class="string">&quot;kubernetes: Communicate with GlusterFS containers over&quot;</span>,</span><br><span class="line">      <span class="string">&quot;            Kubernetes exec api.&quot;</span></span><br><span class="line">    ],</span><br><span class="line">    <span class="attr">&quot;executor&quot;</span>: <span class="string">&quot;ssh&quot;</span>,</span><br><span class="line"></span><br><span class="line">    <span class="attr">&quot;_sshexec_comment&quot;</span>: <span class="string">&quot;SSH username and private key file information&quot;</span>,</span><br><span class="line">    <span class="attr">&quot;sshexec&quot;</span>: &#123;</span><br><span class="line">      <span class="attr">&quot;keyfile&quot;</span>: <span class="string">&quot;/etc/heketi/heketi_key&quot;</span>,</span><br><span class="line">      <span class="attr">&quot;user&quot;</span>: <span class="string">&quot;root&quot;</span>,</span><br><span class="line">      <span class="attr">&quot;port&quot;</span>: <span class="string">&quot;22&quot;</span>,</span><br><span class="line">      <span class="attr">&quot;fstab&quot;</span>: <span class="string">&quot;/etc/fstab&quot;</span></span><br><span class="line">    &#125;,</span><br><span class="line"></span><br><span class="line">    <span class="attr">&quot;_kubeexec_comment&quot;</span>: <span class="string">&quot;Kubernetes configuration&quot;</span>,</span><br><span class="line">    <span class="attr">&quot;kubeexec&quot;</span>: &#123;</span><br><span class="line">      <span class="attr">&quot;host&quot;</span> :<span class="string">&quot;https://kubernetes.host:8443&quot;</span>,</span><br><span class="line">      <span class="attr">&quot;cert&quot;</span> : <span class="string">&quot;/path/to/crt.file&quot;</span>,</span><br><span class="line">      <span class="attr">&quot;insecure&quot;</span>: <span class="literal">false</span>,</span><br><span class="line">      <span class="attr">&quot;user&quot;</span>: <span class="string">&quot;kubernetes username&quot;</span>,</span><br><span class="line">      <span class="attr">&quot;password&quot;</span>: <span class="string">&quot;password for kubernetes user&quot;</span>,</span><br><span class="line">      <span class="attr">&quot;namespace&quot;</span>: <span class="string">&quot;OpenShift project or Kubernetes namespace&quot;</span>,</span><br><span class="line">      <span class="attr">&quot;fstab&quot;</span>: <span class="string">&quot;Optional: Specify fstab file on node.  Default is /etc/fstab&quot;</span></span><br><span class="line">    &#125;,</span><br><span class="line"></span><br><span class="line">    <span class="attr">&quot;_db_comment&quot;</span>: <span class="string">&quot;Database file name&quot;</span>,</span><br><span class="line">    <span class="attr">&quot;brick_min_size_gb&quot;</span>: <span class="number">1</span>,</span><br><span class="line">    <span class="attr">&quot;db&quot;</span>: <span class="string">&quot;/dcos/heketi/heketi.db&quot;</span>,</span><br><span class="line"></span><br><span class="line">    <span class="attr">&quot;_loglevel_comment&quot;</span>: [</span><br><span class="line">      <span class="string">&quot;Set log level. Choices are:&quot;</span>,</span><br><span class="line">      <span class="string">&quot;  none, critical, error, warning, info, debug&quot;</span>,</span><br><span class="line">      <span class="string">&quot;Default is warning&quot;</span></span><br><span class="line">    ],</span><br><span class="line">    <span class="attr">&quot;loglevel&quot;</span> : <span class="string">&quot;debug&quot;</span></span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><blockquote><p> 注：这里需要注意只是测试的话用mock 授权，standalone模式就 ssh 授权，k8s下就 kubernetes授权。</p></blockquote><p>重启heketi：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-1 ~]#  systemctl enable heketi</span><br><span class="line">[root@k8s-1 ~]#  systemctl restart heketi</span><br></pre></td></tr></table></figure><p>测试heketi是否好用：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-1 ~]#  curl http://localhost:8088/hello</span><br></pre></td></tr></table></figure><p>通过topology文件对接glusterfs集群：</p><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">  <span class="attr">&quot;clusters&quot;</span>: [</span><br><span class="line">    &#123;</span><br><span class="line">      <span class="attr">&quot;nodes&quot;</span>: [</span><br><span class="line">        &#123;</span><br><span class="line">          <span class="attr">&quot;node&quot;</span>: &#123;</span><br><span class="line">            <span class="attr">&quot;hostnames&quot;</span>: &#123;</span><br><span class="line">              <span class="attr">&quot;manage&quot;</span>: [</span><br><span class="line">                <span class="string">&quot;192.168.186.10&quot;</span></span><br><span class="line">              ],</span><br><span class="line">              <span class="attr">&quot;storage&quot;</span>: [</span><br><span class="line">                <span class="string">&quot;192.168.186.10&quot;</span></span><br><span class="line">              ]</span><br><span class="line">            &#125;,</span><br><span class="line">            <span class="attr">&quot;zone&quot;</span>: <span class="number">1</span></span><br><span class="line">          &#125;,</span><br><span class="line">          <span class="attr">&quot;devices&quot;</span>: [</span><br><span class="line">            <span class="string">&quot;/dev/sdb&quot;</span></span><br><span class="line">          ]</span><br><span class="line">        &#125;,</span><br><span class="line">        &#123;</span><br><span class="line">          <span class="attr">&quot;node&quot;</span>: &#123;</span><br><span class="line">            <span class="attr">&quot;hostnames&quot;</span>: &#123;</span><br><span class="line">              <span class="attr">&quot;manage&quot;</span>: [</span><br><span class="line">                <span class="string">&quot;192.168.186.11&quot;</span></span><br><span class="line">              ],</span><br><span class="line">              <span class="attr">&quot;storage&quot;</span>: [</span><br><span class="line">                <span class="string">&quot;192.168.186.11&quot;</span></span><br><span class="line">              ]</span><br><span class="line">            &#125;,</span><br><span class="line">            <span class="attr">&quot;zone&quot;</span>: <span class="number">1</span></span><br><span class="line">          &#125;,</span><br><span class="line">          <span class="attr">&quot;devices&quot;</span>: [</span><br><span class="line">            <span class="string">&quot;/dev/sdb&quot;</span></span><br><span class="line">          ]</span><br><span class="line">        &#125;,</span><br><span class="line">        &#123;</span><br><span class="line">          <span class="attr">&quot;node&quot;</span>: &#123;</span><br><span class="line">            <span class="attr">&quot;hostnames&quot;</span>: &#123;</span><br><span class="line">              <span class="attr">&quot;manage&quot;</span>: [</span><br><span class="line">                <span class="string">&quot;192.168.186.12&quot;</span></span><br><span class="line">              ],</span><br><span class="line">              <span class="attr">&quot;storage&quot;</span>: [</span><br><span class="line">                <span class="string">&quot;192.168.186.12&quot;</span></span><br><span class="line">              ]</span><br><span class="line">            &#125;,</span><br><span class="line">            <span class="attr">&quot;zone&quot;</span>: <span class="number">1</span></span><br><span class="line">          &#125;,</span><br><span class="line">          <span class="attr">&quot;devices&quot;</span>: [</span><br><span class="line">            <span class="string">&quot;/dev/sdb&quot;</span></span><br><span class="line">          ]</span><br><span class="line">        &#125;</span><br><span class="line">      ]</span><br><span class="line">    &#125;</span><br><span class="line">  ]</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>将该文件发送给heketi创建：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-1 ~]# heketi-cli --server http://192.168.186.10:8088 --user admin --secret 123456 topology load --json=/etc/heketi/topology.json</span><br></pre></td></tr></table></figure><p>创建成功后，heketi会在每个gluster节点上创建一个逻辑卷组，通过vgscan或vgdisplay可以看到：</p><p><img src="/doc_picture/heketi-3.png" alt="image-20210714163441286"></p><p>创建卷测试：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-1 ~]# heketi-cli --server http://192.168.186.10:8088 --user admin --secret 123456  volume create --size=1</span><br></pre></td></tr></table></figure><h1 id="配置kubernetes使用glusterfs"><a href="#配置kubernetes使用glusterfs" class="headerlink" title="配置kubernetes使用glusterfs"></a>配置kubernetes使用glusterfs</h1><h2 id="以密文的方式创建heketi-userkey的secret"><a href="#以密文的方式创建heketi-userkey的secret" class="headerlink" title="以密文的方式创建heketi userkey的secret"></a>以密文的方式创建heketi userkey的secret</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-1 k8s_gfs]# echo 123456|base64</span><br><span class="line">MTIzNDU2Cg==</span><br><span class="line">[root@k8s-1 k8s_gfs]# cat glusterfs-secret.yaml </span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: Secret</span><br><span class="line">metadata:</span><br><span class="line">  name: heketi-secret</span><br><span class="line">  namespace: default</span><br><span class="line">data:</span><br><span class="line"><span class="meta">  #</span><span class="bash"> base64 encoded password. E.g.: <span class="built_in">echo</span> -n <span class="string">&quot;mypassword&quot;</span> | base64</span></span><br><span class="line">  key: MTIzNDU2Cg==</span><br><span class="line">type: kubernetes.io/glusterfs</span><br></pre></td></tr></table></figure><h2 id="创建Storageclass"><a href="#创建Storageclass" class="headerlink" title="创建Storageclass"></a>创建Storageclass</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-1 k8s_gfs]# cat storageclass_glusterfs.yaml </span><br><span class="line">apiVersion: storage.k8s.io/v1beta1</span><br><span class="line">kind: StorageClass</span><br><span class="line">metadata:</span><br><span class="line">  name: glusterfs</span><br><span class="line">provisioner: kubernetes.io/glusterfs</span><br><span class="line">parameters:</span><br><span class="line">  resturl: &quot;http://192.168.186.10:8088&quot;</span><br><span class="line">  clusterid: &quot;a06343355a5f3e4240662d3963ec7d90&quot;</span><br><span class="line">  restauthenabled: &quot;true&quot;</span><br><span class="line">  secretNamespace: &quot;default&quot;</span><br><span class="line">  secretName: &quot;heketi-secret&quot;</span><br><span class="line">  restuser: &quot;admin&quot;</span><br><span class="line"><span class="meta">  #</span><span class="bash">restuserkey: <span class="string">&quot;123456&quot;</span></span></span><br><span class="line">  gidMin: &quot;40000&quot;</span><br><span class="line">  gidMax: &quot;50000&quot;</span><br><span class="line">  volumetype: &quot;replicate:3&quot;</span><br><span class="line">allowVolumeExpansion: true</span><br></pre></td></tr></table></figure><h2 id="创建PVC"><a href="#创建PVC" class="headerlink" title="创建PVC"></a>创建PVC</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-1 k8s_gfs]# cat pvc_glusterfs.yaml </span><br><span class="line">kind: PersistentVolumeClaim</span><br><span class="line">apiVersion: v1</span><br><span class="line">metadata:</span><br><span class="line">  name: glusterfs-pvc</span><br><span class="line">  namespace: default</span><br><span class="line">  annotations:</span><br><span class="line">    volume.beta.kubernetes.io/storage-class: &quot;glusterfs&quot;</span><br><span class="line">spec:</span><br><span class="line">  accessModes:</span><br><span class="line">  - ReadWriteMany</span><br><span class="line">  resources:</span><br><span class="line">    requests:</span><br><span class="line">      storage: 2Gi</span><br></pre></td></tr></table></figure><h2 id="创建pods测试"><a href="#创建pods测试" class="headerlink" title="创建pods测试"></a>创建pods测试</h2><figure class="highlight yml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line">[<span class="string">root@k8s-1</span> <span class="string">k8s_gfs</span>]<span class="comment"># cat nginx-pod.yaml </span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">extensions/v1beta1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Deployment</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">nginx-gfs</span></span><br><span class="line">  <span class="attr">labels:</span></span><br><span class="line">    <span class="attr">name:</span> <span class="string">nginx-gfs</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">replicas:</span> <span class="number">2</span></span><br><span class="line">  <span class="attr">selector:</span></span><br><span class="line">    <span class="attr">matchLabels:</span></span><br><span class="line">      <span class="attr">name:</span> <span class="string">nginx-gfs</span></span><br><span class="line">  <span class="attr">template:</span></span><br><span class="line">    <span class="attr">metadata:</span></span><br><span class="line">      <span class="attr">labels:</span></span><br><span class="line">        <span class="attr">name:</span> <span class="string">nginx-gfs</span></span><br><span class="line">    <span class="attr">spec:</span></span><br><span class="line">      <span class="attr">containers:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">nginx-gfs</span></span><br><span class="line">        <span class="attr">image:</span> <span class="string">nginx</span></span><br><span class="line">        <span class="attr">ports:</span></span><br><span class="line">        <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">web</span></span><br><span class="line">          <span class="attr">containerPort:</span> <span class="number">80</span></span><br><span class="line">        <span class="attr">volumeMounts:</span></span><br><span class="line">        <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">gfs</span></span><br><span class="line">          <span class="attr">mountPath:</span> <span class="string">/usr/share/nginx/html</span></span><br><span class="line">      <span class="attr">volumes:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">gfs</span></span><br><span class="line">        <span class="attr">persistentVolumeClaim:</span></span><br><span class="line">          <span class="attr">claimName:</span> <span class="string">glusterfs-pvc</span></span><br></pre></td></tr></table></figure><h1 id="参考文档"><a href="#参考文档" class="headerlink" title="参考文档"></a>参考文档</h1><p><a href="https://kubernetes.io/zh/docs/concepts/storage/volumes/">https://kubernetes.io/zh/docs/concepts/storage/volumes/</a></p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;GlusterFS（gfs）由此名字也可看出是文件系统存储相关的软件，它是一个开源的分布式文件系统，具有强大的横向扩展能力。Heketi是一个GlusterFs管理软件，可以管理glusterFS集群的卷创建、删除等操作。Glusterfs作为kubernetes支持的多种</summary>
      
    
    
    
    
    <category term="heketi" scheme="https://slions.github.io/tags/heketi/"/>
    
    <category term="glusterfs" scheme="https://slions.github.io/tags/glusterfs/"/>
    
  </entry>
  
  <entry>
    <title>自己动手实现network namespace</title>
    <link href="https://slions.github.io/2021/07/13/network-namespace%E9%9A%94%E7%A6%BB%E6%B5%8B%E8%AF%95/"/>
    <id>https://slions.github.io/2021/07/13/network-namespace%E9%9A%94%E7%A6%BB%E6%B5%8B%E8%AF%95/</id>
    <published>2021-07-13T15:20:49.000Z</published>
    <updated>2021-07-13T18:41:22.918Z</updated>
    
    <content type="html"><![CDATA[<a href="/2021/07/09/%E8%81%8A%E8%81%8Anamespace/" title="之前的文章">之前的文章</a>提到了6种Linux的namespace隔离技术，其中network namespace为命名空间内的所有进程提供了全新隔离的网络协议栈。这包括网络接口，路由表和**iptables**规则。通过使用网络命名空间就可以实现网络虚拟环境，实现彼此之间的网络隔离，其实我们通过ip命令就可以模拟出网络命名空间。<h1 id="安装软件"><a href="#安装软件" class="headerlink" title="安装软件"></a>安装软件</h1><p>首先我们需要安装iprouter软件：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@slions_pc1 ~]# rpm -q iproute</span><br><span class="line">iproute-4.11.0-14.el7.x86_64</span><br></pre></td></tr></table></figure><p>我本地已经安装了，查看IP命令：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">[root@slions_pc1 ~]# ip help</span><br><span class="line">Usage: ip [ OPTIONS ] OBJECT &#123; COMMAND | help &#125;</span><br><span class="line">       ip [ -force ] -batch filename</span><br><span class="line">where  OBJECT := &#123; link | address | addrlabel | route | rule | neigh | ntable |</span><br><span class="line">                   tunnel | tuntap | maddress | mroute | mrule | monitor | xfrm |</span><br><span class="line">                   netns | l2tp | fou | macsec | tcp_metrics | token | netconf | ila |</span><br><span class="line">                   vrf &#125;</span><br><span class="line">       OPTIONS := &#123; -V[ersion] | -s[tatistics] | -d[etails] | -r[esolve] |</span><br><span class="line">                    -h[uman-readable] | -iec |</span><br><span class="line">                    -f[amily] &#123; inet | inet6 | ipx | dnet | mpls | bridge | link &#125; |</span><br><span class="line">                    -4 | -6 | -I | -D | -B | -0 |</span><br><span class="line">                    -l[oops] &#123; maximum-addr-flush-attempts &#125; | -br[ief] |</span><br><span class="line">                    -o[neline] | -t[imestamp] | -ts[hort] | -b[atch] [filename] |</span><br><span class="line">                    -rc[vbuf] [size] | -n[etns] name | -a[ll] | -c[olor]&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure><h1 id="创建netns"><a href="#创建netns" class="headerlink" title="创建netns"></a>创建netns</h1><p>查看本地是否存在netns</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@slions_pc1 ~]# ip netns list</span><br><span class="line">[root@slions_pc1 ~]#</span><br></pre></td></tr></table></figure><p>创建两个netns，分别叫slions_ns1,slions_ns2</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[root@slions_pc1 ~]# ip netns add slions_ns1</span><br><span class="line">[root@slions_pc1 ~]# ip netns add slions_ns2</span><br><span class="line">[root@slions_pc1 ~]# ip netns list</span><br><span class="line">slions_ns1</span><br><span class="line">slions_ns2</span><br></pre></td></tr></table></figure><p>我们查看下这个命名空间下的网卡信息,可以看到都只有lo环回网卡</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">[root@slions_pc1 ~]# ip netns exec slions_ns1 ifconfig -a</span><br><span class="line">lo: flags=8&lt;LOOPBACK&gt;  mtu 65536</span><br><span class="line">        loop  txqueuelen 1000  (Local Loopback)</span><br><span class="line">        RX packets 0  bytes 0 (0.0 B)</span><br><span class="line">        RX errors 0  dropped 0  overruns 0  frame 0</span><br><span class="line">        TX packets 0  bytes 0 (0.0 B)</span><br><span class="line">        TX errors 0  dropped 0 overruns 0  carrier 0  collisions 0</span><br><span class="line">[root@slions_pc1 ~]# ip netns exec slions_ns2 ifconfig -a</span><br><span class="line">lo: flags=8&lt;LOOPBACK&gt;  mtu 65536</span><br><span class="line">        loop  txqueuelen 1000  (Local Loopback)</span><br><span class="line">        RX packets 0  bytes 0 (0.0 B)</span><br><span class="line">        RX errors 0  dropped 0  overruns 0  frame 0</span><br><span class="line">        TX packets 0  bytes 0 (0.0 B)</span><br><span class="line">        TX errors 0  dropped 0 overruns 0  carrier 0  collisions 0</span><br><span class="line"></span><br></pre></td></tr></table></figure><h1 id="创建veth网卡设备对"><a href="#创建veth网卡设备对" class="headerlink" title="创建veth网卡设备对"></a>创建veth网卡设备对</h1><p>我们需要创建一对儿veth虚拟网卡</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">[root@slions_pc1 ~]# ip link add veth0 type veth peer name veth1</span><br><span class="line">[root@slions_pc1 ~]# ip link show</span><br><span class="line">1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN mode DEFAULT group default qlen 1000</span><br><span class="line">    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00</span><br><span class="line">2: ens33: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc pfifo_fast state UP mode DEFAULT group default qlen 1000</span><br><span class="line">    link/ether 00:0c:29:97:8c:d0 brd ff:ff:ff:ff:ff:ff</span><br><span class="line">3: docker0: &lt;NO-CARRIER,BROADCAST,MULTICAST,UP&gt; mtu 1500 qdisc noqueue state DOWN mode DEFAULT group default</span><br><span class="line">    link/ether 02:42:8a:8f:d7:d5 brd ff:ff:ff:ff:ff:ff</span><br><span class="line">8: veth1@veth0: &lt;BROADCAST,MULTICAST,M-DOWN&gt; mtu 1500 qdisc noop state DOWN mode DEFAULT group default qlen 1000</span><br><span class="line">    link/ether ae:f9:75:fa:93:4d brd ff:ff:ff:ff:ff:ff</span><br><span class="line">9: veth0@veth1: &lt;BROADCAST,MULTICAST,M-DOWN&gt; mtu 1500 qdisc noop state DOWN mode DEFAULT group default qlen 1000</span><br><span class="line">    link/ether d2:24:de:9b:b5:d9 brd ff:ff:ff:ff:ff:ff</span><br></pre></td></tr></table></figure><p>可以看到，此时本地多了8和9两块没有被激活的网卡，并且从网卡名上可以看出对应关系</p><h1 id="移动veth网卡到netns"><a href="#移动veth网卡到netns" class="headerlink" title="移动veth网卡到netns"></a>移动veth网卡到netns</h1><p>把其中一个网卡移动到命名空间slions_ns1中</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@slions_pc1 ~]# ip link set veth1 netns slions_ns1</span><br></pre></td></tr></table></figure><p>在slions_ns1中验证网卡信息</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[root@slions_pc1 ~]# ip netns exec slions_ns1 ip a</span><br><span class="line">1: lo: &lt;LOOPBACK&gt; mtu 65536 qdisc noop state DOWN group default qlen 1000</span><br><span class="line">    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00</span><br><span class="line">8: veth1@if9: &lt;BROADCAST,MULTICAST&gt; mtu 1500 qdisc noop state DOWN group default qlen 1000</span><br><span class="line">    link/ether ae:f9:75:fa:93:4d brd ff:ff:ff:ff:ff:ff link-netnsid 0</span><br></pre></td></tr></table></figure><p>我们再来看下宿主机的网卡信息，发现之前的8号没了，仔细观察会发现slions_ns中新的网卡正是之前的8号网卡，从序号也可以看出来，并且网卡名中的@ifx就是veth设备对端的网卡序号</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">[root@slions_pc1 ~]# ip link show</span><br><span class="line">1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN mode DEFAULT group default qlen 1000</span><br><span class="line">    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00</span><br><span class="line">2: ens33: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc pfifo_fast state UP mode DEFAULT group default qlen 1000</span><br><span class="line">    link/ether 00:0c:29:97:8c:d0 brd ff:ff:ff:ff:ff:ff</span><br><span class="line">3: docker0: &lt;NO-CARRIER,BROADCAST,MULTICAST,UP&gt; mtu 1500 qdisc noqueue state DOWN mode DEFAULT group default</span><br><span class="line">    link/ether 02:42:8a:8f:d7:d5 brd ff:ff:ff:ff:ff:ff</span><br><span class="line">9: veth0@if8: &lt;BROADCAST,MULTICAST&gt; mtu 1500 qdisc noop state DOWN mode DEFAULT group default qlen 1000</span><br><span class="line">    link/ether d2:24:de:9b:b5:d9 brd ff:ff:ff:ff:ff:ff link-netnsid 0</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>我们还可以给slions_ns1中的veth1改名</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">[root@slions_pc1 ~]# ip netns exec slions_ns1 ip link set dev veth1 name eth0</span><br><span class="line">[root@slions_pc1 ~]# ip netns exec slions_ns1 ifconfig -a</span><br><span class="line">eth0: flags=4098&lt;BROADCAST,MULTICAST&gt;  mtu 1500</span><br><span class="line">        ether ae:f9:75:fa:93:4d  txqueuelen 1000  (Ethernet)</span><br><span class="line">        RX packets 0  bytes 0 (0.0 B)</span><br><span class="line">        RX errors 0  dropped 0  overruns 0  frame 0</span><br><span class="line">        TX packets 0  bytes 0 (0.0 B)</span><br><span class="line">        TX errors 0  dropped 0 overruns 0  carrier 0  collisions 0</span><br><span class="line"></span><br><span class="line">lo: flags=8&lt;LOOPBACK&gt;  mtu 65536</span><br><span class="line">        loop  txqueuelen 1000  (Local Loopback)</span><br><span class="line">        RX packets 0  bytes 0 (0.0 B)</span><br><span class="line">        RX errors 0  dropped 0  overruns 0  frame 0</span><br><span class="line">        TX packets 0  bytes 0 (0.0 B)</span><br><span class="line">        TX errors 0  dropped 0 overruns 0  carrier 0  collisions 0</span><br></pre></td></tr></table></figure><h1 id="激活veth网卡"><a href="#激活veth网卡" class="headerlink" title="激活veth网卡"></a>激活veth网卡</h1><p>给宿主机veth网卡设置ip地址为10.0.0.1，激活网卡</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">[root@slions_pc1 ~]# ip addr add 10.0.0.1/24 dev veth0</span><br><span class="line">[root@slions_pc1 ~]# ip link set veth0 up</span><br><span class="line">[root@slions_pc1 ~]# ip a</span><br><span class="line">1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000</span><br><span class="line">    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00</span><br><span class="line">    inet 127.0.0.1/8 scope host lo</span><br><span class="line">       valid_lft forever preferred_lft forever</span><br><span class="line">    inet6 ::1/128 scope host</span><br><span class="line">       valid_lft forever preferred_lft forever</span><br><span class="line">2: ens33: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc pfifo_fast state UP group default qlen 1000</span><br><span class="line">    link/ether 00:0c:29:97:8c:d0 brd ff:ff:ff:ff:ff:ff</span><br><span class="line">    inet 192.168.100.10/24 brd 192.168.100.255 scope global noprefixroute ens33</span><br><span class="line">       valid_lft forever preferred_lft forever</span><br><span class="line">    inet6 fe80::8199:4454:db1e:911c/64 scope link tentative noprefixroute dadfailed</span><br><span class="line">       valid_lft forever preferred_lft forever</span><br><span class="line">    inet6 fe80::22c9:e9bb:e5a1:9403/64 scope link noprefixroute</span><br><span class="line">       valid_lft forever preferred_lft forever</span><br><span class="line">3: docker0: &lt;NO-CARRIER,BROADCAST,MULTICAST,UP&gt; mtu 1500 qdisc noqueue state DOWN group default</span><br><span class="line">    link/ether 02:42:8a:8f:d7:d5 brd ff:ff:ff:ff:ff:ff</span><br><span class="line">    inet 172.17.0.1/16 brd 172.17.255.255 scope global docker0</span><br><span class="line">       valid_lft forever preferred_lft forever</span><br><span class="line">    inet6 fe80::42:8aff:fe8f:d7d5/64 scope link</span><br><span class="line">       valid_lft forever preferred_lft forever</span><br><span class="line">9: veth0@if8: &lt;NO-CARRIER,BROADCAST,MULTICAST,UP&gt; mtu 1500 qdisc noqueue state LOWERLAYERDOWN group default qlen 1000</span><br><span class="line">    link/ether d2:24:de:9b:b5:d9 brd ff:ff:ff:ff:ff:ff link-netnsid 0</span><br><span class="line">    inet 10.0.0.1/24 scope global veth0</span><br><span class="line">       valid_lft forever preferred_lft forever</span><br></pre></td></tr></table></figure><p>给slions_ns1命名空间的veth网卡设置ip地址为10.0.0.2，激活网卡</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">[root@slions_pc1 ~]# ip netns exec slions_ns1 ifconfig eth0 10.0.0.2/24 up</span><br><span class="line">[root@slions_pc1 ~]# ip netns exec slions_ns1 ifconfig -a</span><br><span class="line">eth0: flags=4163&lt;UP,BROADCAST,RUNNING,MULTICAST&gt;  mtu 1500</span><br><span class="line">        inet 10.0.0.2  netmask 255.255.255.0  broadcast 10.0.0.255</span><br><span class="line">        inet6 fe80::acf9:75ff:fefa:934d  prefixlen 64  scopeid 0x20&lt;link&gt;</span><br><span class="line">        ether ae:f9:75:fa:93:4d  txqueuelen 1000  (Ethernet)</span><br><span class="line">        RX packets 8  bytes 656 (656.0 B)</span><br><span class="line">        RX errors 0  dropped 0  overruns 0  frame 0</span><br><span class="line">        TX packets 8  bytes 656 (656.0 B)</span><br><span class="line">        TX errors 0  dropped 0 overruns 0  carrier 0  collisions 0</span><br><span class="line"></span><br><span class="line">lo: flags=8&lt;LOOPBACK&gt;  mtu 65536</span><br><span class="line">        loop  txqueuelen 1000  (Local Loopback)</span><br><span class="line">        RX packets 0  bytes 0 (0.0 B)</span><br><span class="line">        RX errors 0  dropped 0  overruns 0  frame 0</span><br><span class="line">        TX packets 0  bytes 0 (0.0 B)</span><br><span class="line">        TX errors 0  dropped 0 overruns 0  carrier 0  collisions 0</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>在宿主机ping slions_ns1的网卡地址</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">[root@slions_pc1 ~]# ping 10.0.0.2 -c 3</span><br><span class="line">PING 10.0.0.2 (10.0.0.2) 56(84) bytes of data.</span><br><span class="line">64 bytes from 10.0.0.2: icmp_seq=1 ttl=64 time=0.013 ms</span><br><span class="line">64 bytes from 10.0.0.2: icmp_seq=2 ttl=64 time=0.023 ms</span><br><span class="line">64 bytes from 10.0.0.2: icmp_seq=3 ttl=64 time=0.024 ms</span><br><span class="line"></span><br><span class="line">--- 10.0.0.2 ping statistics ---</span><br><span class="line">3 packets transmitted, 3 received, 0% packet loss, time 1999ms</span><br><span class="line">rtt min/avg/max/mdev = 0.013/0.020/0.024/0.005 ms</span><br></pre></td></tr></table></figure><p>可以ping通，此时证明宿主机和slions_ns1网络命名空间已经通过veth网卡实现了网络联通</p><h1 id="测试两个netns联通性"><a href="#测试两个netns联通性" class="headerlink" title="测试两个netns联通性"></a>测试两个netns联通性</h1><p>还记不记得最开始我们还创建了一个slions_ns2的网络命名空间，我们把宿主机的那块veth0网卡加到其里面去进行测试</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">[root@slions_pc1 ~]# ip link set dev veth0 netns slions_ns2</span><br><span class="line">[root@slions_pc1 ~]# ip netns exec slions_ns2 ifconfig -a</span><br><span class="line">lo: flags=8&lt;LOOPBACK&gt;  mtu 65536</span><br><span class="line">        loop  txqueuelen 1000  (Local Loopback)</span><br><span class="line">        RX packets 0  bytes 0 (0.0 B)</span><br><span class="line">        RX errors 0  dropped 0  overruns 0  frame 0</span><br><span class="line">        TX packets 0  bytes 0 (0.0 B)</span><br><span class="line">        TX errors 0  dropped 0 overruns 0  carrier 0  collisions 0</span><br><span class="line"></span><br><span class="line">veth0: flags=4098&lt;BROADCAST,MULTICAST&gt;  mtu 1500</span><br><span class="line">        ether d2:24:de:9b:b5:d9  txqueuelen 1000  (Ethernet)</span><br><span class="line">        RX packets 24  bytes 2000 (1.9 KiB)</span><br><span class="line">        RX errors 0  dropped 0  overruns 0  frame 0</span><br><span class="line">        TX packets 24  bytes 2000 (1.9 KiB)</span><br><span class="line">        TX errors 0  dropped 0 overruns 0  carrier 0  collisions 0</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>发现网卡没被激活，手动再设置下</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">[root@slions_pc1 ~]# ip netns exec slions_ns2 ifconfig veth0 10.0.0.3/24 up</span><br><span class="line">[root@slions_pc1 ~]# ip netns exec slions_ns2 ifconfig -a</span><br><span class="line">lo: flags=8&lt;LOOPBACK&gt;  mtu 65536</span><br><span class="line">        loop  txqueuelen 1000  (Local Loopback)</span><br><span class="line">        RX packets 0  bytes 0 (0.0 B)</span><br><span class="line">        RX errors 0  dropped 0  overruns 0  frame 0</span><br><span class="line">        TX packets 0  bytes 0 (0.0 B)</span><br><span class="line">        TX errors 0  dropped 0 overruns 0  carrier 0  collisions 0</span><br><span class="line"></span><br><span class="line">veth0: flags=4163&lt;UP,BROADCAST,RUNNING,MULTICAST&gt;  mtu 1500</span><br><span class="line">        inet 10.0.0.3  netmask 255.255.255.0  broadcast 10.0.0.255</span><br><span class="line">        inet6 fe80::d024:deff:fe9b:b5d9  prefixlen 64  scopeid 0x20&lt;link&gt;</span><br><span class="line">        ether d2:24:de:9b:b5:d9  txqueuelen 1000  (Ethernet)</span><br><span class="line">        RX packets 24  bytes 2000 (1.9 KiB)</span><br><span class="line">        RX errors 0  dropped 0  overruns 0  frame 0</span><br><span class="line">        TX packets 30  bytes 2516 (2.4 KiB)</span><br><span class="line">        TX errors 0  dropped 0 overruns 0  carrier 0  collisions 0</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>这时从slions_ns2中测试能否ping通slions_ns1</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">[root@slions_pc1 ~]# ip netns exec slions_ns2 ping 10.0.0.2 -c 3</span><br><span class="line">PING 10.0.0.2 (10.0.0.2) 56(84) bytes of data.</span><br><span class="line">64 bytes from 10.0.0.2: icmp_seq=1 ttl=64 time=0.014 ms</span><br><span class="line">64 bytes from 10.0.0.2: icmp_seq=2 ttl=64 time=0.023 ms</span><br><span class="line">64 bytes from 10.0.0.2: icmp_seq=3 ttl=64 time=0.023 ms</span><br><span class="line"></span><br><span class="line">--- 10.0.0.2 ping statistics ---</span><br><span class="line">3 packets transmitted, 3 received, 0% packet loss, time 1999ms</span><br><span class="line">rtt min/avg/max/mdev = 0.014/0.020/0.023/0.004 ms</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>和料想的一样可以ping通。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;a href=&quot;/2021/07/09/%E8%81%8A%E8%81%8Anamespace/&quot; title=&quot;之前的文章&quot;&gt;之前的文章&lt;/a&gt;提到了6种Linux的namespace隔离技术，其中network namespace为命名空间内的所有进程提供了全新隔离的网络协</summary>
      
    
    
    
    
    <category term="docker" scheme="https://slions.github.io/tags/docker/"/>
    
  </entry>
  
  <entry>
    <title>cadvisor+node_exporter+prometheus+grafana实现docker与主机监控</title>
    <link href="https://slions.github.io/2021/07/12/cadvisor-node-exporter-prometheus-grafana%E5%AE%9E%E7%8E%B0docker%E4%B8%8E%E4%B8%BB%E6%9C%BA%E7%9B%91%E6%8E%A7/"/>
    <id>https://slions.github.io/2021/07/12/cadvisor-node-exporter-prometheus-grafana%E5%AE%9E%E7%8E%B0docker%E4%B8%8E%E4%B8%BB%E6%9C%BA%E7%9B%91%E6%8E%A7/</id>
    <published>2021-07-12T15:49:40.000Z</published>
    <updated>2021-07-13T03:51:30.328Z</updated>
    
    <content type="html"><![CDATA[<p>网上一大堆关于docker监控的工具和案例，这章主要说下如何通过cadvisor+node_exporter+prometheus完成对docker及主机的监控，并且通过grafana来完成监控数据的展示。</p><p>简述下今天用到的几个组件，其中node_exporter是用来监控主机信息的，cadvisor是用来监控容器信息的，这俩组件对于prometheus而言都是采集器的作用，prometheus会收集这些采集器的监控数据，存储到内置的数据库中，后续可以通过Prometheus表达式来进行过滤查看等操作（这篇不作为重点讲），最后Grafana 会对接Prometheus，将监控数据通过Web UI的方式展示出来。</p><h1 id="环境准备"><a href="#环境准备" class="headerlink" title="环境准备"></a>环境准备</h1><p>先说下我本地的环境：</p><table><thead><tr><th align="left">主机名</th><th align="left">IP地址</th><th align="left">已运行docker容器</th></tr></thead><tbody><tr><td align="left">slions_pc1</td><td align="left">192.168.100.10</td><td align="left">slions_nginx1;slions_busybox1</td></tr><tr><td align="left">slions_pc2</td><td align="left">192.168.100.11</td><td align="left">slions_nginx2;slions_busybox2</td></tr></tbody></table><p>准备了2台机器，并且上面已经启动了一些容器。</p><h1 id="安装node-exporter"><a href="#安装node-exporter" class="headerlink" title="安装node_exporter"></a>安装node_exporter</h1><p>2台机器都执行以下命令（–name处根据自己喜好改）：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">[root@slions_pc1 ~]# docker run -d \</span><br><span class="line"><span class="meta">&gt;</span><span class="bash">   --net=<span class="string">&quot;host&quot;</span> \</span></span><br><span class="line"><span class="bash">&gt;   --pid=<span class="string">&quot;host&quot;</span> \</span></span><br><span class="line"><span class="bash">&gt;   -v <span class="string">&quot;/:/host:ro,rslave&quot;</span> \</span></span><br><span class="line"><span class="bash">&gt;   --name slions_node-exporter1 \</span></span><br><span class="line"><span class="bash">&gt;   quay.io/prometheus/node-exporter:latest \</span></span><br><span class="line"><span class="bash">&gt;   --path.rootfs=/host</span></span><br></pre></td></tr></table></figure><p>安装好后可以检查下node_exporter默认的监听端口是否存在，如下说明正常</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@slions_pc1 ~]# ss -lntp|grep 9100</span><br><span class="line">LISTEN     0      128         :::9100                    :::*                   users:((&quot;node_exporter&quot;,pid=12094,fd=3))</span><br></pre></td></tr></table></figure><p>可以使用chrome访问下，出现如下则说明成功，如果你使用浏览器刷不出来数据请先确保防火墙是否关闭。</p><p><img src="/doc_picture/jk-1.png" alt="image-20210713101912265"></p><h1 id="安装cadvisor"><a href="#安装cadvisor" class="headerlink" title="安装cadvisor"></a>安装cadvisor</h1><p>2台机器都执行以下命令（–name处根据自己喜好改）：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">[root@slions_pc1 ~]# docker run \</span><br><span class="line"><span class="meta">&gt;</span><span class="bash">   --volume=/:/rootfs:ro \</span></span><br><span class="line"><span class="bash">&gt;   --volume=/var/run:/var/run:ro \</span></span><br><span class="line"><span class="bash">&gt;   --volume=/sys:/sys:ro \</span></span><br><span class="line"><span class="bash">&gt;   --volume=/var/lib/docker/:/var/lib/docker:ro \</span></span><br><span class="line"><span class="bash">&gt;   --publish=8080:8080 \</span></span><br><span class="line"><span class="bash">&gt;   --detach=<span class="literal">true</span> \</span></span><br><span class="line"><span class="bash">&gt;   --name=slions_cadvisor1 \</span></span><br><span class="line"><span class="bash">&gt;   --privileged \</span></span><br><span class="line"><span class="bash">&gt;   --network host \</span></span><br><span class="line"><span class="bash">&gt;   google/cadvisor</span></span><br></pre></td></tr></table></figure><p>安装好后可以检查下node_exporter默认的监听端口是否存在，如下说明正常</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@slions_pc1 ~]# ss -lntp|grep 8080</span><br><span class="line">LISTEN     0      128         :::8080                    :::*                   users:((&quot;cadvisor&quot;,pid=12344,fd=9))</span><br></pre></td></tr></table></figure><p>同样我们可以使用chrome访问下主机的8080，如图出现了cadvisor的监控数据。</p><p><img src="/doc_picture/jk-2.png" alt="image-20210713104008822"></p><h1 id="安装prometheus"><a href="#安装prometheus" class="headerlink" title="安装prometheus"></a>安装prometheus</h1><p>选择第一台机器作为prometheus服务端，prometheus服务需要通过配置文件来读取监控目标，我们先在本地编写配置文件，后续通过挂载的方式置入服务中：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line">[<span class="string">root@slions_pc1</span> <span class="string">~</span>]<span class="comment"># cat prometheus/prometheus.yml</span></span><br><span class="line"><span class="attr">global:</span></span><br><span class="line">  <span class="attr">scrape_interval:</span>     <span class="string">15s</span> <span class="comment"># By default, scrape targets every 15 seconds.</span></span><br><span class="line">  <span class="attr">evaluation_interval:</span> <span class="string">15s</span> <span class="comment"># Evaluate rules every 15 seconds.</span></span><br><span class="line"></span><br><span class="line">  <span class="comment"># Attach these extra labels to all timeseries collected by this Prometheus instance.</span></span><br><span class="line">  <span class="attr">external_labels:</span></span><br><span class="line">    <span class="attr">monitor:</span> <span class="string">&#x27;slions-monitor&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="attr">rule_files:</span></span><br><span class="line"><span class="comment">#  - &#x27;prometheus.rules.yml&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="attr">scrape_configs:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">job_name:</span> <span class="string">&#x27;prometheus&#x27;</span></span><br><span class="line">    <span class="attr">scrape_interval:</span> <span class="string">5s</span></span><br><span class="line">    <span class="attr">static_configs:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">targets:</span> [<span class="string">&#x27;localhost:9090&#x27;</span>]</span><br><span class="line"></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">job_name:</span> <span class="string">&#x27;node&#x27;</span></span><br><span class="line">    <span class="attr">scrape_interval:</span> <span class="string">5s</span></span><br><span class="line">    <span class="attr">static_configs:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">targets:</span></span><br><span class="line">        <span class="bullet">-</span> <span class="number">192.168</span><span class="number">.100</span><span class="number">.10</span><span class="string">:9100</span></span><br><span class="line">        <span class="bullet">-</span> <span class="number">192.168</span><span class="number">.100</span><span class="number">.11</span><span class="string">:9100</span></span><br><span class="line"></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">job_name:</span> <span class="string">&#x27;docker&#x27;</span></span><br><span class="line">    <span class="attr">scrape_interval:</span> <span class="string">5s</span></span><br><span class="line">    <span class="attr">static_configs:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">targets:</span></span><br><span class="line">        <span class="bullet">-</span> <span class="number">192.168</span><span class="number">.100</span><span class="number">.10</span><span class="string">:8080</span></span><br><span class="line">        <span class="bullet">-</span> <span class="number">192.168</span><span class="number">.100</span><span class="number">.11</span><span class="string">:8080</span></span><br><span class="line"></span><br></pre></td></tr></table></figure><p>启动Prometheus服务：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@slions_pc1 ~]# docker run -d -p 9090:9090 --volume /root/prometheus/prometheus.yml:/etc/prometheus/prometheus.yml --name slions_prometheus --network host prom/prometheus</span><br></pre></td></tr></table></figure><p>验证服务是否正常启动：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@slions_pc1 ~]# ss -lntp|grep 9090</span><br><span class="line">LISTEN     0      128         :::9090                    :::*                   users:((&quot;prometheus&quot;,pid=13113,fd=7))</span><br></pre></td></tr></table></figure><p>chrome访问<code>http://192.168.100.10:9090</code>可以看到已经出现了prometheus的页面</p><p><img src="/doc_picture/jk-3.png" alt="image-20210713110434391"></p><p>点击<code>Status</code>中的<code>Targets</code>按钮，会发现正是我们配置的那几个监控目标</p><p><img src="/doc_picture/jk-4.png" alt="image-20210713110811438"></p><h1 id="安装grafana"><a href="#安装grafana" class="headerlink" title="安装grafana"></a>安装grafana</h1><p>在第一台机器部署grafana，启动时设置登录密码</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@slions_pc1 ~]# docker run -itd --name slions_grafana -p 3000:3000 -e &quot;GF_SERVER_ROOT_URL=http://grafana.server.name&quot; -e &quot;GF_SECURITY_ADMIN_PASSWORD=slions&quot; --network host grafana/grafana</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>查看服务是否正常：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@slions_pc1 ~]# ss -lntp|grep 3000</span><br><span class="line">LISTEN     0      128         :::3000                    :::*                   users:((&quot;grafana-server&quot;,pid=13581,fd=10))</span><br></pre></td></tr></table></figure><p>使用浏览器查看，用户名为admin,密码是我启动服务时设置的slions</p><p><img src="/doc_picture/jk-5.png" alt="image-20210713111849209"></p><p>选择<code>DATA SOURCES</code>来添加数据源:</p><p><img src="/doc_picture/jk-6.png" alt="image-20210713112133355"></p><p>选择Prometheus后，将URL填写上我们自己的服务地址就可以保存了：</p><p><img src="/doc_picture/jk-7.png" alt="image-20210713112328341"></p><p>我们可以直接找一个合适的模板来导入</p><p><img src="/doc_picture/jk-8.png" alt="image-20210713112714427"></p><p>模板在grafana官网找就好：</p><p><img src="/doc_picture/jk-9.png" alt="image-20210713112947279"></p><p>比如我找到一个看起来不错的模板，点击Download JSON下载到本地</p><p><img src="/doc_picture/jk-10.png" alt="image-20210713113715380"></p><p>然后在import页面导入此JSON文件就好了，选择生效于我们刚创建好的slions_Prometheus:</p><p><img src="/doc_picture/jk-11.png" alt="image-20210713113812164"></p><p>最后让我们看一下监控的效果吧。</p><p><img src="/doc_picture/jk-12.png" alt="image-20210713114101676"></p><h1 id="参考文档"><a href="#参考文档" class="headerlink" title="参考文档"></a>参考文档</h1><p>cadvisor: <a href="https://github.com/google/cadvisor">https://github.com/google/cadvisor</a></p><p>node_exporter:  <a href="https://github.com/prometheus/node_exporter">https://github.com/prometheus/node_exporter</a></p><p>prometheus: <a href="https://github.com/prometheus/prometheus">https://github.com/prometheus/prometheus</a>  、<a href="https://prometheus.io/">https://prometheus.io/</a></p><p>grafana:  <a href="https://grafana.com/docs/grafana/latest/installation/docker/">https://grafana.com/docs/grafana/latest/installation/docker/</a></p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;网上一大堆关于docker监控的工具和案例，这章主要说下如何通过cadvisor+node_exporter+prometheus完成对docker及主机的监控，并且通过grafana来完成监控数据的展示。&lt;/p&gt;
&lt;p&gt;简述下今天用到的几个组件，其中node_export</summary>
      
    
    
    
    
    <category term="docker" scheme="https://slions.github.io/tags/docker/"/>
    
    <category term="prometheus" scheme="https://slions.github.io/tags/prometheus/"/>
    
  </entry>
  
  <entry>
    <title>理解存储驱动overlay2</title>
    <link href="https://slions.github.io/2021/07/12/%E7%90%86%E8%A7%A3%E5%AD%98%E5%82%A8%E9%A9%B1%E5%8A%A8overlay2/"/>
    <id>https://slions.github.io/2021/07/12/%E7%90%86%E8%A7%A3%E5%AD%98%E5%82%A8%E9%A9%B1%E5%8A%A8overlay2/</id>
    <published>2021-07-12T04:00:14.000Z</published>
    <updated>2021-07-12T10:20:42.908Z</updated>
    
    <content type="html"><![CDATA[<p>容器中通过使用不同的rootfs来模拟了不同的操作系统及应用，不过，如果使用每个镜像都需要一个独立的根文件系统的话，那想必磁盘早已拥挤不堪了；且一个镜像可以同时运行多个容器，每个容器对文件的改动该怎么办？</p><p>Linux提供了一种叫做联合文件系统的文件系统，它具备如下特性：</p><ul><li>联合挂载：将多个目录按层次组合，一并挂载到一个联合挂载点。</li><li>写时复制：对联合挂载点的修改不会影响到底层的多个目录，而是使用其他目录记录修改的操作。</li></ul><p>目前有多种文件系统可以被当作联合文件系统，实现如上的功能：overlay2，aufs，devicemapper，btrfs，zfs，vfs等等。而overlay2就是其中的佼佼者，也是docker目前推荐的文件系统：<a href="https://docs.docker.com/storage/storagedriver/select-storage-driver/">https://docs.docker.com/storage/storagedriver/select-storage-driver/</a></p><h1 id="Overlay2"><a href="#Overlay2" class="headerlink" title="Overlay2"></a>Overlay2</h1><p>overlay2是一个类似于aufs的现代的联合文件系统，并且更快。overlay2已被收录进linux内核，它需要内核版本不低于4.0，如果是RHEL或Centos的话则不低于3.10.0-514。</p><p>我们可以查看下自己环境docker配置的存储驱动是什么：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line">[root@slions_pc1 ~]# docker info</span><br><span class="line">Containers: 0</span><br><span class="line"> Running: 0</span><br><span class="line"> Paused: 0</span><br><span class="line"> Stopped: 0</span><br><span class="line">Images: 1</span><br><span class="line">Server Version: 18.09.3</span><br><span class="line">Storage Driver: overlay2</span><br><span class="line"> Backing Filesystem: xfs</span><br><span class="line"> Supports d_type: true</span><br><span class="line"> Native Overlay Diff: true</span><br><span class="line">Logging Driver: json-file</span><br><span class="line">Cgroup Driver: cgroupfs</span><br><span class="line">Plugins:</span><br><span class="line"> Volume: local</span><br><span class="line"> Network: bridge host macvlan null overlay</span><br><span class="line"> Log: awslogs fluentd gcplogs gelf journald json-file local logentries splunk syslog</span><br><span class="line">Swarm: inactive</span><br><span class="line">Runtimes: runc</span><br><span class="line">Default Runtime: runc</span><br><span class="line">Init Binary: docker-init</span><br><span class="line">containerd version: d71fcd7d8303cbf684402823e425e9dd2e99285d</span><br><span class="line">runc version: N/A</span><br><span class="line">init version: fec3683</span><br><span class="line">Security Options:</span><br><span class="line"> seccomp</span><br><span class="line">  Profile: default</span><br><span class="line">Kernel Version: 3.10.0-957.el7.x86_64</span><br><span class="line">Operating System: CentOS Linux 7 (Core)</span><br><span class="line">OSType: linux</span><br><span class="line">Architecture: x86_64</span><br><span class="line">CPUs: 2</span><br><span class="line">Total Memory: 3.683GiB</span><br><span class="line">Name: slions_pc1</span><br><span class="line">ID: 7W4N:NDNK:BDXL:ADXV:ONCZ:PUA7:KSK2:73JQ:Q3HS:AEEK:VKAL:H3RC</span><br><span class="line">Docker Root Dir: /var/lib/docker</span><br><span class="line">Debug Mode (client): false</span><br><span class="line">Debug Mode (server): false</span><br><span class="line">Registry: https://index.docker.io/v1/</span><br><span class="line">Labels:</span><br><span class="line">Experimental: false</span><br><span class="line">Insecure Registries:</span><br><span class="line"> 127.0.0.0/8</span><br><span class="line">Live Restore Enabled: false</span><br><span class="line">Product License: Community Engine</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>默认就是overlay2，如果你本地环境不是overlay2，只需编辑<code>/etc/docker/daemon.json</code>, 添加以下内容并重启docker。</p><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">  <span class="attr">&quot;storage-driver&quot;</span>: <span class="string">&quot;overlay2&quot;</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>docker默认的存储目录是<code>/var/lib/docker</code>，如下：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">[root@slions_pc1 ~]# ls -l /var/lib/docker</span><br><span class="line">总用量 0</span><br><span class="line">drwx------. 2 root root  24 7月  10 11:52 builder</span><br><span class="line">drwx------. 4 root root  92 7月  10 11:52 buildkit</span><br><span class="line">drwx------. 3 root root  78 7月  12 13:08 containers</span><br><span class="line">drwx------. 3 root root  22 7月  10 11:52 image</span><br><span class="line">drwxr-x---. 3 root root  19 7月  10 11:52 network</span><br><span class="line">drwx------. 6 root root 261 7月  12 14:52 overlay2</span><br><span class="line">drwx------. 4 root root  32 7月  10 11:52 plugins</span><br><span class="line">drwx------. 2 root root   6 7月  12 12:07 runtimes</span><br><span class="line">drwx------. 2 root root   6 7月  10 11:52 swarm</span><br><span class="line">drwx------. 2 root root   6 7月  12 12:07 tmp</span><br><span class="line">drwx------. 2 root root   6 7月  10 11:52 trust</span><br><span class="line">drwx------. 2 root root  25 7月  10 11:52 volumes</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>在这里，我们只关心<code>image</code>和<code>overlay2</code>就足够了。</p><p>在我本地起了一个叫slions的容器，容器id为<code>47c26762a49a</code>，镜像id 为<code>4cdc5dd7eaad</code>，这两个id作为了容器和镜像的唯一标识符，我们后面会用到</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">[root@slions_pc1 ~]# docker run -itd --name slions nginx:latest</span><br><span class="line">47c26762a49a3f397e6796cf499f97acbc606a68c04d08b9cd169d6e33469c99</span><br><span class="line">[root@slions_pc1 ~]# docker ps</span><br><span class="line">CONTAINER ID        IMAGE               COMMAND                  CREATED             STATUS              PORTS               NAMES</span><br><span class="line">47c26762a49a        nginx:latest        &quot;/docker-entrypoint.…&quot;   3 seconds ago       Up 2 seconds        80/tcp              slions</span><br><span class="line">[root@slions_pc1 ~]# docker images -a</span><br><span class="line">REPOSITORY          TAG                 IMAGE ID            CREATED             SIZE</span><br><span class="line">nginx               latest              4cdc5dd7eaad        5 days ago          133MB</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>docker会在<code>/var/lib/docker/image</code>目录下按每个存储驱动的名字创建一个目录，如这里的<code>overlay2</code>。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@slions_pc1 ~]# ll /var/lib/docker/image/</span><br><span class="line">总用量 0</span><br><span class="line">drwx------. 5 root root 81 7月  12 14:52 overlay2</span><br></pre></td></tr></table></figure><p>查看overlay2下的目录结构：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">[root@slions_pc1 ~]# tree -L 2 /var/lib/docker/image/overlay2/</span><br><span class="line">/var/lib/docker/image/overlay2/</span><br><span class="line">├── distribution</span><br><span class="line">│   ├── diffid-by-digest</span><br><span class="line">│   └── v2metadata-by-diffid</span><br><span class="line">├── imagedb</span><br><span class="line">│   ├── content</span><br><span class="line">│   └── metadata</span><br><span class="line">├── layerdb</span><br><span class="line">│   ├── mounts</span><br><span class="line">│   ├── sha256</span><br><span class="line">│   └── tmp</span><br><span class="line">└── repositories.json</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>看名字就能知道这儿是用来存放元数据的， 其中还细分为了<code>imagedb</code>和<code>layerdb</code>，因为在docker中，image是由多个layer组合而成的，换句话就是layer是一个共享的层，可能有多个image会指向某个layer。<br> 那如何才能确认image包含了哪些layer呢？答案就在<code>imagedb</code>这个目录中去找。</p><p>比如上面启动的slions容器，我们已知image id为<code>4cdc5dd7eaad</code>，接着打印<code>/var/lib/docker/image/overlay2/imagedb/content/sha256</code>这个目录：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@slions_pc1 ~]# ll /var/lib/docker/image/overlay2/imagedb/content/sha256</span><br><span class="line">总用量 8</span><br><span class="line">-rw-------. 1 root root 7729 7月  12 15:59 4cdc5dd7eaadff5080649e8d0014f2f8d36d4ddf2eff2fdf577dd13da85c5d2f</span><br></pre></td></tr></table></figure><p><code>4cdc5dd7eaadff5080649e8d0014f2f8d36d4ddf2eff2fdf577dd13da85c5d2f</code>正是记录slions镜像元数据的文件，接下来cat一下这个文件，得到一个长长的json：</p><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">[root@slions_pc1 ~]# cat /var/lib/docker/image/overlay2/imagedb/content/sha256/<span class="number">4</span>cdc5dd7eaadff5080649e8d0014f2f8d36d4ddf2eff2fdf577dd13da85c5d2f |jq</span><br><span class="line">&#123;</span><br><span class="line">  .......</span><br><span class="line">  <span class="attr">&quot;os&quot;</span>: <span class="string">&quot;linux&quot;</span>,</span><br><span class="line">  <span class="attr">&quot;rootfs&quot;</span>: &#123;</span><br><span class="line">    <span class="attr">&quot;type&quot;</span>: <span class="string">&quot;layers&quot;</span>,</span><br><span class="line">    <span class="attr">&quot;diff_ids&quot;</span>: [</span><br><span class="line">      <span class="string">&quot;sha256:764055ebc9a7a290b64d17cf9ea550f1099c202d83795aa967428ebdf335c9f7&quot;</span>,</span><br><span class="line">      <span class="string">&quot;sha256:ace9ed9bcfafbc909bc3e9451490652f685959db02a4e01e0528a868ee8eab3e&quot;</span>,</span><br><span class="line">      <span class="string">&quot;sha256:48b4a40de3597ec0a28c2d4508dec64ae685ed0da77b128d0fb5c69cada91882&quot;</span>,</span><br><span class="line">      <span class="string">&quot;sha256:c553c6ba5f1354e1980871b413e057950e0c02d2d7a66b39de2e03836048fda9&quot;</span>,</span><br><span class="line">      <span class="string">&quot;sha256:d97733c0a3b64c08bc0dd286926a8eff1b162b4d9fad229eab807c6dc516c172&quot;</span>,</span><br><span class="line">      <span class="string">&quot;sha256:9d1af766c81806211d5453b711169103e4f5c3c2609e1dfb9ea4dee7e96a7968&quot;</span></span><br><span class="line">    ]</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>查看最后的rootfs部分，diff_ids是一个包含了6个元素的数组，这6个元素正是组成<code>nginx</code>镜像的6个layerID，从上往下看，就是底层到顶层，也就是说<code>764055e...</code>是image的最底层。既然得到了组成这个image的所有layerID，那么我们就可以带着这些layerID去寻找对应的layer了。</p><p>接下来，我们返回到上一层的<code>layerdb</code>中，先打印一下这个目录：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[root@slions_pc1 ~]# ll /var/lib/docker/image/overlay2/layerdb/</span><br><span class="line">总用量 4</span><br><span class="line">drwxr-xr-x. 3 root root   78 7月  12 16:00 mounts</span><br><span class="line">drwxr-xr-x. 8 root root 4096 7月  12 15:59 sha256</span><br><span class="line">drwxr-xr-x. 2 root root    6 7月  12 15:59 tmp</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>在这里我们只管<code>mounts</code>和<code>sha256</code>两个目录，再打印一下<code>sha256</code>目录:</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">[root@slions_pc1 ~]# ll /var/lib/docker/image/overlay2/layerdb/sha256/</span><br><span class="line">总用量 0</span><br><span class="line">drwx------. 2 root root 85 7月  12 15:59 2c78bcd3187437a7a5d9d8dbf555b3574ba7d143c1852860f9df0a46d5df056a</span><br><span class="line">drwx------. 2 root root 85 7月  12 15:59 435c6dad68b58885ad437e5f35f53e071213134eb9e4932b445eac7b39170700</span><br><span class="line">drwx------. 2 root root 85 7月  12 15:59 63d268dd303e176ba45c810247966ff8d1cb9a5bce4a404584087ec01c63de15</span><br><span class="line">drwx------. 2 root root 71 7月  12 15:59 764055ebc9a7a290b64d17cf9ea550f1099c202d83795aa967428ebdf335c9f7</span><br><span class="line">drwx------. 2 root root 85 7月  12 15:59 b27eb5bbca70862681631b492735bac31d3c1c558c774aca9c0e36f1b50ba915</span><br><span class="line">drwx------. 2 root root 85 7月  12 15:59 bdf28aff423adfe7c6cb938eced2f19a32efa9fa3922a3c5ddce584b139dc864</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>在这里，我们仅仅发现<code>764055e..</code>这个最底层的layer，那么剩余的layer为什么会没有呢？那是因为docker1.10版本以后layer层之间的关系通过chainId来进行关联。</p><p>公式是：chainID=sha256sum(H(chainID) diffid)，也就是<code>764055e..</code>的上一层的sha256 id是：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@slions_pc1 ~]# echo -n &quot;sha256:764055ebc9a7a290b64d17cf9ea550f1099c202d83795aa967428ebdf335c9f7 sha256:ace9ed9bcfafbc909bc3e9451490652f685959db02a4e01e0528a868ee8eab3e&quot;  | sha256sum -</span><br><span class="line">2c78bcd3187437a7a5d9d8dbf555b3574ba7d143c1852860f9df0a46d5df056a  -</span><br></pre></td></tr></table></figure><p>依次类推，我们就能找出所有的layerID的组合。<br>但是上面我们提到，<code>/var/lib/docker/image/overlay2/layerdb</code>存的只是元数据，那么真实的rootfs到底存在哪里呢？</p><p>答案就在<code>cache-id</code>中。我们打印一下</p><p><code>/var/lib/docker/image/overlay2/layerdb/sha256/764055ebc9a7a290b64d17cf9ea550f1099c202d83795aa967428ebdf335c9f7/cache-id</code>:</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@slions_pc1 ~]# cat /var/lib/docker/image/overlay2/layerdb/sha256/764055ebc9a7a290b64d17cf9ea550f1099c202d83795aa967428ebdf335c9f7/cache-id</span><br><span class="line">45f0a48f47ef308ead10a1b92856a0d3b8a54294518bd178ca6288c7d54a38be</span><br></pre></td></tr></table></figure><p>这个id就是对应<code>/var/lib/docker/overlay2/45f0a48f47ef308ead10a1b92856a0d3b8a54294518bd178ca6288c7d54a38be</code>。因此，以此类推，更高一层的layer对应的cache-id也能找到对应的rootfs，当这些rootfs的diff目录通过联合挂载的方式挂载到某个目录，就能提供整个容器需要的rootfs了。</p><h2 id="overlay存储流程"><a href="#overlay存储流程" class="headerlink" title="overlay存储流程"></a>overlay存储流程</h2><p><img src="/doc_picture/overlay2.png" alt="overlay2"></p><p>最下层是一个 lower 层，也就是镜像层，它是一个只读层。右上层是一个 upper 层，upper 是容器的读写层，upper 层采用了写时复制的机制，也就是说只有对某些文件需要进行修改的时候才会从 lower 层把这个文件拷贝上来，之后所有的修改操作都会对 upper 层的副本进行修改。</p><p>upper 并列的有一个 workdir，它的作用是充当一个中间层的作用。也就是说，当对 upper 层里面的副本进行修改时，会先放到 workdir，然后再从 workdir 移到 upper 里面去，这个是 overlay 的工作机制。</p><p>最上面的是 mergedir，是一个统一视图层。从 mergedir 里面可以看到 upper 和 lower 中所有数据的整合，然后我们 docker exec 到容器里面，看到一个文件系统其实就是 mergedir 统一视图层。</p><p>在运行容器后，可以通过mount命令查看其具体挂载信息：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@slions_pc1 ~]# mount -t overlay</span><br><span class="line">overlay on /var/lib/docker/overlay2/0b319b849790fe43488a0f5aa76ec318640b05d7d1ec4e162ee80ebc16c43232/merged type overlay (rw,relatime,seclabel,lowerdir=/var/lib/docker/overlay2/l/XZ6TYQO6XXCBXKH6E4N3N7QTWU:/var/lib/docker/overlay2/l/U6O63XRKJYL3MKFKZCZ33EEDE6:/var/lib/docker/overlay2/l/Y2XZUZZM42C6HPF6BHHLAMGY2G:/var/lib/docker/overlay2/l/P35XQEQ2NQELR5QWTI3POCZPJG:/var/lib/docker/overlay2/l/KRCQJWNO5RDBHNQOQXFCLT2JLX:/var/lib/docker/overlay2/l/5KKQYPGMGGXBSLAIEHV72IJ2QH:/var/lib/docker/overlay2/l/4PZ2UIG6UG3GF2PLSFP6C6X7AO,upperdir=/var/lib/docker/overlay2/0b319b849790fe43488a0f5aa76ec318640b05d7d1ec4e162ee80ebc16c43232/diff,workdir=/var/lib/docker/overlay2/0b319b849790fe43488a0f5aa76ec318640b05d7d1ec4e162ee80ebc16c43232/work)</span><br></pre></td></tr></table></figure><p>可以看到：</p><p>merged:<br><code>/var/lib/docker/overlay2/0b319b849790fe43488a0f5aa76ec318640b05d7d1ec4e162ee80ebc16c43232/merged</code></p><p>upperdir:<br><code>/var/lib/docker/overlay2/0b319b849790fe43488a0f5aa76ec318640b05d7d1ec4e162ee80ebc16c43232/diff</code></p><p>workdir:<br><code>/var/lib/docker/overlay2/0b319b849790fe43488a0f5aa76ec318640b05d7d1ec4e162ee80ebc16c43232/work</code></p><p>lowerdir:<br><code>/var/lib/docker/overlay2/l/XZ6TYQO6XXCBXKH6E4N3N7QTWU:...:.../overlay2/l/IG6UG3GF2PLSFP6C6X7AO</code><br>冒号分隔多个lowerdir，从左到右层次越低。</p><p>细心的你肯定发现了，lowerdir有7个，但是怎么看起来这么奇怪呢，其实观察下<code>/var/lib/docker/overlay2/l/</code>就会发现，正是之前我们得到的那6个rootfs目录的链接文件外加一个新的容器目录文件。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">[root@slions_pc1 overlay2]# ll /var/lib/docker/overlay2/l/</span><br><span class="line">总用量 0</span><br><span class="line">lrwxrwxrwx. 1 root root 72 7月  12 15:59 4PZ2UIG6UG3GF2PLSFP6C6X7AO -&gt; ../45f0a48f47ef308ead10a1b92856a0d3b8a54294518bd178ca6288c7d54a38be/diff</span><br><span class="line">lrwxrwxrwx. 1 root root 72 7月  12 15:59 5KKQYPGMGGXBSLAIEHV72IJ2QH -&gt; ../de169f3af7d5f47b3d83eeb21408ecb637dd5fd4847d31f7644a72830c16db65/diff</span><br><span class="line">lrwxrwxrwx. 1 root root 72 7月  12 16:00 AD3GPWKO46I5LBSKACROCGTCFN -&gt; ../0b319b849790fe43488a0f5aa76ec318640b05d7d1ec4e162ee80ebc16c43232/diff</span><br><span class="line">lrwxrwxrwx. 1 root root 72 7月  12 15:59 KRCQJWNO5RDBHNQOQXFCLT2JLX -&gt; ../e78f609099df22f4ab410322759923ebab67112a3a88fc323b4bdeb98b00d8d2/diff</span><br><span class="line">lrwxrwxrwx. 1 root root 72 7月  12 15:59 P35XQEQ2NQELR5QWTI3POCZPJG -&gt; ../18998b6cffbcc8eb5984c1077e00d02d6d2f150a889c8c856055364bfa16b0a2/diff</span><br><span class="line">lrwxrwxrwx. 1 root root 72 7月  12 15:59 U6O63XRKJYL3MKFKZCZ33EEDE6 -&gt; ../192d460630fe2fefa59151b3a7b37032346487ac6af719d3be3f2e1dfd373494/diff</span><br><span class="line">lrwxrwxrwx. 1 root root 77 7月  12 16:00 XZ6TYQO6XXCBXKH6E4N3N7QTWU -&gt; ../0b319b849790fe43488a0f5aa76ec318640b05d7d1ec4e162ee80ebc16c43232-init/diff</span><br><span class="line">lrwxrwxrwx. 1 root root 72 7月  12 15:59 Y2XZUZZM42C6HPF6BHHLAMGY2G -&gt; ../aca1597583b8862fae70d6bfc7487a041092a7b9d1cf400802d11225b2fb3f5b/diff</span><br><span class="line"></span><br></pre></td></tr></table></figure><h2 id="overlay实验"><a href="#overlay实验" class="headerlink" title="overlay实验"></a>overlay实验</h2><p><strong>创建并挂载测试</strong></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">[root@slions_pc1 ~]# mkdir tmp</span><br><span class="line">[root@slions_pc1 ~]# cd tmp/</span><br><span class="line">[root@slions_pc1 tmp]# mkdir -p lower&#123;1..2&#125;/dir upper/dir worker merge</span><br><span class="line">[root@slions_pc1 tmp]# ll</span><br><span class="line">总用量 0</span><br><span class="line">drwxr-xr-x. 3 root root 17 7月  12 16:42 lower1</span><br><span class="line">drwxr-xr-x. 3 root root 17 7月  12 16:42 lower2</span><br><span class="line">drwxr-xr-x. 2 root root  6 7月  12 16:42 merge</span><br><span class="line">drwxr-xr-x. 3 root root 17 7月  12 16:42 upper</span><br><span class="line">drwxr-xr-x. 2 root root  6 7月  12 16:42 worker</span><br><span class="line">[root@slions_pc1 tmp]# for i in 1 2 ;do touch lower$i/foo$i;done</span><br><span class="line">[root@slions_pc1 tmp]# tree</span><br><span class="line">.</span><br><span class="line">├── lower1</span><br><span class="line">│   ├── dir</span><br><span class="line">│   └── foo1</span><br><span class="line">├── lower2</span><br><span class="line">│   ├── dir</span><br><span class="line">│   └── foo2</span><br><span class="line">├── merge</span><br><span class="line">├── upper</span><br><span class="line">│   └── dir</span><br><span class="line">└── worker</span><br><span class="line">[root@slions_pc1 tmp]# mount -t overlay overlay -o lowerdir=lower1:lower2,upperdir=upper,workdir=worker merge/</span><br><span class="line">[root@slions_pc1 tmp]# mount |grep overlay</span><br><span class="line">...</span><br><span class="line">overlay on /root/tmp/merge type overlay (rw,relatime,seclabel,lowerdir=lower1:lower2,upperdir=upper,workdir=worker)</span><br></pre></td></tr></table></figure><p>挂载了一个名为overlay的overlay类型的文件系统，挂载点为merged目录。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line">[root@slions_pc1 tmp]# touch upper/foo3</span><br><span class="line">[root@slions_pc1 tmp]# echo &quot;from lower1&quot; &gt;lower1/dir/aa</span><br><span class="line">[root@slions_pc1 tmp]# echo &quot;from lower2&quot; &gt;lower2/dir/aa</span><br><span class="line">[root@slions_pc1 tmp]# echo &quot;from lower1 bb&quot; &gt;lower2/dir/bb</span><br><span class="line">[root@slions_pc1 tmp]# echo &quot;from upper&quot; &gt;upper/dir/bb</span><br><span class="line">[root@slions_pc1 tmp]# tree</span><br><span class="line">.</span><br><span class="line">├── lower1</span><br><span class="line">│   ├── dir</span><br><span class="line">│   │   └── aa</span><br><span class="line">│   └── foo1</span><br><span class="line">├── lower2</span><br><span class="line">│   ├── dir</span><br><span class="line">│   │   ├── aa</span><br><span class="line">│   │   └── bb</span><br><span class="line">│   └── foo2</span><br><span class="line">├── merge</span><br><span class="line">│   ├── dir</span><br><span class="line">│   │   ├── aa</span><br><span class="line">│   │   └── bb</span><br><span class="line">│   ├── foo1</span><br><span class="line">│   ├── foo2</span><br><span class="line">│   └── foo3</span><br><span class="line">├── upper</span><br><span class="line">│   ├── dir</span><br><span class="line">│   │   └── bb</span><br><span class="line">│   └── foo3</span><br><span class="line">└── worker</span><br><span class="line">    └── work</span><br><span class="line">[root@slions_pc1 tmp]# cat merge/dir/aa</span><br><span class="line">from lower1</span><br><span class="line">[root@slions_pc1 tmp]# cat merge/dir/bb</span><br><span class="line">from upper</span><br><span class="line"></span><br></pre></td></tr></table></figure><p><strong>删除文件测试</strong></p><p>删除文件时，会在upper层创建whiteout的特殊文件，overlayfs会自动过过滤掉和whiteout文件自身以及和它同名的lower层文件和目录，达到了隐藏文件的目的，让用户以为文件已经被删除了。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br></pre></td><td class="code"><pre><span class="line">[root@slions_pc1 tmp]# tree</span><br><span class="line">.</span><br><span class="line">├── lower1</span><br><span class="line">│   ├── dir</span><br><span class="line">│   │   └── aa</span><br><span class="line">│   └── foo1</span><br><span class="line">├── lower2</span><br><span class="line">│   ├── dir</span><br><span class="line">│   │   ├── aa</span><br><span class="line">│   │   └── bb</span><br><span class="line">│   └── foo2</span><br><span class="line">├── merge</span><br><span class="line">│   ├── dir</span><br><span class="line">│   │   ├── aa</span><br><span class="line">│   │   └── bb</span><br><span class="line">│   ├── foo1</span><br><span class="line">│   ├── foo2</span><br><span class="line">│   └── foo3</span><br><span class="line">├── upper</span><br><span class="line">│   ├── dir</span><br><span class="line">│   │   └── bb</span><br><span class="line">│   └── foo3</span><br><span class="line">└── worker</span><br><span class="line">    └── work</span><br><span class="line"></span><br><span class="line">10 directories, 12 files</span><br><span class="line">[root@slions_pc1 tmp]# rm merge/foo1</span><br><span class="line">rm：是否删除普通空文件 &quot;merge/foo1&quot;？y</span><br><span class="line">[root@slions_pc1 tmp]# tree</span><br><span class="line">.</span><br><span class="line">├── lower1</span><br><span class="line">│   ├── dir</span><br><span class="line">│   │   └── aa</span><br><span class="line">│   └── foo1</span><br><span class="line">├── lower2</span><br><span class="line">│   ├── dir</span><br><span class="line">│   │   ├── aa</span><br><span class="line">│   │   └── bb</span><br><span class="line">│   └── foo2</span><br><span class="line">├── merge</span><br><span class="line">│   ├── dir</span><br><span class="line">│   │   ├── aa</span><br><span class="line">│   │   └── bb</span><br><span class="line">│   ├── foo2</span><br><span class="line">│   └── foo3</span><br><span class="line">├── upper</span><br><span class="line">│   ├── dir</span><br><span class="line">│   │   └── bb</span><br><span class="line">│   ├── foo1</span><br><span class="line">│   └── foo3</span><br><span class="line">└── worker</span><br><span class="line">    └── work</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure><h1 id="参考文档"><a href="#参考文档" class="headerlink" title="参考文档"></a>参考文档</h1><p>Use the OverlayFS storage driver：<a href="https://docs.docker.com/storage/storagedriver/overlayfs-driver/#how-the-overlay2-driver-works">https://docs.docker.com/storage/storagedriver/overlayfs-driver/#how-the-overlay2-driver-works</a></p><p>Docker storage drivers：<a href="https://docs.docker.com/storage/storagedriver/select-storage-driver/">https://docs.docker.com/storage/storagedriver/select-storage-driver/</a></p><p>Docker存储驱动之–overlay2：<a href="https://www.jianshu.com/p/3826859a6d6e">https://www.jianshu.com/p/3826859a6d6e</a></p><p>深入理解overlayfs（二）：使用与原理分析：<a href="https://blog.csdn.net/linshenyuan1213/article/details/82527712">https://blog.csdn.net/linshenyuan1213/article/details/82527712</a></p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;容器中通过使用不同的rootfs来模拟了不同的操作系统及应用，不过，如果使用每个镜像都需要一个独立的根文件系统的话，那想必磁盘早已拥挤不堪了；且一个镜像可以同时运行多个容器，每个容器对文件的改动该怎么办？&lt;/p&gt;
&lt;p&gt;Linux提供了一种叫做联合文件系统的文件系统，它具备</summary>
      
    
    
    
    
    <category term="docker" scheme="https://slions.github.io/tags/docker/"/>
    
  </entry>
  
  <entry>
    <title>Centos镜像那么小能用么？</title>
    <link href="https://slions.github.io/2021/07/11/Centos%E9%95%9C%E5%83%8F%E9%82%A3%E4%B9%88%E5%B0%8F%E8%83%BD%E7%94%A8%E4%B9%88%EF%BC%9F/"/>
    <id>https://slions.github.io/2021/07/11/Centos%E9%95%9C%E5%83%8F%E9%82%A3%E4%B9%88%E5%B0%8F%E8%83%BD%E7%94%A8%E4%B9%88%EF%BC%9F/</id>
    <published>2021-07-11T13:20:06.000Z</published>
    <updated>2021-07-11T14:19:20.159Z</updated>
    
    <content type="html"><![CDATA[<p>不知道大家在使用docker的过程中有没有过这种困惑，就是下载了个centos镜像，可以看到只有200多M,这和我们之前使用传统方式启一个centos系统动不动4、5个G简直有天壤之别，这么小能运行么，稳定么？</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">[root@slions_pc1 ~]# docker images</span><br><span class="line">REPOSITORY          TAG                 IMAGE ID            CREATED             SIZE</span><br><span class="line">nginx               latest              4cdc5dd7eaad        4 days ago          133MB</span><br><span class="line">tomcat              latest              36ef696ea43d        8 days ago          667MB</span><br><span class="line">ubuntu              latest              9873176a8ff5        3 weeks ago         72.7MB</span><br><span class="line">busybox             latest              69593048aa3a        4 weeks ago         1.24MB</span><br><span class="line">centos              7                   8652b9f0cb4c        7 months ago        204MB</span><br><span class="line">[root@slions_pc1 ~]# docker run -it --name slions centos:7</span><br><span class="line">[root@1633937d7a61 /]# ls /</span><br><span class="line">anaconda-post.log  bin  dev  etc  home  lib  lib64  media  mnt  opt  proc  root  run  sbin  srv  sys  tmp  usr  var</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>之前我也不是很了解这块，最近又系统的看了下docker的机制，接下来就来说说Centos镜像为啥这么小。</p><p>Linux操作系统由2部分构成的，bootfs（kernel空间）和rootfs(用户空间)。</p><p>Linux启动时会加载bootfs文件系统，之后bootfs就会被卸载掉，rootfs用户空间包括了/dev,/proc,/bin…..</p><p>换句话说，bootfs就是在主机中加载下内核和物理机共用的，并不属于容器的一个部分，对于base镜像来说底层直接使用主机的kernel就够了，自己只需要提供一个rootfs就行了。对于一个精简版的OS来说，rootfs可以非常小，只需包括最基本的命令，工具还有程序的库就可以完成了。</p><p>我们可以再看下centos:7由那几层构成的。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[root@slions_pc1 ~]# docker history --no-trunc centos:7</span><br><span class="line">IMAGE                                                                     CREATED             CREATED BY                                                                                                                                                                                                                                                                                                                                                                                                  SIZE                COMMENT</span><br><span class="line">sha256:8652b9f0cb4c0599575e5a003f5906876e10c1ceb2ab9fe1786712dac14a50cf   7 months ago        /bin/sh -c #(nop)  CMD [&quot;/bin/bash&quot;]                                                                                                                                                                                                                                                                                                                                                                        0B</span><br><span class="line">&lt;missing&gt;                                                                 7 months ago        /bin/sh -c #(nop)  LABEL org.label-schema.schema-version=1.0 org.label-schema.name=CentOS Base Image org.label-schema.vendor=CentOS org.label-schema.license=GPLv2 org.label-schema.build-date=20201113 org.opencontainers.image.title=CentOS Base Image org.opencontainers.image.vendor=CentOS org.opencontainers.image.licenses=GPL-2.0-only org.opencontainers.image.created=2020-11-13 00:00:00+00:00   0B</span><br><span class="line">&lt;missing&gt;                                                                 7 months ago        /bin/sh -c #(nop) ADD file:b3ebbe8bd304723d43b7b44a6d990cd657b63d93d6a2a9293983a30bfc1dfa53 in /                                                                                                                                                                                                                                                                                                            204MB</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>其实不同的Linux发行版本主要的区别就在rootfs上，与内核的区别不大，所以docker可以同时支持多种Linux镜像，来模拟出多种操作系统的环境。</p><p>需要注意的是docker容器的内核版本会与当前主机的内核版本一致，并且不能修改，如果存在软件对内核版本有依赖就不推荐使用docker部署。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;不知道大家在使用docker的过程中有没有过这种困惑，就是下载了个centos镜像，可以看到只有200多M,这和我们之前使用传统方式启一个centos系统动不动4、5个G简直有天壤之别，这么小能运行么，稳定么？&lt;/p&gt;
&lt;figure class=&quot;highlight sh</summary>
      
    
    
    
    
    <category term="docker" scheme="https://slions.github.io/tags/docker/"/>
    
  </entry>
  
  <entry>
    <title>浅谈docker框架</title>
    <link href="https://slions.github.io/2021/07/11/%E6%B5%85%E8%B0%88docker%E6%A1%86%E6%9E%B6/"/>
    <id>https://slions.github.io/2021/07/11/%E6%B5%85%E8%B0%88docker%E6%A1%86%E6%9E%B6/</id>
    <published>2021-07-11T06:37:14.000Z</published>
    <updated>2021-07-11T15:38:27.034Z</updated>
    
    <content type="html"><![CDATA[<p>如果你之前使用过docker应该发现其并不难，把所谓的三大核心“镜像”，“容器”，“仓库”掌握了就能满足日常工作中的大多数应用内容。但如果想要更深入些了解docker,就需要熟悉下docker的框架结构，正巧前段时间同组大佬通过docker源码来分享了一波学习心得，也使得我对docker的理解更深了些，下面来一起分析下docker的框架结构。</p><h1 id="docker架构"><a href="#docker架构" class="headerlink" title="docker架构"></a>docker架构</h1><p>在我的本地环境执行docker version，会发现里面有client端与server端，证明docker是一个C/S模式的架构。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">[root@kubemaster ~]# docker version</span><br><span class="line">Client: Docker Engine - Community</span><br><span class="line"> Version:           19.03.4</span><br><span class="line"> API version:       1.40</span><br><span class="line"> Go version:        go1.12.10</span><br><span class="line"> Git commit:        9013bf583a</span><br><span class="line"> Built:             Fri Oct 18 15:52:22 2019</span><br><span class="line"> OS/Arch:           linux/amd64</span><br><span class="line"> Experimental:      false</span><br><span class="line"></span><br><span class="line">Server: Docker Engine - Community</span><br><span class="line"> Engine:</span><br><span class="line">  Version:          19.03.4</span><br><span class="line">  API version:      1.40 (minimum version 1.12)</span><br><span class="line">  Go version:       go1.12.10</span><br><span class="line">  Git commit:       9013bf583a</span><br><span class="line">  Built:            Fri Oct 18 15:50:54 2019</span><br><span class="line">  OS/Arch:          linux/amd64</span><br><span class="line">  Experimental:     false</span><br><span class="line"> containerd:</span><br><span class="line">  Version:          1.3.7</span><br><span class="line">  GitCommit:        8fba4e9a7d01810a393d5d25a3621dc101981175</span><br><span class="line"> runc:</span><br><span class="line">  Version:          1.0.0-rc10</span><br><span class="line">  GitCommit:        dc9208a3303feef5b3839f4323d9beb36df0a9dd</span><br><span class="line"> docker-init:</span><br><span class="line">  Version:          0.18.0</span><br><span class="line">  GitCommit:        fec3683</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>一般来说我们可以把client端与server端分开，默认情况它们通过API进行连接。修改<code>/usr/lib/systemd/system/docker.service</code>,在<code>ExecStart</code>处添加<code> -H 0.0.0.0:31375</code>,让docker服务端来监听主机下的31375端口。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line">[root@slions_pc1 ~]# cat /usr/lib/systemd/system/docker.service</span><br><span class="line">[Unit]</span><br><span class="line">Description=Docker Application Container Engine</span><br><span class="line">Documentation=https://docs.docker.com</span><br><span class="line">BindsTo=containerd.service</span><br><span class="line">After=network-online.target firewalld.service containerd.service</span><br><span class="line">Wants=network-online.target</span><br><span class="line">Requires=docker.socket</span><br><span class="line"></span><br><span class="line">[Service]</span><br><span class="line">Type=notify</span><br><span class="line"><span class="meta">#</span><span class="bash"> the default is not to use systemd <span class="keyword">for</span> cgroups because the delegate issues still</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> exists and systemd currently does not support the cgroup feature <span class="built_in">set</span> required</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> <span class="keyword">for</span> containers run by docker</span></span><br><span class="line">ExecStart=/usr/bin/dockerd -H fd:// --containerd=/run/containerd/containerd.sock -H 0.0.0.0:31375</span><br><span class="line">ExecReload=/bin/kill -s HUP $MAINPID</span><br><span class="line">TimeoutSec=0</span><br><span class="line">RestartSec=2</span><br><span class="line">Restart=always</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> Note that StartLimit* options were moved from <span class="string">&quot;Service&quot;</span> to <span class="string">&quot;Unit&quot;</span> <span class="keyword">in</span> systemd 229.</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> Both the old, and new location are accepted by systemd 229 and up, so using the old location</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> to make them work <span class="keyword">for</span> either version of systemd.</span></span><br><span class="line">StartLimitBurst=3</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> Note that StartLimitInterval was renamed to StartLimitIntervalSec <span class="keyword">in</span> systemd 230.</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> Both the old, and new name are accepted by systemd 230 and up, so using the old name to make</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> this option work <span class="keyword">for</span> either version of systemd.</span></span><br><span class="line">StartLimitInterval=60s</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> Having non-zero Limit*s causes performance problems due to accounting overhead</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> <span class="keyword">in</span> the kernel. We recommend using cgroups to <span class="keyword">do</span> container-local accounting.</span></span><br><span class="line">LimitNOFILE=infinity</span><br><span class="line">LimitNPROC=infinity</span><br><span class="line">LimitCORE=infinity</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> Comment TasksMax <span class="keyword">if</span> your systemd version does not supports it.</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> Only systemd 226 and above support this option.</span></span><br><span class="line">TasksMax=infinity</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> <span class="built_in">set</span> delegate yes so that systemd does not reset the cgroups of docker containers</span></span><br><span class="line">Delegate=yes</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> <span class="built_in">kill</span> only the docker process, not all processes <span class="keyword">in</span> the cgroup</span></span><br><span class="line">KillMode=process</span><br><span class="line"></span><br><span class="line">[Install]</span><br><span class="line">WantedBy=multi-user.target</span><br></pre></td></tr></table></figure><p>修改好后我们重启服务，此时docker成功监听在31375端口。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[root@slions_pc1 ~]# systemctl daemon-reload</span><br><span class="line">[root@slions_pc1 ~]# systemctl restart docker</span><br><span class="line">[root@slions_pc1 ~]# ss -lntp|grep docker</span><br><span class="line">LISTEN     0      128         :::31375                   :::*                   users:((&quot;dockerd&quot;,pid=10808,fd=3))</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>这时客户端通过指明连接的docker server地址，即可查看对应服务端的镜像情况。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[root@slions_pc1 ~]# docker -H 192.168.100.10:31375 images</span><br><span class="line">REPOSITORY          TAG                 IMAGE ID            CREATED             SIZE</span><br><span class="line">tomcat              latest              36ef696ea43d        8 days ago          667MB</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>我们知道了docker是c/s模式架构后，下面来细致化的讲解下架构模块。</p><h1 id="docker架构图"><a href="#docker架构图" class="headerlink" title="docker架构图"></a>docker架构图</h1><p><img src="/doc_picture/docker_jg1.png" alt="img"></p><p>大致的流程可概况为：</p><ol><li>用户是使用Docker Client与Docker Daemon建立通信，并发送请求给后者。</li><li>Docker Daemon作为Docker架构中的主体部分，首先提供Server的功能使其可以接受Docker Client的请求；</li><li>Engine执行Docker内部的一系列工作，每一项工作都是以一个Job的形式的存在。</li><li>Job的运行过程中，当需要容器镜像时，则从Docker Registry中下载镜像，并通过镜像管理驱动graphdriver将下载镜像以Graph的形式存储；</li><li>当需要为Docker创建网络环境时，通过网络管理驱动networkdriver创建并配置Docker容器网络环境；</li><li>当需要限制Docker容器运行资源或执行用户指令等操作时，则通过execdriver来完成。</li><li>libcontainer是一项独立的容器管理包，networkdriver以及execdriver都是通过libcontainer来实现具体对容器进行的操作。</li></ol><h1 id="模块组件分析"><a href="#模块组件分析" class="headerlink" title="模块组件分析"></a>模块组件分析</h1><h2 id="Docker-Client"><a href="#Docker-Client" class="headerlink" title="Docker Client"></a>Docker Client</h2><p>client是docker架构中与daemon建立通讯联系的客户端。用户可以通过docker可执行文件向docker daemon发送请求管理容器。</p><p>Docker Client可以通过以下三种方式和Docker Daemon建立通信：tcp://host:port，unix://path_to_socket和fd://socketfd。</p><p>Docker Client发送容器管理请求后，由Docker Daemon接受并处理请求，当Docker Client接收到返回的请求相应并简单处理后，Docker Client一次完整的生命周期就结束了。</p><h2 id="Docker-Daemon"><a href="#Docker-Daemon" class="headerlink" title="Docker Daemon"></a>Docker Daemon</h2><p>docker daemon是docker的常驻进程，主要作用是接收并分发client请求和管理所有容器。</p><p>docker daemon有三驾马车一个是<code>server</code>,一个是 <code>engine</code>,一个是 <code>job</code>。</p><p><img src="/doc_picture/docker_jg2.png" alt="img"></p><ul><li><strong>docker server：</strong><ol><li>Docker Server相当于C/S架构的服务端。功能为接受并调度分发Docker Client发送的请求。接受请求后，Server通过路由与分发调度，找到相应的Handler来执行请求。</li><li>Docker的启动过程中,会创建了一个mux.Router，提供请求的路由功能。该mux.Router中添加了众多的路由项，每一个路由项由HTTP请求方法（PUT、POST、GET或DELETE）、URL、Handler三部分组成。创建完mux.Router之后，Docker将Server的监听地址以及mux.Router作为参数，创建一个httpSrv=http.Server{}，最终执行httpSrv.Serve()为请求服务。</li><li>在Server的服务过程中，Server在listener上接受Docker Client的访问请求，并创建一个全新的goroutine来服务该请求。在goroutine中，首先读取请求内容，然后做解析工作，接着找到相应的路由项，随后调用相应的Handler来处理该请求，最后Handler处理完请求之后回复该请求。</li></ol></li></ul><p><img src="/doc_picture/docker_jg3.png" alt="img"></p><ul><li><p><strong>Engine</strong></p><ol><li>Engine是Docke的运行引擎，同时也是Docker运行的核心模块。它扮演Docker container存储仓库的角色，并且通过执行job的方式来操纵管理这些容器。</li><li>在Engine数据结构的设计与实现过程中，有一个handler对象。该handler对象存储的都是关于众多特定job的handler处理访问。</li></ol></li><li><p><strong>job</strong></p><p>​    一个Job可以认为是Docker架构中Engine内部最基本的工作执行单元。Docker可以做的每一项工作，都可以抽象为一个job。</p></li></ul><h2 id="Docker-Registry"><a href="#Docker-Registry" class="headerlink" title="Docker Registry"></a>Docker Registry</h2><p>Docker Registry就是镜像注册中心，用户如果不进行配置的话默认的地址是dokcerhub地址，这个大家应该都知道。</p><h2 id="Graph"><a href="#Graph" class="headerlink" title="Graph"></a>Graph</h2><p><img src="/doc_picture/docker_jg4.png" alt="img"></p><ul><li><p><strong>Repository</strong></p><ol><li>已下载镜像的保管者（包括下载镜像和dockerfile构建的镜像）。</li><li>镜像的存储类型有aufs，devicemapper,Btrfs，overlay2等。docker info中的<code>Storage Driver</code></li><li>同时在Graph的本地目录中，关于每一个的容器镜像，具体存储的信息有：该容器镜像的元数据，容器镜像的大小信息，以及该容器镜像所代表的具体rootfs。</li></ol></li><li><p><strong>GraphDB</strong></p><p>​    已下载容器镜像之间对应关系的记录者。</p></li></ul><h2 id="Driver"><a href="#Driver" class="headerlink" title="Driver"></a>Driver</h2><p>Driver是Docker架构中的驱动模块。通过Driver驱动，Docker可以实现对Docker容器执行环境的定制。即<u>Graph负责镜像的存储，Driver负责容器的执行。</u></p><ul><li><strong>graphdriver</strong><ol><li>graphdriver主要用于完成容器镜像的管理，包括存储与获取。值得注意的地方是，不同文件系统的存储驱动器也不相同，比如redhat系的操作系统使用的是overlay2，通过xfs文件系统来管理；如果是Ubuntu则使用的是aufs;如果是suse则是btrfs。建议不要修改默认的docker storage driver，跟随系统默认。</li><li>存储：docker pull下载的镜像由graphdriver存储到本地的指定目录（Graph中）。</li><li>获取：docker run（create）用镜像来创建容器的时候由graphdriver到本地Graph中获取镜像。</li></ol></li></ul><p><img src="/doc_picture/docker_jg5.png" alt="img"></p><ul><li><p><strong>networkdriver</strong></p><p>networkdriver的用途是完成Docker容器网络环境的配置，其中包括</p><ol><li>Docker启动时为Docker环境创建网桥；</li><li>Docker容器创建时为其创建专属虚拟网卡设备；</li><li>Docker容器分配IP、端口并与宿主机做端口映射，设置容器防火墙策略等。</li></ol></li></ul><p><img src="/doc_picture/docker_jg6.png" alt="img"></p><ul><li><strong>execdriver</strong><ol><li>execdriver作为Docker容器的执行驱动，负责创建容器运行命名空间，负责容器资源使用的统计与限制，负责容器内部进程的真正运行等。</li><li>现在execdriver默认使用native驱动，不依赖于LXC。</li></ol></li></ul><p><img src="/doc_picture/docker_jg7.png" alt="img"></p><h2 id="libcontainer"><a href="#libcontainer" class="headerlink" title="libcontainer"></a>libcontainer</h2><p>libcontainer是golang编写的第三方库，主要封装了一些linux内核与容器相关的API；因为屏蔽了所有内核操作，保证了容器与底层的松耦合性</p><p>Docker可以直接调用libcontainer，而最终操纵容器的namespace、cgroups、apparmor、网络设备以及防火墙规则等。</p><p><img src="/doc_picture/docker_jg8.png" alt="img"></p><h1 id="参考资料："><a href="#参考资料：" class="headerlink" title="参考资料："></a>参考资料：</h1><p>《Docker源码分析》</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;如果你之前使用过docker应该发现其并不难，把所谓的三大核心“镜像”，“容器”，“仓库”掌握了就能满足日常工作中的大多数应用内容。但如果想要更深入些了解docker,就需要熟悉下docker的框架结构，正巧前段时间同组大佬通过docker源码来分享了一波学习心得，也使得我</summary>
      
    
    
    
    
    <category term="docker" scheme="https://slions.github.io/tags/docker/"/>
    
  </entry>
  
  <entry>
    <title>聊聊Cgroup</title>
    <link href="https://slions.github.io/2021/07/10/%E8%81%8A%E8%81%8ACgroup/"/>
    <id>https://slions.github.io/2021/07/10/%E8%81%8A%E8%81%8ACgroup/</id>
    <published>2021-07-10T10:32:32.000Z</published>
    <updated>2021-07-11T06:35:02.823Z</updated>
    
    <content type="html"><![CDATA[<p>此前已经聊了容器隔离技术<code>namespace</code>,这次我们接着来看看如何来限制容器的资源。</p><h1 id="什么是Cgroup"><a href="#什么是Cgroup" class="headerlink" title="什么是Cgroup"></a>什么是Cgroup</h1><p>容器的资源限制是通过Cgroup实现的，Cgroups 的全称是 Control Group，属于linux内核提供的一个特性，它最主要的作用就是限制一个进程组能够使用的资源上限，包括 CPU、内存、磁盘、网络带宽等等。</p><p>此外，Cgroups 还能够对进程进行优先级设置、审计，以及将进程挂起和恢复等操作。</p><p>在 Linux 中，Cgroups 给用户暴露出来的操作接口是文件系统，默认linux系统在启动时已经挂载到了 /sys/fs/cgroup 路径下。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">[root@slions_pc1 ~]# mount -t cgroup</span><br><span class="line">cgroup on /sys/fs/cgroup/systemd type cgroup (rw,nosuid,nodev,noexec,relatime,seclabel,xattr,release_agent=/usr/lib/systemd/systemd-cgroups                                              -agent,name=systemd)</span><br><span class="line">cgroup on /sys/fs/cgroup/cpu,cpuacct type cgroup (rw,nosuid,nodev,noexec,relatime,seclabel,cpuacct,cpu)</span><br><span class="line">cgroup on /sys/fs/cgroup/net_cls,net_prio type cgroup (rw,nosuid,nodev,noexec,relatime,seclabel,net_prio,net_cls)</span><br><span class="line">cgroup on /sys/fs/cgroup/cpuset type cgroup (rw,nosuid,nodev,noexec,relatime,seclabel,cpuset)</span><br><span class="line">cgroup on /sys/fs/cgroup/devices type cgroup (rw,nosuid,nodev,noexec,relatime,seclabel,devices)</span><br><span class="line">cgroup on /sys/fs/cgroup/freezer type cgroup (rw,nosuid,nodev,noexec,relatime,seclabel,freezer)</span><br><span class="line">cgroup on /sys/fs/cgroup/perf_event type cgroup (rw,nosuid,nodev,noexec,relatime,seclabel,perf_event)</span><br><span class="line">cgroup on /sys/fs/cgroup/hugetlb type cgroup (rw,nosuid,nodev,noexec,relatime,seclabel,hugetlb)</span><br><span class="line">cgroup on /sys/fs/cgroup/memory type cgroup (rw,nosuid,nodev,noexec,relatime,seclabel,memory)</span><br><span class="line">cgroup on /sys/fs/cgroup/pids type cgroup (rw,nosuid,nodev,noexec,relatime,seclabel,pids)</span><br><span class="line">cgroup on /sys/fs/cgroup/blkio type cgroup (rw,nosuid,nodev,noexec,relatime,seclabel,blkio)</span><br><span class="line"></span><br></pre></td></tr></table></figure><h1 id="查看Cgroup"><a href="#查看Cgroup" class="headerlink" title="查看Cgroup"></a>查看Cgroup</h1><p>可以看到，在 /sys/fs/cgroup 下面有很多诸如 cpuset、cpu、 memory 这样的子目录，也叫子系统。这些都是我这台机器当前可以被 Cgroups 进行限制的资源种类。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@slions_pc1 ~]# ls /sys/fs/cgroup/</span><br><span class="line">blkio  cpu  cpuacct  cpu,cpuacct  cpuset  devices  freezer  hugetlb  memory  net_cls  net_cls,net_prio  net_prio  perf_event  pids  systemd</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>而在子系统对应的资源种类下，你就可以看到该类资源具体可以被限制的方法。</p><h1 id="Cgroup子系统"><a href="#Cgroup子系统" class="headerlink" title="Cgroup子系统"></a>Cgroup子系统</h1><p>对实际资源的分配和管理是由各个Cgroup子系统来完成的。</p><h2 id="cpuset子系统"><a href="#cpuset子系统" class="headerlink" title="cpuset子系统"></a>cpuset子系统</h2><p>cpuset可以为一组进程分配指定的CPU和内存节点。</p><p>主要的接口有：</p><ul><li>cpuset.cpus: 允许进程使用的CPU列表</li><li>cpuset.mems: 允许进程使用的内存节点列表</li></ul><h2 id="cpu子系统"><a href="#cpu子系统" class="headerlink" title="cpu子系统"></a>cpu子系统</h2><p>cpu子系统用于限制进程的CPU占用率。</p><p>主要的接口有：</p><ul><li>cpu.shares：CPU比重分配</li><li>cpu.cfs_period_us和cpu.cfs_quota_us：CPU带宽限制(微秒)</li><li>cpu.rt_period_us和cpu.rt.runtime_us：实时进程的CPU带宽限制</li></ul><h2 id="cpuacct子系统"><a href="#cpuacct子系统" class="headerlink" title="cpuacct子系统"></a>cpuacct子系统</h2><p>cpuacct子系统用来统计各Cgroup的CPU使用情况。</p><ul><li>cpuacct.stat: 报告这个Cgroup分别在用户态和内核态消耗的CPU时间（USER_HZ）</li><li>cpuacct.usage: 报告这个Cgroup消耗的总CPU时间</li><li>cpuacct.usage_percpu: 报告这个Cgroup在各CPU上消耗的CPU时间</li></ul><h2 id="memory子系统"><a href="#memory子系统" class="headerlink" title="memory子系统"></a>memory子系统</h2><p>memory子系统用来限制Cgroup所使用的内存上限。</p><ul><li>memory.limit_in_bytes：内存上限</li><li>memory.memsw.limit_in_bytes：内存+swap上限</li><li>memory.oom_control：设置为0则当内存使用量超过上限时系统不会杀死进程，而是阻塞进程直到有内存被释放</li><li>memory.stat： 汇报内存使用情况</li></ul><h2 id="blkio子系统"><a href="#blkio子系统" class="headerlink" title="blkio子系统"></a>blkio子系统</h2><p>blkio子系统用来限制Cgroup的block I/O带宽。</p><ul><li><p>blkio.weight: 设置权重</p></li><li><p>blkio.weight_device：对具体的设备设置权重</p></li><li><p>blkio.throttle.write_bps_device：对具体的设备设置每秒写磁盘的带宽上限</p></li><li><p>blkio.throttle.read_bps_device: 对具体的设备设置每秒读磁盘的带宽上限</p></li></ul><h2 id="device子系统"><a href="#device子系统" class="headerlink" title="device子系统"></a>device子系统</h2><p>用来控制Cgroup的进程对那些设备有访问权限。</p><ul><li><p>devices.list：只读文件，显示目前允许被访问的设备列表</p></li><li><p>devices.deny：只写文件，允许相应的设备访问权限</p></li><li><p>devices.allow：只写文件，禁止相应的设备访问权限</p></li></ul><h1 id="如何限制资源"><a href="#如何限制资源" class="headerlink" title="如何限制资源"></a>如何限制资源</h1><p>介绍了那么多Cgroup的概念，我们下面就拿限制cpu使用率为例，来讲讲如何限制这些资源。</p><p>首先本地下载用来设置Cgroup的工具包。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@slions_pc1 ~]# yum install  libcgroup-tools.x86_64 -y</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>接下来通过cgcreate命令来创建一个自己的控制组slions。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line">[root@slions_pc1 cpu]# cgcreate -g cpu:slions</span><br><span class="line">[root@slions_pc1 cpu]# ll</span><br><span class="line">总用量 0</span><br><span class="line">-rw-r--r--.  1 root root 0 7月   9 14:54 cgroup.clone_children</span><br><span class="line">--w--w--w-.  1 root root 0 7月   9 14:54 cgroup.event_control</span><br><span class="line">-rw-r--r--.  1 root root 0 7月   9 14:54 cgroup.procs</span><br><span class="line">-r--r--r--.  1 root root 0 7月   9 14:54 cgroup.sane_behavior</span><br><span class="line">-r--r--r--.  1 root root 0 7月   9 14:54 cpuacct.stat</span><br><span class="line">-rw-r--r--.  1 root root 0 7月   9 14:54 cpuacct.usage</span><br><span class="line">-r--r--r--.  1 root root 0 7月   9 14:54 cpuacct.usage_percpu</span><br><span class="line">-rw-r--r--.  1 root root 0 7月   9 14:54 cpu.cfs_period_us</span><br><span class="line">-rw-r--r--.  1 root root 0 7月   9 14:54 cpu.cfs_quota_us</span><br><span class="line">-rw-r--r--.  1 root root 0 7月   9 14:54 cpu.rt_period_us</span><br><span class="line">-rw-r--r--.  1 root root 0 7月   9 14:54 cpu.rt_runtime_us</span><br><span class="line">-rw-r--r--.  1 root root 0 7月   9 14:54 cpu.shares</span><br><span class="line">-r--r--r--.  1 root root 0 7月   9 14:54 cpu.stat</span><br><span class="line">drwxr-xr-x.  3 root root 0 7月  10 12:34 docker</span><br><span class="line">-rw-r--r--.  1 root root 0 7月   9 14:54 notify_on_release</span><br><span class="line">-rw-r--r--.  1 root root 0 7月   9 14:54 release_agent</span><br><span class="line">drwxr-xr-x.  2 root root 0 7月  10 21:50 slions</span><br><span class="line">drwxr-xr-x. 69 root root 0 7月  10 12:04 system.slice</span><br><span class="line">-rw-r--r--.  1 root root 0 7月   9 14:54 tasks</span><br><span class="line">drwxr-xr-x.  2 root root 0 7月  10 11:52 user.slice</span><br><span class="line">[root@slions_pc1 cpu]# ll slions/</span><br><span class="line">总用量 0</span><br><span class="line">-rw-rw-r--. 1 root root 0 7月  10 21:50 cgroup.clone_children</span><br><span class="line">--w--w----. 1 root root 0 7月  10 21:50 cgroup.event_control</span><br><span class="line">-rw-rw-r--. 1 root root 0 7月  10 21:50 cgroup.procs</span><br><span class="line">-r--r--r--. 1 root root 0 7月  10 21:50 cpuacct.stat</span><br><span class="line">-rw-rw-r--. 1 root root 0 7月  10 21:50 cpuacct.usage</span><br><span class="line">-r--r--r--. 1 root root 0 7月  10 21:50 cpuacct.usage_percpu</span><br><span class="line">-rw-rw-r--. 1 root root 0 7月  10 21:50 cpu.cfs_period_us</span><br><span class="line">-rw-rw-r--. 1 root root 0 7月  10 21:50 cpu.cfs_quota_us</span><br><span class="line">-rw-rw-r--. 1 root root 0 7月  10 21:50 cpu.rt_period_us</span><br><span class="line">-rw-rw-r--. 1 root root 0 7月  10 21:50 cpu.rt_runtime_us</span><br><span class="line">-rw-rw-r--. 1 root root 0 7月  10 21:50 cpu.shares</span><br><span class="line">-r--r--r--. 1 root root 0 7月  10 21:50 cpu.stat</span><br><span class="line">-rw-rw-r--. 1 root root 0 7月  10 21:50 notify_on_release</span><br><span class="line">-rw-rw-r--. 1 root root 0 7月  10 21:50 tasks</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>会发现/sys/fs/cgroup/cpu目录下已经多了一个slions的目录，并且自动创建了一些文件，其中task文件就是记录进程pid的地方，换句话说，只要是在此task中的进程都会被此控制组所限制。</p><p>接着写一个死循环后台跑着，获得进程号为44577，执行top看到cpu使用率为100%</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">[root@slions_pc1 cpu]# while :;do :;done &amp;</span><br><span class="line">[1] 44577</span><br><span class="line">[root@slions_pc1 ~]# top -p 44577</span><br><span class="line">top - 22:00:31 up 10:35,  3 users,  load average: 0.78, 0.27, 0.13</span><br><span class="line">Tasks:   1 total,   1 running,   0 sleeping,   0 stopped,   0 zombie</span><br><span class="line">%Cpu(s): 50.1 us,  0.0 sy,  0.0 ni, 49.9 id,  0.0 wa,  0.0 hi,  0.0 si,  0.0 st</span><br><span class="line">KiB Mem :  3861520 total,  1600076 free,   391708 used,  1869736 buff/cache</span><br><span class="line">KiB Swap:  1044476 total,  1044476 free,        0 used.  3067128 avail Mem</span><br><span class="line"></span><br><span class="line">   PID USER      PR  NI    VIRT    RES    SHR S  %CPU %MEM     TIME+ COMMAND</span><br><span class="line"> 44577 root      20   0  116296   1284    132 R   100  0.0   1:33.98 bash</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>我们把此进程号写入slions/tasks中</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">[root@slions_pc1 cpu]# echo 44577 &gt; slions/tasks</span><br><span class="line">[root@slions_pc1 cpu]# cat slions/tasks</span><br><span class="line">44577</span><br><span class="line">[root@slions_pc1 cpu]# top -p 44577</span><br><span class="line">top - 22:05:02 up 10:39,  3 users,  load average: 0.99, 0.69, 0.35</span><br><span class="line">Tasks:   1 total,   1 running,   0 sleeping,   0 stopped,   0 zombie</span><br><span class="line"><span class="meta">%</span><span class="bash">Cpu(s): 50.2 us,  0.2 sy,  0.0 ni, 49.7 id,  0.0 wa,  0.0 hi,  0.0 si,  0.0 st</span></span><br><span class="line">KiB Mem :  3861520 total,  1598680 free,   393096 used,  1869744 buff/cache</span><br><span class="line">KiB Swap:  1044476 total,  1044476 free,        0 used.  3065736 avail Mem</span><br><span class="line"></span><br><span class="line">   PID USER      PR  NI    VIRT    RES    SHR S  %CPU %MEM     TIME+ COMMAND</span><br><span class="line"> 44577 root      20   0  116296   1284    132 R  99.7  0.0   6:04.97 bash</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>没有变化，这是因为默认没有任何限制。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[root@slions_pc1 cpu]# cat slions/cpu.cfs_quota_us</span><br><span class="line">-1</span><br><span class="line">[root@slions_pc1 cpu]# cat slions/cpu.cfs_period_us</span><br><span class="line">100000</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>接下来，我们可以通过修改这些文件的内容来设置限制。</p><p>向 slions组里的 cfs_quota_us 文件写入 30 ms（30000 us），它意味着在每 100 ms 的时间里，被该控制组限制的进程只能使用 30 ms 的 CPU 时间，也就是说这个进程只能使用到 30% 的 CPU 带宽。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">[root@slions_pc1 cpu]# echo 30000 &gt; slions/cpu.cfs_quota_us</span><br><span class="line">[root@slions_pc1 cpu]# top -p 44577</span><br><span class="line">top - 22:18:53 up 10:53,  3 users,  load average: 0.34, 0.79, 0.67</span><br><span class="line">Tasks:   1 total,   1 running,   0 sleeping,   0 stopped,   0 zombie</span><br><span class="line"><span class="meta">%</span><span class="bash">Cpu(s): 15.0 us,  0.0 sy,  0.0 ni, 85.0 id,  0.0 wa,  0.0 hi,  0.0 si,  0.0 st</span></span><br><span class="line">KiB Mem :  3861520 total,  1599448 free,   392324 used,  1869748 buff/cache</span><br><span class="line">KiB Swap:  1044476 total,  1044476 free,        0 used.  3066508 avail Mem</span><br><span class="line"></span><br><span class="line">   PID USER      PR  NI    VIRT    RES    SHR S  %CPU %MEM     TIME+ COMMAND</span><br><span class="line"> 44577 root      20   0  116296   1284    132 R  30.0  0.0  18:50.61 bash</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>我们可以再起一个死循环进程，并且放入slions控制组。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">[root@slions_pc1 cpu]# while :;do :;done &amp;</span><br><span class="line">[2] 44658</span><br><span class="line">[root@slions_pc1 cpu]# echo 44658 &gt;&gt; slions/tasks</span><br><span class="line">[root@slions_pc1 cpu]# cat slions/tasks</span><br><span class="line">44577</span><br><span class="line">44658</span><br><span class="line">[root@slions_pc1 cpu]#top</span><br><span class="line">top - 22:22:49 up 10:57,  3 users,  load average: 0.08, 0.43, 0.55</span><br><span class="line">Tasks: 129 total,   3 running, 126 sleeping,   0 stopped,   0 zombie</span><br><span class="line"><span class="meta">%</span><span class="bash">Cpu(s): 15.3 us,  0.2 sy,  0.0 ni, 84.6 id,  0.0 wa,  0.0 hi,  0.0 si,  0.0 st</span></span><br><span class="line">KiB Mem :  3861520 total,  1598784 free,   393092 used,  1869644 buff/cache</span><br><span class="line">KiB Swap:  1044476 total,  1044476 free,        0 used.  3065852 avail Mem</span><br><span class="line"></span><br><span class="line">   PID USER      PR  NI    VIRT    RES    SHR S  %CPU %MEM     TIME+ COMMAND</span><br><span class="line"> 44577 root      20   0  116296   1284    132 R  15.3  0.0  19:45.59 bash</span><br><span class="line"> 44658 root      20   0  116296   1464    312 R  15.3  0.0   0:42.83 bash</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>可以看到，这两个进程各占了15%，加起来正好是我们的cfs_quota_us 限制。</p><h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><p>Linux Cgroups 的设计还是比较易用的，简单粗暴地理解呢，它就是一个子系统目录加上一组资源限制文件的组合。而对于 Docker 等 Linux 容器项目来说，它们只需要在每个子系统下面，为每个容器创建一个控制组（即创建一个新目录），然后在启动容器进程之后，把这个进程的 PID 填写到对应控制组的 tasks 文件中就可以了。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;此前已经聊了容器隔离技术&lt;code&gt;namespace&lt;/code&gt;,这次我们接着来看看如何来限制容器的资源。&lt;/p&gt;
&lt;h1 id=&quot;什么是Cgroup&quot;&gt;&lt;a href=&quot;#什么是Cgroup&quot; class=&quot;headerlink&quot; title=&quot;什么是Cgroup&quot;&gt;</summary>
      
    
    
    
    
    <category term="docker" scheme="https://slions.github.io/tags/docker/"/>
    
  </entry>
  
  <entry>
    <title>大话docker</title>
    <link href="https://slions.github.io/2021/07/09/%E5%A4%A7%E8%AF%9Ddocker/"/>
    <id>https://slions.github.io/2021/07/09/%E5%A4%A7%E8%AF%9Ddocker/</id>
    <published>2021-07-09T09:10:41.000Z</published>
    <updated>2021-07-10T11:30:46.686Z</updated>
    
    <content type="html"><![CDATA[<h1 id="唠唠背景"><a href="#唠唠背景" class="headerlink" title="唠唠背景"></a>唠唠背景</h1><p>自我工作以来就一直听到周围人在谈docker和容器，记得真正第一次接触docker是17年在上海，我当时被外派到某大型外资硬件提供商做云计算工程师， 主要做的工作就是负责维护原有的openstack云平台和ceph集群，以及日常服务器的一些上下架和维护工作。因为我之前的项目中openstack都是以传统服务来部署的，日常运维什么的基本没啥问题，而这次面对的环境是容器化后的openstack(kolla实现)，也算是逼自己要好好看看docker这块儿的东西了，当时九州云属于kolla项目的主要贡献者，也和我们有很多合作关系，所以期间有很多不太理解的地方也是请教了相关的几位大佬。</p><p>18年离开了上海，入职了北京某家金融领域解决方案提供商，主要做的也一直是容器化相关的事，从docker到k8s，从慢慢摸索到推出适合本公司产品的定制化容器云平台，期间我会有很多时候需要向甲方以及公司同事介绍我们在做的事儿，以及用到的技术、行业的现状等等，借着最近时间充裕，打算写一篇关于介绍docker的文章，如果有幸被您看到，也请提出宝贵的意见，十分感谢。</p><blockquote><p>以下内容多属于普及性文字，不会涉及太多技术点，如果有时间会抽专门的章节来具体阐述。</p></blockquote><h1 id="什么是容器"><a href="#什么是容器" class="headerlink" title="什么是容器"></a>什么是容器</h1><blockquote><p>纵观整个IT界历史长河，一个新兴技术能够火爆与当时的市场痛点是息息相关的，就比如当初如日中天的AWS和Openstack提供了云计算的模式，使得万物云化，资源按需使用，大大的提高了服务器的资源利用率，降低了人力物力的成本。慢慢的人们发现，当申请好一批虚拟机后，通过手工或者自动化脚本部署自己的应用时会出现各种奇奇怪怪的问题，因为本地环境与云端虚拟机环境不一致，所以导致了大量的排错过程。后面PaaS理念的一经提出（应用托管），赢得了广大开发者的关注，而docker项目的发布为应用打包问题提供了一份近乎完美的解决方案。</p></blockquote><h2 id="容器与操作系统"><a href="#容器与操作系统" class="headerlink" title="容器与操作系统"></a>容器与操作系统</h2><p>我们经常把容器叫做docker,但其实容器还有coreOS的rkt。在介绍容器前，大家可以先想想操作系统是如何管理进程的，我们登录到一个操作系统内执行</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> ps -elf</span> </span><br></pre></td></tr></table></figure><p>可以看到各种进程，这些进程包括了系统自带的和用户部署的，并且进程间可以互相看到，也可以互相通信，这些进程共享着同一份的文件系统，意味着他们可以操作同一份文件，最后他们还使用着同样的系统资源。</p><p>上述这些特点会产生一系列的问题：</p><ul><li>具有高级权限的进程可以攻击其他进程</li><li>具有高级权限的进程可以对其他进程所需的文件增删改查</li><li>应用之间会存在资源争抢问题</li></ul><p>容器本质上是把系统中为同一个业务目标服务的相关进程合成一组，放在一个叫做<strong>namespace</strong>的空间中，同一个namespace中的进程能够互相通信，但看不见其他namespace中的进程。每个namespace可以拥有自己独立的主机名、进程ID系统、IPC、网络、文件系统、用户等等资源。使得容器这个父进程只对自己的子进程有感知，而对于宿主机其他进程互不感知。</p><p>linux提供了<strong>chroot</strong>的系统调用方式可以把一个子目录变为根目录，容器会在chroot的帮助下获得一个独立的文件系统，这样进程对文件系统的增删改查都不会影响到其他进程的使用。</p><p>此外，为了限制namespace对物理资源的使用，对进程能使用的CPU、内存、io等资源需要做一定的限制。这就是<strong>Cgroup</strong>技术。</p><p><strong>总结：</strong></p><p><strong>基于传统应用部署方式的痛点，docker通过linux的namespaces实现了资源视图的隔离，通过chroot来提供独立的文件系统，通过cgroup控制资源使用率。</strong></p><h2 id="容器与虚拟机"><a href="#容器与虚拟机" class="headerlink" title="容器与虚拟机"></a>容器与虚拟机</h2><p>提容器不得不提虚拟机，他们都是为应用提供封装和隔离的。</p><ul><li>虚拟化层：虚拟机有Hypervisor层和GuestOS，Docker省去了Hypervisor，其虚拟化技术是基于内核的Cgroup和Namespace技术，Docker通过libcontainer与内核交互，所以在很多方面，它的性能与物理机非常接近。</li><li>启动速度：docker启动快速属于秒级别。虚拟机通常需要几分钟去启动。</li><li>隔离性：docker属于进程之间的隔离，那么多个容器还是使用着同一个操作系统内核，所以docker的隔离性更弱，虚拟机可实现系统级别隔离。</li><li>迭代：虚拟机可以通过导出模板实现环境交付的一致性，但镜像分发无法体系化；Docker在Dockerfile中记录了容器构建过程，可在集群中实现快速分发和快速部署，快速回滚。</li><li>系统支持量：同样配置的服务器支持的容器数是虚拟机的十倍以上。</li></ul><p><img src="/doc_picture/docker-1.png" alt="image-20210709184759385"></p><h1 id="docker的优势"><a href="#docker的优势" class="headerlink" title="docker的优势"></a>docker的优势</h1><ul><li>更快速的交付和部署；开发者可以使用一个标准的镜像来构建一套开发容器，开发完成之后，运维人员可以直接使用这个容器来 部署代码。 Docker 可以快速创建容器，快速迭代应用程序，并让整个过程全程可见，使团队中的其他成员 更容易理解应用程序是如何创建和工作的。 Docker 容器很轻很快！容器的启动时间是秒级的，大量地节约 开发、测试、部署的时间。</li><li>更高效的虚拟化；不需要额外的hypervisor 支持，它是内核级的虚拟化，因此可以实现更高的性能和效率。</li><li>更轻松的迁移和扩展；Docker 容器几乎可以在任意的平台上运行，包括物理机、虚拟机、云平台、服务器等。 这种兼容性可以让用户把一个应用程序从一个平台直接迁移到另外一个。 </li><li>更简单的管理；使用 Docker，只需要小小的修改，就可以替代以往大量的更新工作。所有的修改都以增量的方式被分发和 更新，从而实现自动化并且高效的管理。 </li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;唠唠背景&quot;&gt;&lt;a href=&quot;#唠唠背景&quot; class=&quot;headerlink&quot; title=&quot;唠唠背景&quot;&gt;&lt;/a&gt;唠唠背景&lt;/h1&gt;&lt;p&gt;自我工作以来就一直听到周围人在谈docker和容器，记得真正第一次接触docker是17年在上海，我当时被外派到某大型外资硬</summary>
      
    
    
    
    
    <category term="docker" scheme="https://slions.github.io/tags/docker/"/>
    
  </entry>
  
  <entry>
    <title>安装docker</title>
    <link href="https://slions.github.io/2021/07/09/%E5%AE%89%E8%A3%85docker/"/>
    <id>https://slions.github.io/2021/07/09/%E5%AE%89%E8%A3%85docker/</id>
    <published>2021-07-09T06:52:04.000Z</published>
    <updated>2021-07-10T10:50:40.535Z</updated>
    
    <content type="html"><![CDATA[<h1 id="有网环境"><a href="#有网环境" class="headerlink" title="有网环境"></a>有网环境</h1><p>可联网的情况下，可以选择直接通过yum来安装指定版本的docker。</p><h2 id="验证环境"><a href="#验证环境" class="headerlink" title="验证环境"></a>验证环境</h2><p>当前我的环境可以联网，并且没有安装任何与docker有关的软件包。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">[root@slions_pc1 ~]# ping www.baidu.com</span><br><span class="line">PING www.a.shifen.com (220.181.38.150) 56(84) bytes of data.</span><br><span class="line">64 bytes from 220.181.38.150 (220.181.38.150): icmp_seq=1 ttl=128 time=6.33 ms</span><br><span class="line">64 bytes from 220.181.38.150 (220.181.38.150): icmp_seq=2 ttl=128 time=6.26 ms</span><br><span class="line">64 bytes from 220.181.38.150 (220.181.38.150): icmp_seq=3 ttl=128 time=5.65 ms</span><br><span class="line">64 bytes from 220.181.38.150 (220.181.38.150): icmp_seq=4 ttl=128 time=6.24 ms</span><br><span class="line">^C</span><br><span class="line">--- www.a.shifen.com ping statistics ---</span><br><span class="line">4 packets transmitted, 4 received, 0% packet loss, time 3007ms</span><br><span class="line">rtt min/avg/max/mdev = 5.656/6.125/6.333/0.272 ms</span><br><span class="line">[root@slions_pc1 ~]# yum list installed|grep docker</span><br><span class="line"></span><br></pre></td></tr></table></figure><h2 id="安装相关工具和配置"><a href="#安装相关工具和配置" class="headerlink" title="安装相关工具和配置"></a>安装相关工具和配置</h2><p>安装管理repository及扩展包的工具并添加阿里docker源 </p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@slions_pc1 ~]# yum install yum-utils -y &amp;&amp; yum-config-manager --add-repo https://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>生成缓存</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">[root@slions_pc1 ~]# yum makecache</span><br><span class="line">已加载插件：fastestmirror, langpacks</span><br><span class="line">Loading mirror speeds from cached hostfile</span><br><span class="line"> * base: mirrors.163.com</span><br><span class="line"> * extras: mirrors.huaweicloud.com</span><br><span class="line"> * updates: mirrors.huaweicloud.com</span><br><span class="line">base                                                                                                                                                              | 3.6 kB  00:00:00</span><br><span class="line">docker-ce-stable                                                                                                                                                  | 3.5 kB  00:00:00</span><br><span class="line">extras                                                                                                                                                            | 2.9 kB  00:00:00</span><br><span class="line">updates                                                                                                                                                           | 2.9 kB  00:00:00</span><br><span class="line">(1/10): docker-ce-stable/7/x86_64/updateinfo                                                                                                                      |   55 B  00:00:00</span><br><span class="line">(2/10): docker-ce-stable/7/x86_64/filelists_db                                                                                                                    |  26 kB  00:00:00</span><br><span class="line">(3/10): docker-ce-stable/7/x86_64/primary_db                                                                                                                      |  62 kB  00:00:00</span><br><span class="line">(4/10): base/7/x86_64/filelists_db                                                                                                                                | 7.2 MB  00:00:00</span><br><span class="line">(5/10): docker-ce-stable/7/x86_64/other_db                                                                                                                        | 119 kB  00:00:00</span><br><span class="line">(6/10): extras/7/x86_64/other_db                                                                                                                                  | 143 kB  00:00:00</span><br><span class="line">(7/10): updates/7/x86_64/filelists_db                                                                                                                             | 5.1 MB  00:00:00</span><br><span class="line">(8/10): extras/7/x86_64/filelists_db                                                                                                                              | 235 kB  00:00:00</span><br><span class="line">(9/10): updates/7/x86_64/other_db                                                                                                                                 | 681 kB  00:00:00</span><br><span class="line">(10/10): base/7/x86_64/other_db                                                                                                                                   | 2.6 MB  00:00:02</span><br><span class="line">元数据缓存已建立</span><br></pre></td></tr></table></figure><p>查看当前可供下载的docker版本</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line">[root@slions_pc1 ~]# yum list docker-ce --showduplicates | sort -r</span><br><span class="line">已加载插件：fastestmirror, langpacks</span><br><span class="line">可安装的软件包</span><br><span class="line"> * updates: mirrors.huaweicloud.com</span><br><span class="line">Loading mirror speeds from cached hostfile</span><br><span class="line"> * extras: mirrors.huaweicloud.com</span><br><span class="line">docker-ce.x86_64            3:20.10.7-3.el7                     docker-ce-stable</span><br><span class="line">docker-ce.x86_64            3:20.10.6-3.el7                     docker-ce-stable</span><br><span class="line">docker-ce.x86_64            3:20.10.5-3.el7                     docker-ce-stable</span><br><span class="line">docker-ce.x86_64            3:20.10.4-3.el7                     docker-ce-stable</span><br><span class="line">docker-ce.x86_64            3:20.10.3-3.el7                     docker-ce-stable</span><br><span class="line">docker-ce.x86_64            3:20.10.2-3.el7                     docker-ce-stable</span><br><span class="line">docker-ce.x86_64            3:20.10.1-3.el7                     docker-ce-stable</span><br><span class="line">docker-ce.x86_64            3:20.10.0-3.el7                     docker-ce-stable</span><br><span class="line">docker-ce.x86_64            3:19.03.9-3.el7                     docker-ce-stable</span><br><span class="line">docker-ce.x86_64            3:19.03.8-3.el7                     docker-ce-stable</span><br><span class="line">docker-ce.x86_64            3:19.03.7-3.el7                     docker-ce-stable</span><br><span class="line">docker-ce.x86_64            3:19.03.6-3.el7                     docker-ce-stable</span><br><span class="line">docker-ce.x86_64            3:19.03.5-3.el7                     docker-ce-stable</span><br><span class="line">docker-ce.x86_64            3:19.03.4-3.el7                     docker-ce-stable</span><br><span class="line">docker-ce.x86_64            3:19.03.3-3.el7                     docker-ce-stable</span><br><span class="line">docker-ce.x86_64            3:19.03.2-3.el7                     docker-ce-stable</span><br><span class="line">docker-ce.x86_64            3:19.03.15-3.el7                    docker-ce-stable</span><br><span class="line">docker-ce.x86_64            3:19.03.14-3.el7                    docker-ce-stable</span><br><span class="line">docker-ce.x86_64            3:19.03.1-3.el7                     docker-ce-stable</span><br><span class="line">docker-ce.x86_64            3:19.03.13-3.el7                    docker-ce-stable</span><br><span class="line">docker-ce.x86_64            3:19.03.12-3.el7                    docker-ce-stable</span><br><span class="line">docker-ce.x86_64            3:19.03.11-3.el7                    docker-ce-stable</span><br><span class="line">docker-ce.x86_64            3:19.03.10-3.el7                    docker-ce-stable</span><br><span class="line">docker-ce.x86_64            3:19.03.0-3.el7                     docker-ce-stable</span><br><span class="line">docker-ce.x86_64            3:18.09.9-3.el7                     docker-ce-stable</span><br><span class="line">docker-ce.x86_64            3:18.09.8-3.el7                     docker-ce-stable</span><br><span class="line">docker-ce.x86_64            3:18.09.7-3.el7                     docker-ce-stable</span><br><span class="line">docker-ce.x86_64            3:18.09.6-3.el7                     docker-ce-stable</span><br><span class="line">docker-ce.x86_64            3:18.09.5-3.el7                     docker-ce-stable</span><br><span class="line">docker-ce.x86_64            3:18.09.4-3.el7                     docker-ce-stable</span><br><span class="line">docker-ce.x86_64            3:18.09.3-3.el7                     docker-ce-stable</span><br><span class="line">docker-ce.x86_64            3:18.09.2-3.el7                     docker-ce-stable</span><br><span class="line">docker-ce.x86_64            3:18.09.1-3.el7                     docker-ce-stable</span><br><span class="line">docker-ce.x86_64            3:18.09.0-3.el7                     docker-ce-stable</span><br><span class="line">docker-ce.x86_64            18.06.3.ce-3.el7                    docker-ce-stable</span><br><span class="line">docker-ce.x86_64            18.06.2.ce-3.el7                    docker-ce-stable</span><br><span class="line">docker-ce.x86_64            18.06.1.ce-3.el7                    docker-ce-stable</span><br><span class="line">docker-ce.x86_64            18.06.0.ce-3.el7                    docker-ce-stable</span><br><span class="line">docker-ce.x86_64            18.03.1.ce-1.el7.centos             docker-ce-stable</span><br><span class="line">docker-ce.x86_64            18.03.0.ce-1.el7.centos             docker-ce-stable</span><br><span class="line">docker-ce.x86_64            17.12.1.ce-1.el7.centos             docker-ce-stable</span><br><span class="line">docker-ce.x86_64            17.12.0.ce-1.el7.centos             docker-ce-stable</span><br><span class="line">docker-ce.x86_64            17.09.1.ce-1.el7.centos             docker-ce-stable</span><br><span class="line">docker-ce.x86_64            17.09.0.ce-1.el7.centos             docker-ce-stable</span><br><span class="line">...</span><br></pre></td></tr></table></figure><h2 id="安装指定版本"><a href="#安装指定版本" class="headerlink" title="安装指定版本"></a>安装指定版本</h2><p>选择指定版本进行安装即可，填入版本号到下面的命令（如：docker-ce-18.09.3-3.el7）</p><blockquote><p>#yum install docker-ce- &lt;$VERSION_STRING&gt;  docker-ce-cli- &lt;$VERSION_STRING&gt;  containerd.io</p></blockquote><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@slions_pc1 ~]# yum install docker-ce-18.09.3-3.el7 docker-ce-cli-18.09.3-3.el7 containerd.io -y</span><br></pre></td></tr></table></figure><h1 id="无网环境"><a href="#无网环境" class="headerlink" title="无网环境"></a>无网环境</h1><p>无网的环境下，最简单的方式是我们提前将指定版本的docker包下载到本地，后续传到机器上解压，将可执行命令移动到软件运行目录即可</p><p>docker二进制包地址： <a href="https://download.docker.com/linux/static/stable/x86_64/">https://download.docker.com/linux/static/stable/x86_64/</a></p><h2 id="下载二进制包"><a href="#下载二进制包" class="headerlink" title="下载二进制包"></a>下载二进制包</h2><p>比如我们想安装18.09.3版本的docker,则提前下载到本地</p><p><img src="/doc_picture/docker-tar1.png" alt="image-20210709154417198"></p><h2 id="上传和配置"><a href="#上传和配置" class="headerlink" title="上传和配置"></a>上传和配置</h2><p>将软件包上传于所需机器的任意目录</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[root@slions_pc1 ~]# pwd</span><br><span class="line">/root</span><br><span class="line">[root@slions_pc1 ~]# ls</span><br><span class="line">anaconda-ks.cfg  docker-18.09.3.tgz</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>解压此压缩包，并将可执行文件都拷贝到程序执行目录</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">[root@slions_pc1 ~]# tar zxvf docker-18.09.3.tgz</span><br><span class="line">docker/</span><br><span class="line">docker/ctr</span><br><span class="line">docker/containerd-shim</span><br><span class="line">docker/containerd</span><br><span class="line">docker/docker-proxy</span><br><span class="line">docker/docker</span><br><span class="line">docker/dockerd</span><br><span class="line">docker/runc</span><br><span class="line">docker/docker-init</span><br><span class="line">[root@slions_pc1 ~]# ls</span><br><span class="line">anaconda-ks.cfg  docker  docker-18.09.3.tgz</span><br><span class="line">[root@slions_pc1 ~]# cp docker/* /usr/bin/</span><br></pre></td></tr></table></figure><p>添加docker服务启动文件即可</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;有网环境&quot;&gt;&lt;a href=&quot;#有网环境&quot; class=&quot;headerlink&quot; title=&quot;有网环境&quot;&gt;&lt;/a&gt;有网环境&lt;/h1&gt;&lt;p&gt;可联网的情况下，可以选择直接通过yum来安装指定版本的docker。&lt;/p&gt;
&lt;h2 id=&quot;验证环境&quot;&gt;&lt;a href=&quot;</summary>
      
    
    
    
    
    <category term="docker" scheme="https://slions.github.io/tags/docker/"/>
    
  </entry>
  
  <entry>
    <title>聊聊namespace</title>
    <link href="https://slions.github.io/2021/07/09/%E8%81%8A%E8%81%8Anamespace/"/>
    <id>https://slions.github.io/2021/07/09/%E8%81%8A%E8%81%8Anamespace/</id>
    <published>2021-07-09T06:49:12.000Z</published>
    <updated>2021-07-10T11:03:05.000Z</updated>
    
    <content type="html"><![CDATA[<p>我们谈及docker，都知道docker容器本质上是宿主机的进程，Docker通过namespace实现了资源隔离，通过cgroups实现了资源限制，接下来先聊聊namespaces是怎么一回事。</p><h1 id="namespace是什么"><a href="#namespace是什么" class="headerlink" title="namespace是什么"></a>namespace是什么</h1><blockquote><p>Namespace是将内核的全局资源做封装，使得每个namespace都有一份独立的资源，因此不同的进程在各自的namespace中对同一种资源的使用不会互相干扰。</p><p> ——摘自《Docker进阶与实战》</p></blockquote><p>举个例子，执行sethostname这个系统调用时，可以改变系统的主机名，这个主机名就是一个内核的全局资源，内核通过UTS Namespace，可以将不同的进程分隔在不同的UTS namespace中，在某个Namespace修改主机名时，另一个Namespace中的主机名还是保持不变。</p><p> 说白了docker就是通过namespace使得进程在资源视图上实现隔离，目前linux共实现了6种namespace</p><p><img src="/doc_picture/docker-2.png" alt="image-20210710005510508"></p><p>接下来进行些实验来更好的理解namespace是如何进行隔离的。</p><h1 id="namespace实验"><a href="#namespace实验" class="headerlink" title="namespace实验"></a>namespace实验</h1><p>打开一台linux服务器，我们知道进程相关的信息都会在/proc目录下，如下图，/proc下的每个数字就代表了linux系统中的一个进程。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">[root@slions_pc1 ~]# ls /proc/</span><br><span class="line">1      10149  10683  11     18  26    3002  36    4317  4331  5     8035  8806  8833  9631       cmdline      fb           keys        misc          schedstat      sysvipc</span><br><span class="line">10     10150  10700  11097  19  27    3006  37    4321  4400  51    8042  8813  8836  9639       consoles     filesystems  key-users   modules       scsi           timer_list</span><br><span class="line">10019  10152  10704  11122  2   2722  3060  38    4322  4413  53    8052  8814  8864  98         cpuinfo      fs           kmsg        mounts        self           timer_stats</span><br><span class="line">10023  10164  10706  11124  20  28    3074  4271  4325  4415  66    8059  8815  8876  9926       crypto       interrupts   kpagecount  mpt           slabinfo       tty</span><br><span class="line">10025  10581  10741  12     21  29    3087  4273  4326  4423  7     8069  8816  8925  acpi       devices      iomem        kpageflags  mtrr          softirqs       uptime</span><br><span class="line">10060  10582  10865  13     22  2909  3095  4283  4327  46    8     8795  8817  9     asound     diskstats    ioports      loadavg     net           stat           version</span><br><span class="line">10093  10586  10912  14     23  2911  3099  4286  4328  47    8017  8796  8820  9407  buddyinfo  dma          irq          locks       pagetypeinfo  swaps          vmallocinfo</span><br><span class="line">10100  10588  10924  15     24  2947  3104  4307  4329  48    8024  8797  8821  9408  bus        driver       kallsyms     mdstat      partitions    sys            vmstat</span><br><span class="line">10148  10628  10940  16     25  3     35    4313  4330  49    8029  8801  8830  9411  cgroups    execdomains  kcore        meminfo     sched_debug   sysrq-trigger  zoneinfo</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>每个数字目录下的ns目录就代表这个进程的namespace。</p><p><img src="/doc_picture/docker-4.png" alt="image-20210710121836489"></p><p>我们随便选取2个进程，查看其ns目录中的文件可以发现里面就对应着上边讲的linux 的6种namespace，每一项 namespace 都附带一个编号，这是唯一标识 namespace 的，如果两个进程指向的 namespace 编号相同，则表示它们同在该 namespace 下。可以看到进程为1和10的各种namespace编号都一样，所以它们同属一个namespace下。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">[root@slions_pc1 ~]# ll /proc/1/ns/</span><br><span class="line">总用量 0</span><br><span class="line">lrwxrwxrwx. 1 root root 0 7月  10 12:01 ipc -&gt; ipc:[4026531839]</span><br><span class="line">lrwxrwxrwx. 1 root root 0 7月  10 12:01 mnt -&gt; mnt:[4026531840]</span><br><span class="line">lrwxrwxrwx. 1 root root 0 7月  10 12:01 net -&gt; net:[4026531956]</span><br><span class="line">lrwxrwxrwx. 1 root root 0 7月  10 12:01 pid -&gt; pid:[4026531836]</span><br><span class="line">lrwxrwxrwx. 1 root root 0 7月  10 12:01 user -&gt; user:[4026531837]</span><br><span class="line">lrwxrwxrwx. 1 root root 0 7月  10 12:01 uts -&gt; uts:[4026531838]</span><br><span class="line">[root@slions_pc1 ~]# ll /proc/10/ns/</span><br><span class="line">总用量 0</span><br><span class="line">lrwxrwxrwx. 1 root root 0 7月  10 12:01 ipc -&gt; ipc:[4026531839]</span><br><span class="line">lrwxrwxrwx. 1 root root 0 7月  10 12:01 mnt -&gt; mnt:[4026531840]</span><br><span class="line">lrwxrwxrwx. 1 root root 0 7月  10 12:01 net -&gt; net:[4026531956]</span><br><span class="line">lrwxrwxrwx. 1 root root 0 7月  10 12:01 pid -&gt; pid:[4026531836]</span><br><span class="line">lrwxrwxrwx. 1 root root 0 7月  10 12:01 user -&gt; user:[4026531837]</span><br><span class="line">lrwxrwxrwx. 1 root root 0 7月  10 12:01 uts -&gt; uts:[4026531838]</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>现在我启动一个docker进行测试。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">[root@slions_pc1 ~]# docker run -d tomcat</span><br><span class="line">Unable to find image &#x27;tomcat:latest&#x27; locally</span><br><span class="line">latest: Pulling from library/tomcat</span><br><span class="line">0bc3020d05f1: Pull complete</span><br><span class="line">a110e5871660: Pull complete</span><br><span class="line">83d3c0fa203a: Pull complete</span><br><span class="line">a8fd09c11b02: Pull complete</span><br><span class="line">96ebf1506065: Pull complete</span><br><span class="line">b8bf70f9cc4d: Pull complete</span><br><span class="line">3f6da67b9e68: Pull complete</span><br><span class="line">257407776119: Pull complete</span><br><span class="line">7bd0a187fb92: Pull complete</span><br><span class="line">307fc4df04c9: Pull complete</span><br><span class="line">Digest: sha256:a5abf192aceed45620dbb2e09f8abdec2b96108b86365a988c85e753c28cd36b</span><br><span class="line">Status: Downloaded newer image for tomcat:latest</span><br><span class="line">78192daee11695b6f2d973e4998c4f7a84b855b84f3a88ebe1ed5653d499d934</span><br><span class="line">[root@slions_pc1 ~]# ps -elf|grep [t]omcat</span><br><span class="line">4 S root      42799  42780  1  80   0 - 894104 futex_ 12:34 ?       00:00:05 /usr/local/openjdk-11/bin/java -Djava.util.logging.config.file=/usr/local/tomcat/conf/logging.properties -Djava.util.logging.manager=org.apache.juli.ClassLoaderLogManager -Djdk.tls.ephemeralDHKeySize=2048 -Djava.protocol.handler.pkgs=org.apache.catalina.webresources -Dorg.apache.catalina.security.SecurityListener.UMASK=0027 -Dignore.endorsed.dirs= -classpath /usr/local/tomcat/bin/bootstrap.jar:/usr/local/tomcat/bin/tomcat-juli.jar -Dcatalina.base=/usr/local/tomcat -Dcatalina.home=/usr/local/tomcat -Djava.io.tmpdir=/usr/local/tomcat/temp org.apache.catalina.startup.Bootstrap start</span><br></pre></td></tr></table></figure><p>查看此docker进程的namespace信息 , 对比之前宿主机进程不难看出，它们已经进行了隔离。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">[root@slions_pc1 ~]# ll /proc/42799/ns</span><br><span class="line">总用量 0</span><br><span class="line">lrwxrwxrwx. 1 root root 0 7月  10 12:44 ipc -&gt; ipc:[4026532503]</span><br><span class="line">lrwxrwxrwx. 1 root root 0 7月  10 12:44 mnt -&gt; mnt:[4026532501]</span><br><span class="line">lrwxrwxrwx. 1 root root 0 7月  10 12:34 net -&gt; net:[4026532506]</span><br><span class="line">lrwxrwxrwx. 1 root root 0 7月  10 12:44 pid -&gt; pid:[4026532504]</span><br><span class="line">lrwxrwxrwx. 1 root root 0 7月  10 12:44 user -&gt; user:[4026531837]</span><br><span class="line">lrwxrwxrwx. 1 root root 0 7月  10 12:44 uts -&gt; uts:[4026532502]</span><br><span class="line">[root@slions_pc1 ~]# ll /proc/1/ns</span><br><span class="line">总用量 0</span><br><span class="line">lrwxrwxrwx. 1 root root 0 7月  10 12:01 ipc -&gt; ipc:[4026531839]</span><br><span class="line">lrwxrwxrwx. 1 root root 0 7月  10 12:01 mnt -&gt; mnt:[4026531840]</span><br><span class="line">lrwxrwxrwx. 1 root root 0 7月  10 12:01 net -&gt; net:[4026531956]</span><br><span class="line">lrwxrwxrwx. 1 root root 0 7月  10 12:01 pid -&gt; pid:[4026531836]</span><br><span class="line">lrwxrwxrwx. 1 root root 0 7月  10 12:01 user -&gt; user:[4026531837]</span><br><span class="line">lrwxrwxrwx. 1 root root 0 7月  10 12:01 uts -&gt; uts:[4026531838]</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>上述实验只是验证了一个docker启动后namespace的隔离性，显然对于namespace的理解还远远不够，我们接下来再看看自己如何创建一个namespace。</p><h1 id="创建namespace"><a href="#创建namespace" class="headerlink" title="创建namespace"></a>创建namespace</h1><blockquote><p>对namespace的操作主要是通过clone()，setns()，unshare()来实现的。</p><p>其中clone用来创建新的进程和namespace，unshare用来为已有的进程创建namespace，而setns会将已有的进程放置到已有的namespace</p></blockquote><p>通过调用 clone()，并传入需要隔离资源对应的参数flags(包括CLONE_NEWNS、CLONE_NEWPID、CLONE_NEWIPC、CLONE_NEWUTS、CLONE_NEWUSER、CLONE_NEWNET)，创建出的新进程就处在全新的namespace中了（隔离什么我们自己控制）。</p><p>我们先来看下clone()的用法，在机器上执行<code>man clone</code>找到例子。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br></pre></td><td class="code"><pre><span class="line">EXAMPLE</span><br><span class="line">       The following program demonstrates the use of clone() to create a child process that executes in a separate UTS namespace.  The child changes the hostname in its UTS namespace.</span><br><span class="line">       Both  parent  and  child then display the system hostname, making it possible to see that the hostname differs in the UTS namespaces of the parent and child.  For an example of</span><br><span class="line">       the use of this program, see setns(2).</span><br><span class="line"></span><br><span class="line">   Program source</span><br><span class="line">       #define _GNU_SOURCE</span><br><span class="line">       #include &lt;sys/wait.h&gt;</span><br><span class="line">       #include &lt;sys/utsname.h&gt;</span><br><span class="line">       #include &lt;sched.h&gt;</span><br><span class="line">       #include &lt;string.h&gt;</span><br><span class="line">       #include &lt;stdio.h&gt;</span><br><span class="line">       #include &lt;stdlib.h&gt;</span><br><span class="line">       #include &lt;unistd.h&gt;</span><br><span class="line"></span><br><span class="line">       #define errExit(msg)    do &#123; perror(msg); exit(EXIT_FAILURE); \</span><br><span class="line">                               &#125; while (0)</span><br><span class="line"></span><br><span class="line">       static int              /* Start function for cloned child */</span><br><span class="line">       childFunc(void *arg)</span><br><span class="line">       &#123;</span><br><span class="line">           struct utsname uts;</span><br><span class="line"></span><br><span class="line">           /* Change hostname in UTS namespace of child */</span><br><span class="line"></span><br><span class="line">           if (sethostname(arg, strlen(arg)) == -1)</span><br><span class="line">               errExit(&quot;sethostname&quot;);</span><br><span class="line"></span><br><span class="line">           /* Retrieve and display hostname */</span><br><span class="line"></span><br><span class="line">           if (uname(&amp;uts) == -1)</span><br><span class="line">               errExit(&quot;uname&quot;);</span><br><span class="line">           printf(&quot;uts.nodename in child:  %s\n&quot;, uts.nodename);</span><br><span class="line"></span><br><span class="line">           /* Keep the namespace open for a while, by sleeping.</span><br><span class="line">              This allows some experimentation--for example, another</span><br><span class="line">              process might join the namespace. */</span><br><span class="line"></span><br><span class="line">           sleep(200);</span><br><span class="line"></span><br><span class="line">           return 0;           /* Child terminates now */</span><br><span class="line">       &#125;</span><br><span class="line"></span><br><span class="line">       #define STACK_SIZE (1024 * 1024)    /* Stack size for cloned child */</span><br><span class="line"></span><br><span class="line">       int</span><br><span class="line">       main(int argc, char *argv[])</span><br><span class="line">       &#123;</span><br><span class="line">           char *stack;                    /* Start of stack buffer */</span><br><span class="line">           char *stackTop;                 /* End of stack buffer */</span><br><span class="line">           pid_t pid;</span><br><span class="line">           struct utsname uts;</span><br><span class="line"></span><br><span class="line">           if (argc &lt; 2) &#123;</span><br><span class="line">               fprintf(stderr, &quot;Usage: %s &lt;child-hostname&gt;\n&quot;, argv[0]);</span><br><span class="line">               exit(EXIT_SUCCESS);</span><br><span class="line">           &#125;</span><br><span class="line"></span><br><span class="line">           /* Allocate stack for child */</span><br><span class="line"></span><br><span class="line">           stack = malloc(STACK_SIZE);</span><br><span class="line">           if (stack == NULL)</span><br><span class="line">               errExit(&quot;malloc&quot;);</span><br><span class="line">           stackTop = stack + STACK_SIZE;  /* Assume stack grows downward */</span><br><span class="line"></span><br><span class="line">           /* Create child that has its own UTS namespace;</span><br><span class="line">              child commences execution in childFunc() */</span><br><span class="line"></span><br><span class="line">           pid = clone(childFunc, stackTop, CLONE_NEWUTS | SIGCHLD, argv[1]);</span><br><span class="line">           if (pid == -1)</span><br><span class="line">               errExit(&quot;clone&quot;);</span><br><span class="line">           printf(&quot;clone() returned %ld\n&quot;, (long) pid);</span><br><span class="line"></span><br><span class="line">           /* Parent falls through to here */</span><br><span class="line"></span><br><span class="line">           sleep(1);           /* Give child time to change its hostname */</span><br><span class="line"></span><br><span class="line">           /* Display hostname in parent&#x27;s UTS namespace. This will be</span><br><span class="line">              different from hostname in child&#x27;s UTS namespace. */</span><br><span class="line"></span><br><span class="line">           if (uname(&amp;uts) == -1)</span><br><span class="line">               errExit(&quot;uname&quot;);</span><br><span class="line">           printf(&quot;uts.nodename in parent: %s\n&quot;, uts.nodename);</span><br><span class="line"></span><br><span class="line">           if (waitpid(pid, NULL, 0) == -1)    /* Wait for child */</span><br><span class="line">               errExit(&quot;waitpid&quot;);</span><br><span class="line">           printf(&quot;child has terminated\n&quot;);</span><br><span class="line"></span><br><span class="line">           exit(EXIT_SUCCESS);</span><br><span class="line">       &#125;</span><br></pre></td></tr></table></figure><p>我们来参照例子写一个程序</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line">[root@slions_pc1 test_namespace]<span class="meta"># cat ns.c</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> _GNU_SOURCE</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;sys/types.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;sys/wait.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;stdio.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;sched.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;signal.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;unistd.h&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> STACK_SIZE (1024 * 1024)</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">static</span> <span class="keyword">char</span> child_stack[STACK_SIZE];</span><br><span class="line"><span class="keyword">char</span>* <span class="keyword">const</span> child_args[] = &#123;</span><br><span class="line">     <span class="string">&quot;/bin/bash&quot;</span>,</span><br><span class="line">     <span class="literal">NULL</span></span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">child_main</span><span class="params">(<span class="keyword">void</span>* args)</span> </span>&#123;</span><br><span class="line">     <span class="built_in">printf</span>(<span class="string">&quot;在子进程中!\n&quot;</span>);</span><br><span class="line">     execv(child_args[<span class="number">0</span>], child_args);</span><br><span class="line">     <span class="keyword">return</span> <span class="number">1</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span></span>&#123;</span><br><span class="line">     <span class="built_in">printf</span>(<span class="string">&quot;程序开始:\n&quot;</span>);</span><br><span class="line">     <span class="keyword">int</span> child_pid = clone(child_main, child_stack + STACK_SIZE, SIGCHLD, <span class="literal">NULL</span>);</span><br><span class="line">     waitpid(child_pid, <span class="literal">NULL</span>, <span class="number">0</span>);</span><br><span class="line">     <span class="built_in">printf</span>(<span class="string">&quot;已退出\n&quot;</span>);</span><br><span class="line">     <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>执行测试下：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">[root@slions_pc1 test_namespace]# gcc -Wall ns.c -o ns.o</span><br><span class="line">[root@slions_pc1 test_namespace]# ls</span><br><span class="line">ns.c  ns.o</span><br><span class="line">[root@slions_pc1 test_namespace]# ./ns.o</span><br><span class="line">程序开始:</span><br><span class="line">在子进程中!</span><br><span class="line">[root@slions_pc1 test_namespace]# exit</span><br><span class="line">exit</span><br><span class="line">已退出</span><br><span class="line"></span><br></pre></td></tr></table></figure><h2 id="UTS隔离测试"><a href="#UTS隔离测试" class="headerlink" title="UTS隔离测试"></a>UTS隔离测试</h2><p>修改代码，加入UTS隔离。</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line">[root@slions_pc1 test_namespace]<span class="meta"># cat ns.c</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> _GNU_SOURCE</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;sys/types.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;sys/wait.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;stdio.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;sched.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;signal.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;unistd.h&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> STACK_SIZE (1024 * 1024)</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">static</span> <span class="keyword">char</span> child_stack[STACK_SIZE];</span><br><span class="line"><span class="keyword">char</span>* <span class="keyword">const</span> child_args[] = &#123;</span><br><span class="line">     <span class="string">&quot;/bin/bash&quot;</span>,</span><br><span class="line">     <span class="literal">NULL</span></span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">child_main</span><span class="params">(<span class="keyword">void</span>* args)</span> </span>&#123;</span><br><span class="line">     <span class="built_in">printf</span>(<span class="string">&quot;在子进程中!\n&quot;</span>);</span><br><span class="line">     sethostname(<span class="string">&quot;uts&quot;</span>,<span class="number">12</span>);</span><br><span class="line">     execv(child_args[<span class="number">0</span>], child_args);</span><br><span class="line">     <span class="keyword">return</span> <span class="number">1</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span></span>&#123;</span><br><span class="line">     <span class="built_in">printf</span>(<span class="string">&quot;程序开始:\n&quot;</span>);</span><br><span class="line">     <span class="keyword">int</span> child_pid = clone(child_main, child_stack + STACK_SIZE, CLONE_NEWUTS | SIGCHLD, <span class="literal">NULL</span>);</span><br><span class="line">     waitpid(child_pid, <span class="literal">NULL</span>, <span class="number">0</span>);</span><br><span class="line">     <span class="built_in">printf</span>(<span class="string">&quot;已退出\n&quot;</span>);</span><br><span class="line">     <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br><span class="line">[root@slions_pc1 test_namespace]<span class="meta"># gcc -Wall ns.c -o uts.o</span></span><br><span class="line">[root@slions_pc1 test_namespace]<span class="meta"># ls</span></span><br><span class="line">ns.c  ns.o  uts.o</span><br><span class="line">[root@slions_pc1 test_namespace]# ./uts.o</span><br><span class="line">程序开始:</span><br><span class="line">在子进程中!</span><br><span class="line">[root@uts test_namespace]<span class="meta"># hostname</span></span><br><span class="line">uts</span><br><span class="line">[root@uts test_namespace]<span class="meta"># exit</span></span><br><span class="line"><span class="built_in">exit</span></span><br><span class="line">已退出</span><br><span class="line">[root@slions_pc1 test_namespace]<span class="meta"># hostname</span></span><br><span class="line">slions_pc1</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>可以看到，加了CLONE_NEWUTS flags使得新进程的主机名发生了改变，并且不会影响其他进程的主机名。</p><h2 id="PID隔离测试"><a href="#PID隔离测试" class="headerlink" title="PID隔离测试"></a>PID隔离测试</h2><p>下面再介绍下PID namespaces的实现，修改相关代码：</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line">[root@slions_pc1 test_namespace]<span class="meta"># cat ns.c</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> _GNU_SOURCE</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;sys/types.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;sys/wait.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;stdio.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;sched.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;signal.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;unistd.h&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> STACK_SIZE (1024 * 1024)</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">static</span> <span class="keyword">char</span> child_stack[STACK_SIZE];</span><br><span class="line"><span class="keyword">char</span>* <span class="keyword">const</span> child_args[] = &#123;</span><br><span class="line">     <span class="string">&quot;/bin/bash&quot;</span>,</span><br><span class="line">     <span class="literal">NULL</span></span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">child_main</span><span class="params">(<span class="keyword">void</span>* args)</span> </span>&#123;</span><br><span class="line">     <span class="built_in">printf</span>(<span class="string">&quot;在子进程中!\n&quot;</span>);</span><br><span class="line">     sethostname(<span class="string">&quot;pid&quot;</span>,<span class="number">12</span>);</span><br><span class="line">     execv(child_args[<span class="number">0</span>], child_args);</span><br><span class="line">     <span class="keyword">return</span> <span class="number">1</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span></span>&#123;</span><br><span class="line">     <span class="built_in">printf</span>(<span class="string">&quot;程序开始:\n&quot;</span>);</span><br><span class="line">     <span class="keyword">int</span> child_pid = clone(child_main, child_stack + STACK_SIZE, CLONE_NEWPID | CLONE_NEWUTS | SIGCHLD, <span class="literal">NULL</span>);</span><br><span class="line">     waitpid(child_pid, <span class="literal">NULL</span>, <span class="number">0</span>);</span><br><span class="line">     <span class="built_in">printf</span>(<span class="string">&quot;已退出\n&quot;</span>);</span><br><span class="line">     <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br><span class="line">[root@slions_pc1 test_namespace]<span class="meta"># gcc -Wall ns.c -o pid.o</span></span><br><span class="line">[root@slions_pc1 test_namespace]# ^C</span><br><span class="line">[root@slions_pc1 test_namespace]<span class="meta"># echo $$</span></span><br><span class="line"><span class="number">10588</span></span><br><span class="line">[root@slions_pc1 test_namespace]<span class="meta"># gcc -Wall ns.c -o pid.o</span></span><br><span class="line">[root@slions_pc1 test_namespace]# ./pid.o</span><br><span class="line">程序开始:</span><br><span class="line">在子进程中!</span><br><span class="line">[root@pid test_namespace]<span class="meta"># echo $$</span></span><br><span class="line"><span class="number">1</span></span><br><span class="line">[root@pid test_namespace]<span class="meta"># exit</span></span><br><span class="line"><span class="built_in">exit</span></span><br><span class="line">已退出</span><br><span class="line">[root@slions_pc1 test_namespace]<span class="meta"># echo $$</span></span><br><span class="line"><span class="number">10588</span></span><br><span class="line"></span><br></pre></td></tr></table></figure><p>可以看到，此时新进程的pid成为了1，且与宿主机进程互相隔离。</p><p>PID namespace 隔离非常实用，它对进程的PID重新标号，即两个不同的namespace下的进程可以有相同的PID。每个PID namespace 都有自己的计算程序。内核为所有的PID namespace 维护了一个树状的结构，最顶层是系统初始化时创建的，被称为root namespace ，而它创建的新的PID namespace 被称为child namespace 树的子节点 而原来的PID namespace就是新建的namespace的父节点。通过这种方式，不同的PID namespace会形成一个层级结构，所属父节点可以看子节点中的进程，可以通过信号等手段对子节点中的进程产生影响，反之子节点无法看到父节点的 PID namespace 中任何内容。</p><p>Unix 系统中，PID为1 的为init ，它被称为所有进程的父进程，维护一张进程表，不断检查进程状态，一旦发现某子进程因为父进程错误称为孤儿进程init就会负责收养这个子进程并最终回收资源，结束进程，<strong>所以在要实现的容器中，启动第一个进程也要有实现类似init的功能，维护后续启动进程的运行状态</strong>。</p><p>细心的人可能会发现一个问题，执行完pid隔离后在新进程中执行ps aux命令会出现一堆进程，这些是宿主机的所有进程。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line">[root@slions_pc1 test_namespace]# ./pid.o</span><br><span class="line">程序开始:</span><br><span class="line">在子进程中!</span><br><span class="line">[root@pid test_namespace]# ps aux</span><br><span class="line">USER        PID %CPU %MEM    VSZ   RSS TTY      STAT START   TIME COMMAND</span><br><span class="line">root          1  0.0  0.1 193780  6828 ?        Ss   11:25   0:02 /usr/lib/systemd/systemd --switched-root --system --deserialize 22</span><br><span class="line">root          2  0.0  0.0      0     0 ?        S    11:25   0:00 [kthreadd]</span><br><span class="line">root          3  0.0  0.0      0     0 ?        S    11:25   0:02 [ksoftirqd/0]</span><br><span class="line">root          5  0.0  0.0      0     0 ?        S&lt;   11:25   0:00 [kworker/0:0H]</span><br><span class="line">root          7  0.0  0.0      0     0 ?        S    11:25   0:00 [migration/0]</span><br><span class="line">root          8  0.0  0.0      0     0 ?        S    11:25   0:00 [rcu_bh]</span><br><span class="line">root          9  0.0  0.0      0     0 ?        R    11:25   0:00 [rcu_sched]</span><br><span class="line">root         10  0.0  0.0      0     0 ?        S&lt;   11:25   0:00 [lru-add-drain]</span><br><span class="line">root         11  0.0  0.0      0     0 ?        S    11:25   0:00 [watchdog/0]</span><br><span class="line">root         12  0.0  0.0      0     0 ?        S    11:25   0:00 [watchdog/1]</span><br><span class="line">root         13  0.0  0.0      0     0 ?        S    11:25   0:00 [migration/1]</span><br><span class="line">root         14  0.0  0.0      0     0 ?        S    11:25   0:03 [ksoftirqd/1]</span><br><span class="line">root         16  0.0  0.0      0     0 ?        S&lt;   11:25   0:00 [kworker/1:0H]</span><br><span class="line">...</span><br><span class="line">[root@pid test_namespace]# ls /proc</span><br><span class="line">1      10628  11122  20    28    3074  42686  4313  4330   43504  49    8029  8801  8833  9631       cmdline      fb           keys        misc          schedstat      sysvipc</span><br><span class="line">10     10700  11153  21    29    3087  4271   4317  4331   43505  5     8035  8806  8836  9639       consoles     filesystems  key-users   modules       scsi           timer_list</span><br><span class="line">10148  10704  12     22    2909  3095  4273   4321  43334  43532  51    8042  8813  8864  98         cpuinfo      fs           kmsg        mounts        self           timer_stats</span><br><span class="line">10149  10706  13     23    2911  3099  42780  4322  43446  4400   53    8052  8814  8876  9926       crypto       interrupts   kpagecount  mpt           slabinfo       tty</span><br><span class="line">10150  10741  14     24    2947  3104  42799  4325  43466  4413   66    8059  8815  8925  acpi       devices      iomem        kpageflags  mtrr          softirqs       uptime</span><br><span class="line">10152  10865  16     25    3     35    4283   4326  43467  4415   7     8069  8816  9     asound     diskstats    ioports      loadavg     net           stat           version</span><br><span class="line">10582  10924  18     26    3002  36    4286   4327  43468  4423   8     8795  8817  9407  buddyinfo  dma          irq          locks       pagetypeinfo  swaps          vmallocinfo</span><br><span class="line">10586  10940  19     27    3006  37    42922  4328  43484  46     8017  8796  8820  9408  bus        driver       kallsyms     mdstat      partitions    sys            vmstat</span><br><span class="line">10588  11     2      2722  3060  38    4307   4329  43503  48     8024  8797  8821  9411  cgroups    execdomains  kcore        meminfo     sched_debug   sysrq-trigger  zoneinfo</span><br><span class="line">[root@pid test_namespace]# kill -9 10940</span><br><span class="line">bash: kill: (10940) - 没有那个进程</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>这是因为ps命令是从procfs读取数据的，而此时procfs并没有得到隔离，虽然能看到这些进程，但是它们其实是在另一个PID Namespace中，因此无法向这些进程发送信号。</p><p>那么我们想让宿主机和新进程的/proc互相隔离如何做呢，使用mount隔离是不是就好了呢？</p><h2 id="MOUNT隔离测试"><a href="#MOUNT隔离测试" class="headerlink" title="MOUNT隔离测试"></a>MOUNT隔离测试</h2><p>mount namespace 通过隔离文件系统挂载点对隔离文件系统提供支持，隔离后，不同mount namespace 中的文件结构发生变化也互不影响。可以通过/proc/[pid]/mounts 查看到所在namespace中文件设备统计信息，包括挂载文件的名、文件系统类型、挂载位置等。</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br></pre></td><td class="code"><pre><span class="line">[root@slions_pc1 test_namespace]<span class="meta"># cat ns.c</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> _GNU_SOURCE</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;sys/types.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;sys/wait.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;stdio.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;sched.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;signal.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;unistd.h&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> STACK_SIZE (1024 * 1024)</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">static</span> <span class="keyword">char</span> child_stack[STACK_SIZE];</span><br><span class="line"><span class="keyword">char</span>* <span class="keyword">const</span> child_args[] = &#123;</span><br><span class="line">     <span class="string">&quot;/bin/bash&quot;</span>,</span><br><span class="line">     <span class="literal">NULL</span></span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">child_main</span><span class="params">(<span class="keyword">void</span>* args)</span> </span>&#123;</span><br><span class="line">     <span class="built_in">printf</span>(<span class="string">&quot;在子进程中!\n&quot;</span>);</span><br><span class="line">     sethostname(<span class="string">&quot;pid&quot;</span>,<span class="number">12</span>);</span><br><span class="line">     execv(child_args[<span class="number">0</span>], child_args);</span><br><span class="line">     <span class="keyword">return</span> <span class="number">1</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span></span>&#123;</span><br><span class="line">     <span class="built_in">printf</span>(<span class="string">&quot;程序开始:\n&quot;</span>);</span><br><span class="line">     <span class="keyword">int</span> child_pid = clone(child_main, child_stack + STACK_SIZE, CLONE_NEWNS | CLONE_NEWPID | CLONE_NEWUTS | SIGCHLD, <span class="literal">NULL</span>);</span><br><span class="line">     waitpid(child_pid, <span class="literal">NULL</span>, <span class="number">0</span>);</span><br><span class="line">     <span class="built_in">printf</span>(<span class="string">&quot;已退出\n&quot;</span>);</span><br><span class="line">     <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br><span class="line">[root@slions_pc1 test_namespace]<span class="meta"># gcc -Wall ns.c -o mnt.o</span></span><br><span class="line">[root@slions_pc1 test_namespace]# ./mnt.o</span><br><span class="line">程序开始:</span><br><span class="line">在子进程中!</span><br><span class="line">[root@pid test_namespace]<span class="meta"># echo $$</span></span><br><span class="line"><span class="number">1</span></span><br><span class="line">[root@pid test_namespace]<span class="meta"># ps -elf |head -15</span></span><br><span class="line">F S UID         PID   PPID  C PRI  NI ADDR SZ WCHAN  STIME TTY          TIME CMD</span><br><span class="line"><span class="number">4</span> S root          <span class="number">1</span>      <span class="number">0</span>  <span class="number">0</span>  <span class="number">80</span>   <span class="number">0</span> - <span class="number">48445</span> ep_pol <span class="number">11</span>:<span class="number">25</span> ?        <span class="number">00</span>:<span class="number">00</span>:<span class="number">02</span> /usr/lib/systemd/systemd --switched-root --system --deserialize <span class="number">22</span></span><br><span class="line"><span class="number">1</span> S root          <span class="number">2</span>      <span class="number">0</span>  <span class="number">0</span>  <span class="number">80</span>   <span class="number">0</span> -     <span class="number">0</span> kthrea <span class="number">11</span>:<span class="number">25</span> ?        <span class="number">00</span>:<span class="number">00</span>:<span class="number">00</span> [kthreadd]</span><br><span class="line"><span class="number">1</span> S root          <span class="number">3</span>      <span class="number">2</span>  <span class="number">0</span>  <span class="number">80</span>   <span class="number">0</span> -     <span class="number">0</span> smpboo <span class="number">11</span>:<span class="number">25</span> ?        <span class="number">00</span>:<span class="number">00</span>:<span class="number">02</span> [ksoftirqd/<span class="number">0</span>]</span><br><span class="line"><span class="number">1</span> S root          <span class="number">5</span>      <span class="number">2</span>  <span class="number">0</span>  <span class="number">60</span> <span class="number">-20</span> -     <span class="number">0</span> worker <span class="number">11</span>:<span class="number">25</span> ?        <span class="number">00</span>:<span class="number">00</span>:<span class="number">00</span> [kworker/<span class="number">0</span>:<span class="number">0</span>H]</span><br><span class="line"><span class="number">1</span> S root          <span class="number">7</span>      <span class="number">2</span>  <span class="number">0</span> <span class="number">-40</span>   - -     <span class="number">0</span> smpboo <span class="number">11</span>:<span class="number">25</span> ?        <span class="number">00</span>:<span class="number">00</span>:<span class="number">00</span> [migration/<span class="number">0</span>]</span><br><span class="line"><span class="number">1</span> S root          <span class="number">8</span>      <span class="number">2</span>  <span class="number">0</span>  <span class="number">80</span>   <span class="number">0</span> -     <span class="number">0</span> rcu_gp <span class="number">11</span>:<span class="number">25</span> ?        <span class="number">00</span>:<span class="number">00</span>:<span class="number">00</span> [rcu_bh]</span><br><span class="line"><span class="number">1</span> S root          <span class="number">9</span>      <span class="number">2</span>  <span class="number">0</span>  <span class="number">80</span>   <span class="number">0</span> -     <span class="number">0</span> rcu_gp <span class="number">11</span>:<span class="number">25</span> ?        <span class="number">00</span>:<span class="number">00</span>:<span class="number">00</span> [rcu_sched]</span><br><span class="line"><span class="number">1</span> S root         <span class="number">10</span>      <span class="number">2</span>  <span class="number">0</span>  <span class="number">60</span> <span class="number">-20</span> -     <span class="number">0</span> rescue <span class="number">11</span>:<span class="number">25</span> ?        <span class="number">00</span>:<span class="number">00</span>:<span class="number">00</span> [lru-add-drain]</span><br><span class="line"><span class="number">5</span> S root         <span class="number">11</span>      <span class="number">2</span>  <span class="number">0</span> <span class="number">-40</span>   - -     <span class="number">0</span> smpboo <span class="number">11</span>:<span class="number">25</span> ?        <span class="number">00</span>:<span class="number">00</span>:<span class="number">00</span> [watchdog/<span class="number">0</span>]</span><br><span class="line"><span class="number">5</span> S root         <span class="number">12</span>      <span class="number">2</span>  <span class="number">0</span> <span class="number">-40</span>   - -     <span class="number">0</span> smpboo <span class="number">11</span>:<span class="number">25</span> ?        <span class="number">00</span>:<span class="number">00</span>:<span class="number">00</span> [watchdog/<span class="number">1</span>]</span><br><span class="line"><span class="number">1</span> S root         <span class="number">13</span>      <span class="number">2</span>  <span class="number">0</span> <span class="number">-40</span>   - -     <span class="number">0</span> smpboo <span class="number">11</span>:<span class="number">25</span> ?        <span class="number">00</span>:<span class="number">00</span>:<span class="number">00</span> [migration/<span class="number">1</span>]</span><br><span class="line"><span class="number">1</span> S root         <span class="number">14</span>      <span class="number">2</span>  <span class="number">0</span>  <span class="number">80</span>   <span class="number">0</span> -     <span class="number">0</span> smpboo <span class="number">11</span>:<span class="number">25</span> ?        <span class="number">00</span>:<span class="number">00</span>:<span class="number">03</span> [ksoftirqd/<span class="number">1</span>]</span><br><span class="line"><span class="number">1</span> S root         <span class="number">16</span>      <span class="number">2</span>  <span class="number">0</span>  <span class="number">60</span> <span class="number">-20</span> -     <span class="number">0</span> worker <span class="number">11</span>:<span class="number">25</span> ?        <span class="number">00</span>:<span class="number">00</span>:<span class="number">00</span> [kworker/<span class="number">1</span>:<span class="number">0</span>H]</span><br><span class="line"><span class="number">5</span> S root         <span class="number">18</span>      <span class="number">2</span>  <span class="number">0</span>  <span class="number">80</span>   <span class="number">0</span> -     <span class="number">0</span> devtmp <span class="number">11</span>:<span class="number">25</span> ?        <span class="number">00</span>:<span class="number">00</span>:<span class="number">00</span> [kdevtmpfs]</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>加上CLONE_NEWNS后发现还是可以看到宿主机的/proc信息，前面说了，进程在创建mount namespace时， 会把当前的文件系统结构复制给新的namespace 。新的namespace中所有mount 操作都只影响自身的文件系统，对外界不产生任何影响。这种做法非常严格的实现了隔离，但对某些情况可能并不适用。</p><p>熟悉mount命令的可以知道，mount支持了多种挂载模式</p><table><thead><tr><th>名称</th><th></th></tr></thead><tbody><tr><td>共享挂载</td><td>传播事件的挂载对象</td></tr><tr><td>从属挂载</td><td>接受传播事件的挂载对象</td></tr><tr><td>共享/从属挂载</td><td>即具备传播事件也具备接受传播事件的挂载对象</td></tr><tr><td>私有挂载</td><td>既不传播也不接受事件的挂载对象</td></tr><tr><td>不可绑定挂载</td><td>另一种特殊挂载对象，他们与私有挂载相似，但不允许执行绑定挂载</td></tr></tbody></table><p> <img src="/doc_picture/docker-3.png" alt="img"></p><p>最上一层的mount namespace 下的 /bin 目录 与 child namespace 通过master slave 方式进行挂载传播，当mount namespace中的/bin 目录发生变化时，发生的挂载事件能够自动传播到 child namespace中；/lib 目录使用完全共享挂载，各 namespace 之间发生变化时都会影响；proc 目录使用私有挂载传播方式，各namespace之间互相隔离；最后/root目录一般管理员所有，不能让其他namespace挂载绑定。</p><p>了解了上述，我们可以通过在这个PID namespace里挂载procfs来解决。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">[root@slions_pc1 test_namespace]# ./mnt.o</span><br><span class="line">程序开始:</span><br><span class="line">在子进程中!</span><br><span class="line">[root@pid test_namespace]# ls /proc</span><br><span class="line">1      10628  11122  20    28    3074  42686  4313  4330   43805  49    8029  8801  8833  9631       cmdline      fb           keys        misc          schedstat      sysvipc</span><br><span class="line">10     10700  11153  21    29    3087  4271   4317  4331   43806  5     8035  8806  8836  9639       consoles     filesystems  key-users   modules       scsi           timer_list</span><br><span class="line">10148  10704  12     22    2909  3095  4273   4321  43468  43833  51    8042  8813  8864  98         cpuinfo      fs           kmsg        mounts        self           timer_stats</span><br><span class="line">10149  10706  13     23    2911  3099  42780  4322  43704  4400   53    8052  8814  8876  9926       crypto       interrupts   kpagecount  mpt           slabinfo       tty</span><br><span class="line">10150  10741  14     24    2947  3104  42799  4325  43713  4413   66    8059  8815  8925  acpi       devices      iomem        kpageflags  mtrr          softirqs       uptime</span><br><span class="line">10152  10865  16     25    3     35    4283   4326  43739  4415   7     8069  8816  9     asound     diskstats    ioports      loadavg     net           stat           version</span><br><span class="line">10582  10924  18     26    3002  36    4286   4327  43741  4423   8     8795  8817  9407  buddyinfo  dma          irq          locks       pagetypeinfo  swaps          vmallocinfo</span><br><span class="line">10586  10940  19     27    3006  37    42922  4328  43761  46     8017  8796  8820  9408  bus        driver       kallsyms     mdstat      partitions    sys            vmstat</span><br><span class="line">10588  11     2      2722  3060  38    4307   4329  43782  48     8024  8797  8821  9411  cgroups    execdomains  kcore        meminfo     sched_debug   sysrq-trigger  zoneinfo</span><br><span class="line">[root@pid test_namespace]# mount --make-private -t proc proc /proc/</span><br><span class="line">[root@pid test_namespace]# ls /proc</span><br><span class="line">1       buddyinfo  consoles  diskstats    fb           iomem     kcore      kpagecount  mdstat   mounts  pagetypeinfo  scsi      stat           sysvipc      uptime       zoneinfo</span><br><span class="line">30      bus        cpuinfo   dma          filesystems  ioports   keys       kpageflags  meminfo  mpt     partitions    self      swaps          timer_list   version</span><br><span class="line">acpi    cgroups    crypto    driver       fs           irq       key-users  loadavg     misc     mtrr    sched_debug   slabinfo  sys            timer_stats  vmallocinfo</span><br><span class="line">asound  cmdline    devices   execdomains  interrupts   kallsyms  kmsg       locks       modules  net     schedstat     softirqs  sysrq-trigger  tty          vmstat</span><br><span class="line">[root@pid test_namespace]# exit</span><br><span class="line">exit</span><br><span class="line">已退出</span><br><span class="line">[root@slions_pc1 test_namespace]# ls /proc</span><br><span class="line">ls: 无法读取符号链接/proc/self: 没有那个文件或目录</span><br><span class="line">acpi       cgroups   crypto     driver       fs          irq       key-users   loadavg  misc     mtrr          sched_debug  slabinfo  sys            timer_stats  vmallocinfo</span><br><span class="line">asound     cmdline   devices    execdomains  interrupts  kallsyms  kmsg        locks    modules  net           schedstat    softirqs  sysrq-trigger  tty          vmstat</span><br><span class="line">buddyinfo  consoles  diskstats  fb           iomem       kcore     kpagecount  mdstat   mounts   pagetypeinfo  scsi         stat      sysvipc        uptime       zoneinfo</span><br><span class="line">bus        cpuinfo   dma        filesystems  ioports     keys      kpageflags  meminfo  mpt      partitions    self         swaps     timer_list     version</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>此时可以看到子进程内的/proc下只有自己namespace下的进程信息了，但是由于文件系统挂载点没有隔离，宿主机看到的procfs也会是这个新的procfs，所以我们应该先在宿主机执行<code>mount --make-private -t proc proc /proc/</code></p><h2 id="IPC、USER、NET隔离"><a href="#IPC、USER、NET隔离" class="headerlink" title="IPC、USER、NET隔离"></a>IPC、USER、NET隔离</h2><p>具体的测试步骤同上，不展开说了，感兴趣的可以自行测试下。</p><p><strong>IPC:</strong></p><p>进程间通信(Inter-Process Communication，IPC)涉及的IPC资源包括常见的信号量、消息队列和共享内存。申请IPC资源就申请了一个全局唯一的32位ID，所以IPC namespace中实际上包含了系统IPC标识符以及实现POSIX消息队列的文件系统。在同一个IPC namespace下的进程彼此可见，不同IPC namespace下的进程则互相不可见。</p><p><strong>USER:</strong></p><p>User namespace 主要是隔离用户的用户组ID。也就是说，一个进程的User ID 和Group ID 在User namespace 内外可以是不同的。比较常用的是，在宿主机上以一个非root用户运行创建一个User namespace，然后在User namespace里面却映射成root 用户。这样意味着，这个进程在User namespace里面有root权限，但是在User namespace外面却没有root的权限。</p><p><strong>NET:</strong></p><p>Network namespace 是用来隔离网络设备，IP地址端口等网络栈的namespace。Network namespace 可以让每个容器拥有自己独立的网络设备（虚拟的），而且容器内的应用可以绑定到自己的端口，每个 namesapce 内的端口都不会互相冲突。在宿主机上搭建网桥后，就能很方便的实现容器之间的通信，而且每个容器内的应用都可以使用相同的端口。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;我们谈及docker，都知道docker容器本质上是宿主机的进程，Docker通过namespace实现了资源隔离，通过cgroups实现了资源限制，接下来先聊聊namespaces是怎么一回事。&lt;/p&gt;
&lt;h1 id=&quot;namespace是什么&quot;&gt;&lt;a href=&quot;#na</summary>
      
    
    
    
    
    <category term="docker" scheme="https://slions.github.io/tags/docker/"/>
    
  </entry>
  
</feed>
