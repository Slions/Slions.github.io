<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Hexo</title>
  
  
  <link href="https://slions.github.io/atom.xml" rel="self"/>
  
  <link href="https://slions.github.io/"/>
  <updated>2021-08-07T11:11:00.489Z</updated>
  <id>https://slions.github.io/</id>
  
  <author>
    <name>Jingyu Shi</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>linux误删文件恢复思路</title>
    <link href="https://slions.github.io/2021/08/07/linux%E8%AF%AF%E5%88%A0%E6%96%87%E4%BB%B6%E6%81%A2%E5%A4%8D%E6%80%9D%E8%B7%AF/"/>
    <id>https://slions.github.io/2021/08/07/linux%E8%AF%AF%E5%88%A0%E6%96%87%E4%BB%B6%E6%81%A2%E5%A4%8D%E6%80%9D%E8%B7%AF/</id>
    <published>2021-08-07T10:36:34.000Z</published>
    <updated>2021-08-07T11:11:00.489Z</updated>
    
    <content type="html"><![CDATA[<p>使用linux时对于执行删除操作要慎之又慎，特别是重要的数据最好提前备份。当然，如果真的删除了一个文件时，我们也要冷静思考，想想如何通过其他手段弥补或减小损失。</p><p>在解决问题前，我们先了解下涉及到的基本概念：</p><ul><li><p>我们看到的文件实际上是一个指向inode的链接, inode链接包含了文件的所有属性, 比如权限和所有者, 数据块地址(文件存储在磁盘的这些数据块中)。当你删除(rm)一个文件, 实际删除了指向inode的链接, 并没有删除inode的内容，进程可能还在使用。 只有当inode的所有链接完全移去，然后对应的后端数据块才会写入新的数据。</p></li><li><p>proc是linux的一个伪文件系统，用户和应用程序可以通过 proc 得到系统的信息，并可以改变内核的某些参数。</p></li><li><p>系统上的进程在/proc都有一个目录和自己的名字， 里面包含了一个fd(文件描述符)子目录(进程需要打开文件的所有链接). 如果从文件系统中删除一个文件, 此处还有一个inode的引用:<code>/proc/进程号/fd/文件描述符</code></p></li><li><p>lsof（List Open Files） 命令可以查看你进程开打的文件，打开文件的进程，进程打开的端口(TCP、UDP)等等。</p></li></ul><h1 id="模拟误删场景"><a href="#模拟误删场景" class="headerlink" title="模拟误删场景"></a>模拟误删场景</h1><h2 id="准备环境"><a href="#准备环境" class="headerlink" title="准备环境"></a>准备环境</h2><p>创建一个测试文件testfile，持续监听此文件。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> <span class="keyword">while</span> <span class="literal">true</span>;<span class="keyword">do</span> <span class="built_in">echo</span> `date +%F-%T` &gt;&gt; testfile;sleep 1;<span class="keyword">done</span> &amp;</span></span><br><span class="line">[1] 12354</span><br><span class="line"><span class="meta">$</span><span class="bash"> ls</span></span><br><span class="line">testfile</span><br><span class="line"><span class="meta">$</span><span class="bash"> tail -f testfile</span></span><br><span class="line">2021-08-07-18:31:46</span><br><span class="line">2021-08-07-18:31:47</span><br><span class="line">2021-08-07-18:31:48</span><br><span class="line">2021-08-07-18:31:49</span><br><span class="line">2021-08-07-18:31:50</span><br><span class="line">2021-08-07-18:31:51</span><br><span class="line">2021-08-07-18:31:52</span><br><span class="line">2021-08-07-18:31:53</span><br><span class="line">2021-08-07-18:31:54</span><br><span class="line">2021-08-07-18:31:55</span><br><span class="line">2021-08-07-18:31:56</span><br><span class="line">2021-08-07-18:31:57</span><br><span class="line">2021-08-07-18:31:58</span><br><span class="line">2021-08-07-18:31:59</span><br><span class="line">2021-08-07-18:32:00</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>开启另一个终端，将testfile删除。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> rm  testfile</span></span><br><span class="line">rm：是否删除普通文件 &quot;testfile&quot;？y</span><br></pre></td></tr></table></figure><h2 id="恢复数据"><a href="#恢复数据" class="headerlink" title="恢复数据"></a>恢复数据</h2><p>使用lsof命令查看testfile文件句柄是否释放。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> lsof|grep testfile</span></span><br><span class="line">tail      12376                root    3r      REG              253,0       720     686557 /home/slions/testfile (deleted)</span><br></pre></td></tr></table></figure><p>第一列是进程的名称(命令名), 第二列是进程号(PID), 第四列是文件描述符，r说明是读操作。</p><p>现在我们可以知道12376进程仍有打开文件, 文件描述符是3。</p><p>从/proc里面拷贝出删除前的数据。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> cp /proc/12376/fd/3 testfile.old</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> ls</span></span><br><span class="line">testfile  testfile.old</span><br></pre></td></tr></table></figure><p>接着查看下文件内容，恢复的数据可以完美对应。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> tail testfile.old</span></span><br><span class="line">2021-08-07-18:32:12</span><br><span class="line">2021-08-07-18:32:13</span><br><span class="line">2021-08-07-18:32:14</span><br><span class="line">2021-08-07-18:32:15</span><br><span class="line">2021-08-07-18:32:16</span><br><span class="line">2021-08-07-18:32:17</span><br><span class="line">2021-08-07-18:32:18</span><br><span class="line">2021-08-07-18:32:19</span><br><span class="line">2021-08-07-18:32:20</span><br><span class="line">2021-08-07-18:32:21</span><br><span class="line"><span class="meta">$</span><span class="bash"> head -5 testfile</span></span><br><span class="line">2021-08-07-18:32:22</span><br><span class="line">2021-08-07-18:32:23</span><br><span class="line">2021-08-07-18:32:24</span><br><span class="line">2021-08-07-18:32:25</span><br><span class="line">2021-08-07-18:32:26</span><br></pre></td></tr></table></figure><p>最后要说的一点还是执行删除前三思后行，为了防止误操作带来的损失可以参考<a href="/2021/08/01/alias%E4%B8%8Erm%E7%9A%84%E6%9C%80%E4%BD%B3%E5%AE%9E%E8%B7%B5/" title="之前的文章">之前的文章</a>，提高安全性，愿各位维护的程序永不下线，机器永不宕机。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;使用linux时对于执行删除操作要慎之又慎，特别是重要的数据最好提前备份。当然，如果真的删除了一个文件时，我们也要冷静思考，想想如何通过其他手段弥补或减小损失。&lt;/p&gt;
&lt;p&gt;在解决问题前，我们先了解下涉及到的基本概念：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;我们看到的文件实际上</summary>
      
    
    
    
    
    <category term="linux" scheme="https://slions.github.io/tags/linux/"/>
    
  </entry>
  
  <entry>
    <title>逻辑卷管理LVM</title>
    <link href="https://slions.github.io/2021/08/06/%E9%80%BB%E8%BE%91%E5%8D%B7%E7%AE%A1%E7%90%86LVM/"/>
    <id>https://slions.github.io/2021/08/06/%E9%80%BB%E8%BE%91%E5%8D%B7%E7%AE%A1%E7%90%86LVM/</id>
    <published>2021-08-06T14:18:11.000Z</published>
    <updated>2021-08-07T03:31:09.116Z</updated>
    
    <content type="html"><![CDATA[<p>总所周知不论是机械硬盘还是固态磁盘大小都是固定的，当磁盘空间满了后只能通过删除无用数据来保持磁盘的可用性，这篇介绍的LVM(Logical Volume Manager)是建立在磁盘和分区之上的一个逻辑层，用来提高磁盘分区管理的灵活性，可以随时随地的扩缩容分区大小。</p><h1 id="LVM术语"><a href="#LVM术语" class="headerlink" title="LVM术语"></a>LVM术语</h1><ul><li><p>Physical Volume(PV)</p><p>实际分区需要调整 System ID 成为 LVM 表示(8e) ，然后经过 pvcreate 命令将他转为 LVM 最低层的 PV, 然后才能使用磁盘。</p></li><li><p>Volume Group(VG)</p><p>将多个PV组合起来，使用vgcreate命令创建成卷组，这样卷组包含了多个PV就比较大了，相当于重新整合了多个分区后得到的磁盘。虽然VG是整合多个PV的，但是创建VG时会将VG所有的空间根据指定的PE大小划分为多个PE，在LVM模式下的存储都以PE为单元，类似于文件系统的Block。</p></li><li><p>Physical Extent(PE)</p><p>LVM 预设使用 4MB 的 PE 区块，每个 LV 最多允许有 65534 个 PE ，即 256GB 。PE 属于 LVM 最小存储区。</p></li><li><p>Logical Volume(LV)</p><p>VG相当于整合过的硬盘，那么LV就相当于分区，只不过该分区是通过VG来划分的。VG中有很多PE单元，可以指定将多少个PE划分给一个LV，也可以直接指定大小(如多少兆)来划分。划分为LV之后就相当于划分了分区，只需再对LV进行格式化即可变成普通的文件系统。</p></li><li><p>Logical extent(LE)</p><p>PE是物理存储单元，而LE则是逻辑存储单元，也即为lv中的逻辑存储单元，和pe的大小是一样的。从vg中划分lv，实际上是从vg中划分vg中的pe，只不过划分lv后它不再称为pe，而是成为le。</p></li></ul><p><strong>LVM之所以能够伸缩容量，其原因就在于能够将LV里空闲的PE移出，或向LV中添加空闲的PE。</strong></p><h1 id="LVM的写入机制"><a href="#LVM的写入机制" class="headerlink" title="LVM的写入机制"></a>LVM的写入机制</h1><p>LV是从VG中划分出来的，LV中的PE很可能来自于多个PV。在向LV存储数据时，有多种存储机制，其中两种是：</p><ul><li>线性模式(linear)：先写完来自于同一个PV的PE，再写来自于下一个PV的PE。</li><li>条带模式(striped)：一份数据拆分成多份，分别写入该LV对应的每个PV中，所以读写性能较好，类似于RAID 0。</li></ul><p>尽管striped读写性能较好也<strong>不建议</strong>使用该模式，因为lvm的着重点在于弹性容量扩展而非性能，要实现性能应该使用RAID来实现，而且使用striped模式时要进行容量的扩展和收缩将比较麻烦。默认的是使用线性模式。</p><h1 id="LVM实现图解"><a href="#LVM实现图解" class="headerlink" title="LVM实现图解"></a>LVM实现图解</h1><p><img src="/doc_picture/lvm.jpg" alt="lvm"></p><h1 id="LVM的实现"><a href="#LVM的实现" class="headerlink" title="LVM的实现"></a>LVM的实现</h1><p>看下我本地的环境，/dev/sdb已经分好了4个区，并且system id都设置为了8e。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> lsblk</span></span><br><span class="line">NAME            MAJ:MIN RM  SIZE RO TYPE MOUNTPOINT</span><br><span class="line">sda               8:0    0   20G  0 disk</span><br><span class="line">├─sda1            8:1    0    1G  0 part /boot</span><br><span class="line">└─sda2            8:2    0   19G  0 part</span><br><span class="line">  ├─centos-root 253:0    0   18G  0 lvm  /</span><br><span class="line">  └─centos-swap 253:1    0 1020M  0 lvm  [SWAP]</span><br><span class="line">sdb               8:16   0   20G  0 disk</span><br><span class="line">├─sdb1            8:17   0    5G  0 part</span><br><span class="line">├─sdb2            8:18   0    2G  0 part</span><br><span class="line">├─sdb3            8:19   0   10G  0 part</span><br><span class="line">└─sdb4            8:20   0    3G  0 part</span><br><span class="line">sr0              11:0    1   10G  0 rom</span><br><span class="line"><span class="meta">$</span><span class="bash"> fdisk -l /dev/sdb</span></span><br><span class="line"></span><br><span class="line">磁盘 /dev/sdb：21.5 GB, 21474836480 字节，41943040 个扇区</span><br><span class="line">Units = 扇区 of 1 * 512 = 512 bytes</span><br><span class="line">扇区大小(逻辑/物理)：512 字节 / 512 字节</span><br><span class="line">I/O 大小(最小/最佳)：512 字节 / 512 字节</span><br><span class="line">磁盘标签类型：dos</span><br><span class="line">磁盘标识符：0x76721574</span><br><span class="line"></span><br><span class="line">   设备 Boot      Start         End      Blocks   Id  System</span><br><span class="line">/dev/sdb1            2048    10487807     5242880   8e  Linux LVM</span><br><span class="line">/dev/sdb2        10487808    14682111     2097152   8e  Linux LVM</span><br><span class="line">/dev/sdb3        14682112    35653631    10485760   8e  Linux LVM</span><br><span class="line">/dev/sdb4        35653632    41943039     3144704   8e  Linux LVM</span><br><span class="line"></span><br></pre></td></tr></table></figure><h2 id="管理pv"><a href="#管理pv" class="headerlink" title="管理pv"></a>管理pv</h2><p>管理PV有几个命令：pvscan、pvdisplay、pvcreate、pvremove和pvmove。</p><p>命令很简单，基本都不需要任何选项。</p><table><thead><tr><th>功能</th><th>命令</th></tr></thead><tbody><tr><td>创建pv</td><td>pvcreate</td></tr><tr><td>扫描并列出所有的pv</td><td>pvscan</td></tr><tr><td>列出pv属性信息</td><td>pvdisplay</td></tr><tr><td>移除pv</td><td>pvremove</td></tr><tr><td>移动pv中的数据</td><td>pvmove</td></tr></tbody></table><h3 id="创建PV"><a href="#创建PV" class="headerlink" title="创建PV"></a>创建PV</h3><p>将/dev/sdb[1-3]创建为pv。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> pvcreate /dev/sdb[1-3]</span></span><br><span class="line">  Physical volume &quot;/dev/sdb1&quot; successfully created.</span><br><span class="line">  Physical volume &quot;/dev/sdb2&quot; successfully created.</span><br><span class="line">  Physical volume &quot;/dev/sdb3&quot; successfully created.</span><br><span class="line"></span><br></pre></td></tr></table></figure><h3 id="查看pv属性"><a href="#查看pv属性" class="headerlink" title="查看pv属性"></a>查看pv属性</h3><p>使用pvscan来查看哪些pv和基本属性。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> pvscan</span></span><br><span class="line">  PV /dev/sda2   VG centos          lvm2 [&lt;19.00 GiB / 0    free]</span><br><span class="line">  PV /dev/sdb3                      lvm2 [10.00 GiB]</span><br><span class="line">  PV /dev/sdb1                      lvm2 [5.00 GiB]</span><br><span class="line">  PV /dev/sdb2                      lvm2 [2.00 GiB]</span><br><span class="line">  Total: 4 [&lt;36.00 GiB] / in use: 1 [&lt;19.00 GiB] / in no VG: 3 [17.00 GiB]</span><br></pre></td></tr></table></figure><p>使用pvdisplay查看其中一个pv的属性信息。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> pvdisplay /dev/sdb1</span></span><br><span class="line">  &quot;/dev/sdb1&quot; is a new physical volume of &quot;5.00 GiB&quot;</span><br><span class="line">  --- NEW Physical volume ---</span><br><span class="line">  PV Name               /dev/sdb1</span><br><span class="line">  VG Name</span><br><span class="line">  PV Size               5.00 GiB</span><br><span class="line">  Allocatable           NO</span><br><span class="line">  PE Size               0</span><br><span class="line">  Total PE              0</span><br><span class="line">  Free PE               0</span><br><span class="line">  Allocated PE          0</span><br><span class="line">  PV UUID               viN4Oq-ZeFx-3wTz-4zXf-0Z4N-Urcf-F2eMGi</span><br></pre></td></tr></table></figure><h3 id="查看pe分布"><a href="#查看pe分布" class="headerlink" title="查看pe分布"></a>查看pe分布</h3><p><code>pvdisplay  -m</code>可以查看该设备中PE的使用分布图。以下是某次显示结果。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> pvdisplay  -m /dev/sda2</span></span><br><span class="line">  --- Physical volume ---</span><br><span class="line">  PV Name               /dev/sda2</span><br><span class="line">  VG Name               centos</span><br><span class="line">  PV Size               &lt;19.00 GiB / not usable 3.00 MiB</span><br><span class="line">  Allocatable           yes (but full)</span><br><span class="line">  PE Size               4.00 MiB</span><br><span class="line">  Total PE              4863</span><br><span class="line">  Free PE               0</span><br><span class="line">  Allocated PE          4863</span><br><span class="line">  PV UUID               0TXR4i-laLf-DqVh-lNVK-ybSt-io3c-QKDj5w</span><br><span class="line"></span><br><span class="line">  --- Physical Segments ---</span><br><span class="line">  Physical extent 0 to 4607:                    # 说明第0-4607的PE正被使用。PV中PE的序号是从0开始编号的</span><br><span class="line">    Logical volume      /dev/centos/root</span><br><span class="line">    Logical extents     0 to 4607               # 该PE在LV中的0-4607的LE位置上</span><br><span class="line">  Physical extent 4608 to 4862:                 # 说明4608-4862的PE正使用</span><br><span class="line">    Logical volume      /dev/centos/swap</span><br><span class="line">    Logical extents     0 to 254                # 该PE在LV中的位置是0-254</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>知道了PE的分布，就可以轻松地使用pvmove命令在设备之间进行PE数据的移动。具体关于pvmove的用法，可以自行百度，因为LVM缩容用处不大。</p><h3 id="删除pv"><a href="#删除pv" class="headerlink" title="删除pv"></a>删除pv</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> pvremove /dev/sdb3</span></span><br><span class="line">  Labels on physical volume &quot;/dev/sdb3&quot; successfully wiped.</span><br><span class="line"><span class="meta">$</span><span class="bash"> pvscan</span></span><br><span class="line">  PV /dev/sda2   VG centos          lvm2 [&lt;19.00 GiB / 0    free]</span><br><span class="line">  PV /dev/sdb1                      lvm2 [5.00 GiB]</span><br><span class="line">  PV /dev/sdb2                      lvm2 [2.00 GiB]</span><br><span class="line">  Total: 3 [&lt;26.00 GiB] / in use: 1 [&lt;19.00 GiB] / in no VG: 2 [7.00 GiB]</span><br></pre></td></tr></table></figure><h2 id="管理VG"><a href="#管理VG" class="headerlink" title="管理VG"></a>管理VG</h2><table><thead><tr><th>功能</th><th>命令</th></tr></thead><tbody><tr><td>创建vg</td><td>vgcreate</td></tr><tr><td>扫描并列出所有的vg</td><td>vgscan</td></tr><tr><td>列出vg属性信息</td><td>vgdisplay</td></tr><tr><td>移除vg</td><td>vgremove</td></tr><tr><td>从vg中移除pv</td><td>vgreduce</td></tr><tr><td>将pv添加到vg中</td><td>vgextend</td></tr><tr><td>修改vg属性</td><td>vgchange</td></tr></tbody></table><h3 id="创建vg"><a href="#创建vg" class="headerlink" title="创建vg"></a>创建vg</h3><p>创建一个名为slions_vg1的vg，并将/dev/sdb1与/dev/sdb2加入此vg，指定pe大小为8M（默认为4M）</p><blockquote><p>创建vg后，是很难再修改pe大小的，只有空数据的vg可以修改，但这样还不如重新创建vg。</p><p>创建了vg实际上是在/dev目录下管理了一个vg目录/dev/slions_vg1，不过只有在创建了lv该目录才会被创建，而该vg中创建lv，将会在该目录下生成链接文件指向/dev/dm设备。</p></blockquote><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> vgcreate -s 8M slions_vg1 /dev/sdb[1-2]</span></span><br><span class="line">  Volume group &quot;slions_vg1&quot; successfully created</span><br></pre></td></tr></table></figure><h3 id="查看vg属性"><a href="#查看vg属性" class="headerlink" title="查看vg属性"></a>查看vg属性</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> vgscan</span></span><br><span class="line">  Reading volume groups from cache.</span><br><span class="line">  Found volume group &quot;slions_vg1&quot; using metadata type lvm2</span><br><span class="line">  Found volume group &quot;centos&quot; using metadata type lvm2</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> vgdisplay slions_vg1</span></span><br><span class="line">  --- Volume group ---</span><br><span class="line">  VG Name               slions_vg1</span><br><span class="line">  System ID</span><br><span class="line">  Format                lvm2</span><br><span class="line">  Metadata Areas        2</span><br><span class="line">  Metadata Sequence No  1</span><br><span class="line">  VG Access             read/write</span><br><span class="line">  VG Status             resizable</span><br><span class="line">  MAX LV                0</span><br><span class="line">  Cur LV                0</span><br><span class="line">  Open LV               0</span><br><span class="line">  Max PV                0</span><br><span class="line">  Cur PV                2</span><br><span class="line">  Act PV                2</span><br><span class="line">  VG Size               6.98 GiB</span><br><span class="line">  PE Size               8.00 MiB</span><br><span class="line">  Total PE              894</span><br><span class="line">  Alloc PE / Size       0 / 0</span><br><span class="line">  Free  PE / Size       894 / 6.98 GiB</span><br><span class="line">  VG UUID               PVvkTd-yRWc-PgGt-kBjy-loN3-Unmm-z37JrX</span><br></pre></td></tr></table></figure><h3 id="移除pv"><a href="#移除pv" class="headerlink" title="移除pv"></a>移除pv</h3><p>从slions_vg1中移除一个pv，/dev/sdb2，再vgdisplay，发现pv少了一个，pe相应的也减少了。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> vgreduce slions_vg1 /dev/sdb2</span></span><br><span class="line">  Removed &quot;/dev/sdb2&quot; from volume group &quot;slions_vg1&quot;</span><br><span class="line"><span class="meta">$</span><span class="bash"> vgdisplay slions_vg1</span></span><br><span class="line">  --- Volume group ---</span><br><span class="line">  VG Name               slions_vg1</span><br><span class="line">  System ID</span><br><span class="line">  Format                lvm2</span><br><span class="line">  Metadata Areas        1</span><br><span class="line">  Metadata Sequence No  2</span><br><span class="line">  VG Access             read/write</span><br><span class="line">  VG Status             resizable</span><br><span class="line">  MAX LV                0</span><br><span class="line">  Cur LV                0</span><br><span class="line">  Open LV               0</span><br><span class="line">  Max PV                0</span><br><span class="line">  Cur PV                1</span><br><span class="line">  Act PV                1</span><br><span class="line">  VG Size               4.99 GiB</span><br><span class="line">  PE Size               8.00 MiB</span><br><span class="line">  Total PE              639</span><br><span class="line">  Alloc PE / Size       0 / 0</span><br><span class="line">  Free  PE / Size       639 / 4.99 GiB</span><br><span class="line">  VG UUID               PVvkTd-yRWc-PgGt-kBjy-loN3-Unmm-z37JrX</span><br><span class="line"></span><br></pre></td></tr></table></figure><h3 id="添加pv"><a href="#添加pv" class="headerlink" title="添加pv"></a>添加pv</h3><p>再将刚才删掉的/dev/sdb2添加入slions_vg1中。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> vgextend slions_vg1 /dev/sdb2</span></span><br><span class="line">  Volume group &quot;slions_vg1&quot; successfully extended</span><br></pre></td></tr></table></figure><h3 id="设置vg的状态"><a href="#设置vg的状态" class="headerlink" title="设置vg的状态"></a>设置vg的状态</h3><p>vgchange用于设置卷组的活动状态，卷组的激活状态主要影响的是lv。使用-a选项来设置。</p><p>将slions_vg1设置为活动状态。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> vgchange -a y slions_vg1</span></span><br><span class="line">  0 logical volume(s) in volume group &quot;slions_vg1&quot; now active</span><br></pre></td></tr></table></figure><p>将slions_vg1设置为非活动状态。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> vgchange -a n  slions_vg1</span></span><br><span class="line">  0 logical volume(s) in volume group &quot;slions_vg1&quot; now active</span><br></pre></td></tr></table></figure><h2 id="管理LV"><a href="#管理LV" class="headerlink" title="管理LV"></a>管理LV</h2><table><thead><tr><th>功能</th><th>命令</th></tr></thead><tbody><tr><td>创建lv</td><td>lvcreate</td></tr><tr><td>扫描并列出所有的lv</td><td>lvscan</td></tr><tr><td>列出lv属性信息</td><td>lvdisplay</td></tr><tr><td>移除lv</td><td>lvremove</td></tr><tr><td>缩小lv容量</td><td>lvreduce(lvresize)</td></tr><tr><td>增大lv容量</td><td>lvextend(lvresize)</td></tr><tr><td>改变lv容量</td><td>lvresize</td></tr></tbody></table><h3 id="创建lv"><a href="#创建lv" class="headerlink" title="创建lv"></a>创建lv</h3><p>命令格式：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">lvcreate &#123;-L size(M/G) | -l PEnum&#125; -n lv_name vg_name</span><br><span class="line">-L：根据大小来创建lv，即分配多大空间给此lv</span><br><span class="line">-l：根据PE的数量来创建lv，即分配多少个pe给此lv</span><br><span class="line">-n：指定lv的名称</span><br></pre></td></tr></table></figure><p>当前slions_vg1有894个PE，大小为6.98G。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> vgdisplay slions_vg1 |grep PE</span></span><br><span class="line">  PE Size               8.00 MiB</span><br><span class="line">  Total PE              894</span><br><span class="line">  Alloc PE / Size       0 / 0</span><br><span class="line">  Free  PE / Size       894 / 6.98 GiB</span><br></pre></td></tr></table></figure><p>使用-L和-l分别创建名称为slions_lv1和slions_lv2的lv。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> lvcreate -L 3G -n slions_lv1 slions_vg1</span></span><br><span class="line">  Logical volume &quot;slions_lv1&quot; created.</span><br><span class="line"><span class="meta">$</span><span class="bash"> lvcreate -l +100%FREE -n slions_lv2 slions_vg1   <span class="comment">#使用剩余所有的PE</span></span></span><br><span class="line">  Logical volume &quot;slions_lv2&quot; created.</span><br></pre></td></tr></table></figure><p>创建lv后，将在/dev/firstvg目录中创建对应lv名称的软链接文件，同时也在/dev/mapper目录下创建链接文件，它们都指向/dev/dm设备。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> ll /dev/slions_vg1/</span></span><br><span class="line">总用量 0</span><br><span class="line">lrwxrwxrwx. 1 root root 7 8月   7 10:49 slions_lv1 -&gt; ../dm-2</span><br><span class="line">lrwxrwxrwx. 1 root root 7 8月   7 10:50 slions_lv2 -&gt; ../dm-3</span><br><span class="line"><span class="meta">$</span><span class="bash"> ll /dev/mapper/</span></span><br><span class="line">总用量 0</span><br><span class="line">lrwxrwxrwx. 1 root root       7 8月   6 21:58 centos-root -&gt; ../dm-0</span><br><span class="line">lrwxrwxrwx. 1 root root       7 8月   6 21:58 centos-swap -&gt; ../dm-1</span><br><span class="line">crw-------. 1 root root 10, 236 8月   6 21:58 control</span><br><span class="line">lrwxrwxrwx. 1 root root       7 8月   7 10:49 slions_vg1-slions_lv1 -&gt; ../dm-2</span><br><span class="line">lrwxrwxrwx. 1 root root       7 8月   7 10:50 slions_vg1-slions_lv2 -&gt; ../dm-3</span><br></pre></td></tr></table></figure><h3 id="查看lv属性"><a href="#查看lv属性" class="headerlink" title="查看lv属性"></a>查看lv属性</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> lvscan</span></span><br><span class="line">  ACTIVE            &#x27;/dev/slions_vg1/slions_lv1&#x27; [3.00 GiB] inherit</span><br><span class="line">  ACTIVE            &#x27;/dev/slions_vg1/slions_lv2&#x27; [3.98 GiB] inherit</span><br><span class="line">  ACTIVE            &#x27;/dev/centos/root&#x27; [18.00 GiB] inherit</span><br><span class="line">  ACTIVE            &#x27;/dev/centos/swap&#x27; [1020.00 MiB] inherit</span><br><span class="line"><span class="meta">$</span><span class="bash"> lvdisplay /dev/slions_vg1/slions_lv1</span></span><br><span class="line">  --- Logical volume ---</span><br><span class="line">  LV Path                /dev/slions_vg1/slions_lv1</span><br><span class="line">  LV Name                slions_lv1</span><br><span class="line">  VG Name                slions_vg1</span><br><span class="line">  LV UUID                aUGzfw-sypb-96Ux-hRxn-JXeS-2UIL-812m4T</span><br><span class="line">  LV Write Access        read/write</span><br><span class="line">  LV Creation host, time slions_pc1, 2021-08-07 10:49:52 +0800</span><br><span class="line">  LV Status              available</span><br><span class="line"><span class="meta">  #</span><span class="bash"> open                 0</span></span><br><span class="line">  LV Size                3.00 GiB</span><br><span class="line">  Current LE             384</span><br><span class="line">  Segments               1</span><br><span class="line">  Allocation             inherit</span><br><span class="line">  Read ahead sectors     auto</span><br><span class="line">  - currently set to     256</span><br><span class="line">  Block device           253:2</span><br><span class="line"></span><br></pre></td></tr></table></figure><h3 id="格式化lv"><a href="#格式化lv" class="headerlink" title="格式化lv"></a>格式化lv</h3><p>我们通过格式化lv使其形成文件系统，就可以和普通磁盘一样挂载使用了。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> mkfs.xfs /dev/slions_vg1/slions_lv1</span></span><br><span class="line">meta-data=/dev/slions_vg1/slions_lv1 isize=512    agcount=4, agsize=196608 blks</span><br><span class="line">         =                       sectsz=512   attr=2, projid32bit=1</span><br><span class="line">         =                       crc=1        finobt=0, sparse=0</span><br><span class="line">data     =                       bsize=4096   blocks=786432, imaxpct=25</span><br><span class="line">         =                       sunit=0      swidth=0 blks</span><br><span class="line">naming   =version 2              bsize=4096   ascii-ci=0 ftype=1</span><br><span class="line">log      =internal log           bsize=4096   blocks=2560, version=2</span><br><span class="line">         =                       sectsz=512   sunit=0 blks, lazy-count=1</span><br><span class="line">realtime =none                   extsz=4096   blocks=0, rtextents=0</span><br><span class="line"><span class="meta">$</span><span class="bash"> mkdir /xfs_dir</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> mount /dev/slions_vg1/slions_lv1 /xfs_dir/</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> df -Th|grep xfs_dir</span></span><br><span class="line">/dev/mapper/slions_vg1-slions_lv1 xfs       3.0G   33M  3.0G    2% /xfs_dir</span><br><span class="line"><span class="meta">$</span><span class="bash"> lsblk</span></span><br><span class="line">NAME                      MAJ:MIN RM  SIZE RO TYPE MOUNTPOINT</span><br><span class="line">sda                         8:0    0   20G  0 disk</span><br><span class="line">├─sda1                      8:1    0    1G  0 part /boot</span><br><span class="line">└─sda2                      8:2    0   19G  0 part</span><br><span class="line">  ├─centos-root           253:0    0   18G  0 lvm  /</span><br><span class="line">  └─centos-swap           253:1    0 1020M  0 lvm  [SWAP]</span><br><span class="line">sdb                         8:16   0   20G  0 disk</span><br><span class="line">├─sdb1                      8:17   0    5G  0 part</span><br><span class="line">│ ├─slions_vg1-slions_lv1 253:2    0    3G  0 lvm  /xfs_dir</span><br><span class="line">│ └─slions_vg1-slions_lv2 253:3    0    4G  0 lvm</span><br><span class="line">├─sdb2                      8:18   0    2G  0 part</span><br><span class="line">│ └─slions_vg1-slions_lv2 253:3    0    4G  0 lvm</span><br><span class="line">├─sdb3                      8:19   0   10G  0 part</span><br><span class="line">└─sdb4                      8:20   0    3G  0 part</span><br><span class="line">sr0                        11:0    1   10G  0 rom</span><br></pre></td></tr></table></figure><p>也可以使用file -s查看lv的文件系统，由于/dev/slions_vg1和/dev/mapper下的lv都是链接到/dev/下块设备的链接文件，所以只能对块设备进行查看，否则查看的结果也仅仅只是个链接文件类型。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> file -s /dev/dm-2</span></span><br><span class="line">/dev/dm-2: SGI XFS filesystem data (blksz 4096, inosz 512, v2 dirs)</span><br></pre></td></tr></table></figure><h2 id="LVM扩容"><a href="#LVM扩容" class="headerlink" title="LVM扩容"></a>LVM扩容</h2><p>在文章的开头已经说了，lvm最大的优势就是其可伸缩性，而其伸缩性又更偏重于扩容，这是使用lvm的最大原因。</p><blockquote><p>扩容的实质是将vg中空闲的pe添加到lv中，所以只要vg中有空闲的pe，就可以进行扩容，即使没有空闲的pe，也可以添加pv，将pv加入到vg中增加空闲pe。</p></blockquote><h3 id="扩容流程"><a href="#扩容流程" class="headerlink" title="扩容流程"></a>扩容流程</h3><ol><li>使用lvextend或者lvresize添加更多的pe或容量到lv中</li><li>使用resize2fs命令(xfs则使用xfs_growfs)将lv增加后的容量增加到对应的文件系统中</li></ol><h3 id="扩容示例"><a href="#扩容示例" class="headerlink" title="扩容示例"></a>扩容示例</h3><p>将/dev/sdb3创建为pv并加入slions_vg1，查看此时vg的pe状态，已经多了1279个PE，空余大小为9.99g。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> pvcreate /dev/sdb3</span></span><br><span class="line">  Physical volume &quot;/dev/sdb3&quot; successfully created.</span><br><span class="line"><span class="meta">$</span><span class="bash"> vgextend slions_vg1 /dev/sdb3</span></span><br><span class="line">  Volume group &quot;slions_vg1&quot; successfully extended</span><br><span class="line"><span class="meta">$</span><span class="bash"> vgdisplay slions_vg1 |grep PE</span></span><br><span class="line">  PE Size               8.00 MiB</span><br><span class="line">  Total PE              2173</span><br><span class="line">  Alloc PE / Size       894 / 6.98 GiB</span><br><span class="line">  Free  PE / Size       1279 / 9.99 GiB</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>将其全部添加到slions_lv1中，有两种方式添加：按容量大小添加和按PE数量添加。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> umount /dev/slions_vg1/slions_lv1</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> lvextend -L +5G /dev/slions_vg1/slions_lv1</span></span><br><span class="line">  Size of logical volume slions_vg1/slions_lv1 changed from 3.00 GiB (384 extents) to 8.00 GiB (1024 extents).</span><br><span class="line">  Logical volume slions_vg1/slions_lv1 successfully resized.</span><br><span class="line"><span class="meta">$</span><span class="bash"> vgdisplay slions_vg1 |grep PE</span></span><br><span class="line">  PE Size               8.00 MiB</span><br><span class="line">  Total PE              2173</span><br><span class="line">  Alloc PE / Size       1534 / 11.98 GiB</span><br><span class="line">  Free  PE / Size       639 / 4.99 GiB</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">$</span><span class="bash"> lvextend -l +639 /dev/slions_vg1/slions_lv1</span></span><br><span class="line">  Size of logical volume slions_vg1/slions_lv1 changed from 8.00 GiB (1024 extents) to 12.99 GiB (1663 extents).</span><br><span class="line">  Logical volume slions_vg1/slions_lv1 successfully resized.</span><br><span class="line"><span class="meta">$</span><span class="bash"> lvscan</span></span><br><span class="line">  ACTIVE            &#x27;/dev/slions_vg1/slions_lv1&#x27; [12.99 GiB] inherit</span><br><span class="line">  ACTIVE            &#x27;/dev/slions_vg1/slions_lv2&#x27; [3.98 GiB] inherit</span><br><span class="line">  ACTIVE            &#x27;/dev/centos/root&#x27; [18.00 GiB] inherit</span><br><span class="line">  ACTIVE            &#x27;/dev/centos/swap&#x27; [1020.00 MiB] inherit</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>也可以使用lvresize来增加lv的容量方法和lvextend一样。如：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> lvresize -L +5G /dev/slions_vg1/slions_lv1</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> lvresize -l +639 /dev/slions_vg1/slions_lv1</span></span><br></pre></td></tr></table></figure><p>将slions_lv1挂载，查看该lv对应文件系统的容量。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> df -Th /xfs_dir/</span></span><br><span class="line">文件系统                          类型  容量  已用  可用 已用% 挂载点</span><br><span class="line">/dev/mapper/slions_vg1-slions_lv1 xfs   3.0G   33M  3.0G    2% /xfs_dir</span><br></pre></td></tr></table></figure><p>容量并没有增加，因为只是lv的容量增加了，而文件系统的容量却没有增加。</p><p>需要使用resize2fs工具来改变ext文件系统的大小，如果是xfs文件系统，则使用xfs_growfs。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> xfs_growfs /xfs_dir/</span></span><br><span class="line">meta-data=/dev/mapper/slions_vg1-slions_lv1 isize=512    agcount=4, agsize=196608 blks</span><br><span class="line">         =                       sectsz=512   attr=2, projid32bit=1</span><br><span class="line">         =                       crc=1        finobt=0 spinodes=0</span><br><span class="line">data     =                       bsize=4096   blocks=786432, imaxpct=25</span><br><span class="line">         =                       sunit=0      swidth=0 blks</span><br><span class="line">naming   =version 2              bsize=4096   ascii-ci=0 ftype=1</span><br><span class="line">log      =internal               bsize=4096   blocks=2560, version=2</span><br><span class="line">         =                       sectsz=512   sunit=0 blks, lazy-count=1</span><br><span class="line">realtime =none                   extsz=4096   blocks=0, rtextents=0</span><br><span class="line">data blocks changed from 786432 to 3405824</span><br><span class="line"><span class="meta">$</span><span class="bash"> df -Th /xfs_dir/</span></span><br><span class="line">文件系统                          类型  容量  已用  可用 已用% 挂载点</span><br><span class="line">/dev/mapper/slions_vg1-slions_lv1 xfs    13G   33M   13G    1% /xfs_dir</span><br><span class="line"></span><br></pre></td></tr></table></figure><h1 id="END"><a href="#END" class="headerlink" title="END"></a>END</h1><p>其实LVM里还包括了缩容与快照的功能，使用场景不多，这里也不多阐述，且现在大多文件系统都为xfs，其也不支持收缩，想要了解的可自行百度。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;总所周知不论是机械硬盘还是固态磁盘大小都是固定的，当磁盘空间满了后只能通过删除无用数据来保持磁盘的可用性，这篇介绍的LVM(Logical Volume Manager)是建立在磁盘和分区之上的一个逻辑层，用来提高磁盘分区管理的灵活性，可以随时随地的扩缩容分区大小。&lt;/p&gt;</summary>
      
    
    
    
    
    <category term="linux" scheme="https://slions.github.io/tags/linux/"/>
    
  </entry>
  
  <entry>
    <title>Virtualbox虚机使用介绍</title>
    <link href="https://slions.github.io/2021/08/05/Virtualbox%E8%99%9A%E6%9C%BA%E4%BD%BF%E7%94%A8%E4%BB%8B%E7%BB%8D/"/>
    <id>https://slions.github.io/2021/08/05/Virtualbox%E8%99%9A%E6%9C%BA%E4%BD%BF%E7%94%A8%E4%BB%8B%E7%BB%8D/</id>
    <published>2021-08-05T11:24:23.000Z</published>
    <updated>2021-08-05T11:58:29.730Z</updated>
    
    <content type="html"><![CDATA[<p> VMware workstation是最常见也是最常用的虚拟机工具，但是很多公司不会购买其商用授权，VirtualBox 也是一款虚拟化产品，重要的是其<strong>开源免费</strong>，好早之前在本地安装使用了下，感觉与VMware workstation比还是不太习惯，包括虚拟机的安装和设置也要重新习惯，本文主要讲下安装好软件后如何完成基础设置与网络配置，安装部分比较简单请自行解决。</p><h1 id="新建虚机"><a href="#新建虚机" class="headerlink" title="新建虚机"></a>新建虚机</h1><ol><li>进入virtualbox界面点击新建则开始创建，设置你虚机的名字，存储位置，和基础的系统版本（之前VMware导入virtualbox会报错有冲突，如果想尝试导入的可自行百度）。</li></ol><p><img src="/doc_picture/virtualbox1.png" alt="image-20210805193414305"></p><ol start="2"><li> 下一步设置根分区的大小。</li></ol><p><img src="/doc_picture/virtualbox2.png" alt="image-20210805193500544"></p><ol start="3"><li>确定后设置你的虚机，先选择虚机的启动镜像。这里的iso就决定了你的系统版本与内核版本。</li></ol><p><img src="/doc_picture/virtualbox3.png" alt="image-20210805193540746"></p><ol start="4"><li>配置网卡模式，先那桥接模式测试，下面会说明这几个模式的区别。（之后的环境需配置成两块网卡，一个nat，一个仅主机）</li></ol><p><img src="/doc_picture/virtualbox4.png" alt="image-20210805193612460"></p><ol start="5"><li>确认无误后，启动虚机。</li></ol><p><img src="/doc_picture/virtualbox5.png" alt="image-20210805193650102"></p><ol start="6"><li>接下来的步骤就和VMware虚机安装一样了，建议选择基础设施服务器，要不默认是最小化安装，会少很多命令。（鼠标切换是方向键旁边的CTRL）</li></ol><p><img src="/doc_picture/virtualbox6.png" alt="image-20210805193717617"></p><h1 id="配置网卡"><a href="#配置网卡" class="headerlink" title="配置网卡"></a>配置网卡</h1><ol><li>进入虚机可以看到我们的网卡会通过dhcp自动获取到一个ip,为了防止重启后ip发生改变，需要我们手动配置一个同子网的ip。</li></ol><p><img src="/doc_picture/virtualbox7.png" alt="image-20210805193815935"></p><ol start="2"><li><p>vim /etc/sysconfig/network-scripts/ifcfg-enp0s3</p><p>关键的是BOOTPROTO要设置成static。</p></li></ol><p><img src="/doc_picture/virtualbox8.png" alt="image-20210805193903367"></p><ol start="3"><li>重启网卡</li></ol><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> systemctl restart network</span></span><br></pre></td></tr></table></figure><ol start="4"><li>查看网卡信息已经变成我们设置的ip，ping 百度测试ok。</li></ol><p><img src="/doc_picture/virtualbox9.png" alt="image-20210805194051944"></p><p>​    主机通过ssh连接本地虚机。ok</p><p><img src="/doc_picture/virtualbox10.png" alt="image-20210805194139658"></p><h1 id="克隆虚拟机"><a href="#克隆虚拟机" class="headerlink" title="克隆虚拟机"></a>克隆虚拟机</h1><p>处在关机状态的虚机才能克隆，点击想要克隆的虚机右键选择复制，重要的是下面的重新初始化所有网卡的MAC地址，这样就可以重新分配地址，防止两个虚拟机同时开启时发生冲突。</p><p><img src="/doc_picture/virtualbox11.png" alt="image-20210805194313661"></p><p>为了保持两个虚拟机的独立性，建议采用完全拷贝的模式。</p><p><img src="/doc_picture/virtualbox12.png" alt="image-20210805194359103"></p><p>等待克隆完成就行了，之后修改主机名，修改网卡地址，和vmware操作一样。</p><h1 id="网卡模式说明"><a href="#网卡模式说明" class="headerlink" title="网卡模式说明"></a>网卡模式说明</h1><p>我们可以在安装虚机时会让我们设置网卡的方式，这些也可以通过安装好虚机后修改。</p><p>下面解释下常用的网卡模式意义。</p><p><img src="/doc_picture/virtualbox13.png" alt="image-20210805194504261"></p><h2 id="NAT"><a href="#NAT" class="headerlink" title="NAT"></a>NAT</h2><p>Guest访问网络的所有数据都是由主机做了一层路由转换，Guest并不真实存在于网络中，主机与网络中的任何机器都不能查看和访问到Guest的存在。</p><ul><li>Guest可以访问主机能访问到的所有网络，但是对于主机以及主机网络上的其他机器，Guest又是不可见的，甚至主机也访问不到Guest。</li><li>虚拟机与主机的关系：只能单向访问，虚拟机可以通过网络访问到主机，主机无法通过网络访问到虚拟机。</li><li>虚拟机与网络中其他主机的关系：只能单向访问，虚拟机可以访问到网络中其他主机，其他主机不能通过网络访问到虚拟机。</li><li>虚拟机与虚拟机的关系：相互不能访问，虚拟机与虚拟机各自完全独立，相互间无法通过网络访问彼此。</li></ul><h2 id="Bridged-Adapter"><a href="#Bridged-Adapter" class="headerlink" title="Bridged Adapter"></a>Bridged Adapter</h2><p>它与主机网卡在用一个子网中，访问外网会直接走本地的网卡出去。这时虚拟机能被分配到一个网络中独立的IP，所有网络功能完全和在网络中的真实机器一样。</p><ul><li>虚拟机与主机的关系：可以相互访问，因为虚拟机在真实网络段中有独立IP，主机与虚拟机处于同一网络段中，彼此可以通过各自IP相互访问。</li><li>虚拟机于网络中其他主机的关系：可以相互访问，同样因为虚拟机在真实网络段中有独立IP，虚拟机与所有网络其他主机处于同一网络段中，彼此可以通过各自IP相互访问。</li><li>虚拟机与虚拟机的关系：可以相互访问，原因同上。</li></ul><h2 id="Internal（内部网络）"><a href="#Internal（内部网络）" class="headerlink" title="Internal（内部网络）"></a>Internal（内部网络）</h2><p>内网模式，顾名思义就是内部网络模式：</p><ul><li>虚拟机与外网完全断开，只实现虚拟机于虚拟机之间的内部网络模式。</li><li>虚拟机与主机的关系：不能相互访问，彼此不属于同一个网络，无法相互访问。</li><li>虚拟机与网络中其他主机的关系：不能相互访问，理由同上。</li><li>虚拟机与虚拟机的关系：可以相互访问，前提是在设置网络时，两台虚拟机设置同一网络名称。</li></ul><h2 id="Host-only-Adapter"><a href="#Host-only-Adapter" class="headerlink" title="Host-only Adapter"></a>Host-only Adapter</h2><p>仅主机模式，从名字可以看出只有当前主机可以连接，网上看到可以通过网卡共享和网卡桥接来实现访问外网，那不如直接配成桥接模式。</p><ul><li>虚拟机不可以上网</li><li>虚拟机与虚拟机的关系：可以相互访问</li><li>虚拟机与主机的关系：可以相互访问（注意虚拟机与主机通信是通过主机的名为VirtualBox Host-Only Network的网卡，因此ip是该网卡ip 192.168.56.1，而不是你现在正在上网所用的ip，可以自己配置网段）</li></ul><blockquote><p>可以看出桥接与nat都可以使虚机联网，这里建议使用<strong>双网卡</strong>（nat+仅主机模式）方式，因为桥接网卡设置为使用wifi网卡的话，则断网与更换wifi都会导致虚机不可用，所以推荐使用nat连接外网，使用仅主机来连接本地ssh终端。</p></blockquote><blockquote><p>nat默认使用10.0.2.0/24网段</p><p>仅主机默认使用192.168.56.0/24网段</p></blockquote><h1 id="修改网卡名（可选项）"><a href="#修改网卡名（可选项）" class="headerlink" title="修改网卡名（可选项）"></a>修改网卡名（可选项）</h1><p>可以看到系统默认为我们添加的网卡名是enp0s3和enp0s8，不好记也容易弄混，下面我会演示如何设置成标准的网卡名。</p><ol><li><p>修改系统启动程序文件，/etc/sysconfig/grub</p><p>在GRUB_CMDLINE_LINUX后添加<code>net.ifnames=0 biosdevname=0 </code>保存退出。</p></li></ol><p><img src="/doc_picture/virtualbox14.png" alt="image-20210805195105039"></p><p>   2.更新grub信息。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> grub2-mkconfig -o /boot/grub2/grub.cfg</span></span><br></pre></td></tr></table></figure><p><img src="/doc_picture/virtualbox15.png" alt="image-20210805195158475"></p><ol start="3"><li>重启系统</li></ol><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> reboot</span></span><br></pre></td></tr></table></figure><ol start="4"><li>查看网卡已经变成了eth0与eth1(ip要重新设置下)。</li></ol><p><img src="/doc_picture/virtualbox16.png" alt="image-20210805195316288"></p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt; VMware workstation是最常见也是最常用的虚拟机工具，但是很多公司不会购买其商用授权，VirtualBox 也是一款虚拟化产品，重要的是其&lt;strong&gt;开源免费&lt;/strong&gt;，好早之前在本地安装使用了下，感觉与VMware workstation比还是</summary>
      
    
    
    
    
    <category term="虚拟机相关" scheme="https://slions.github.io/tags/%E8%99%9A%E6%8B%9F%E6%9C%BA%E7%9B%B8%E5%85%B3/"/>
    
  </entry>
  
  <entry>
    <title>df与du结果不一致问题</title>
    <link href="https://slions.github.io/2021/08/03/df%E4%B8%8Edu%E7%BB%93%E6%9E%9C%E4%B8%8D%E4%B8%80%E8%87%B4%E9%97%AE%E9%A2%98/"/>
    <id>https://slions.github.io/2021/08/03/df%E4%B8%8Edu%E7%BB%93%E6%9E%9C%E4%B8%8D%E4%B8%80%E8%87%B4%E9%97%AE%E9%A2%98/</id>
    <published>2021-08-03T10:37:53.000Z</published>
    <updated>2021-08-03T10:56:15.644Z</updated>
    
    <content type="html"><![CDATA[<p><code>df</code>与<code>du</code>命令都是运维人员常用的检测存储空间大小的命令，平时我们并不太关注这两个命令的差别，但是经常会遇到这样一种场景，使用df查看到空间使用率已经非常高了，但使用du命令排查时发现不存在占用空间大的文件， 两者间的结果不一致。</p><h1 id="du与df"><a href="#du与df" class="headerlink" title="du与df"></a>du与df</h1><p><strong>du</strong>，disk usage,是通过搜索文件来计算每个文件的大小然后累加，du能看到的文件只是一些当前存在的，没有被删除的。他计算的大小就是当前他认为存在的所有文件大小的累加和。</p><p><strong>df</strong>，disk free，通过文件系统来快速获取空间大小的信息，当我们删除一个文件的时候，这个文件不是马上就在文件系统当中消失了，而是暂时消失了，当所有程序都不用时，才会根据OS的规则释放掉已经删除的文件， df记录的是通过文件系统获取到的文件的大小，他比du强的地方就是能够看到已经删除的文件，而且计算大小的时候，把这一部分的空间也加上了，更精确了。</p><p>因此,如果用户删除了一个正在运行的应用所打开的某个目录下的文件，则du命令返回的值显示出减去了该文件后的目录的大小。但df命令并不显示减去该文件后的大小。直到该运行的应用关闭了这个打开的文件，df返回的值才显示出减去了该文件后的文件系统的使用情况。</p><p>通过lsof工具我们可以直观的排查到具体的问题进程，以便解决：</p><h1 id="模拟案例"><a href="#模拟案例" class="headerlink" title="模拟案例"></a>模拟案例</h1><p>分别创建一个500M和1000M大小的文件。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> dd <span class="keyword">if</span>=/dev/zero of=500mFile bs=1M count=500</span></span><br><span class="line">记录了500+0 的读入</span><br><span class="line">记录了500+0 的写出</span><br><span class="line">524288000字节(524 MB)已复制，1.34728 秒，389 MB/秒</span><br><span class="line"><span class="meta">$</span><span class="bash"> dd <span class="keyword">if</span>=/dev/zero of=1000mFile bs=1M count=1000</span></span><br><span class="line">记录了1000+0 的读入</span><br><span class="line">记录了1000+0 的写出</span><br><span class="line">1048576000字节(1.0 GB)已复制，13.3469 秒，78.6 MB/秒</span><br><span class="line"><span class="meta">$</span><span class="bash"> du -ha *</span></span><br><span class="line">1000M   1000mFile</span><br><span class="line">500M    500mFile</span><br><span class="line"><span class="meta">$</span><span class="bash"> df -h `<span class="built_in">pwd</span>`</span></span><br><span class="line">文件系统                 容量  已用  可用 已用% 挂载点</span><br><span class="line">/dev/mapper/centos-root   18G  4.0G   15G   22% /</span><br></pre></td></tr></table></figure><p>然后开两个终端分别使用tail命令查看。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 终端1执行</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> tail -f 500mFile</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 终端2执行</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> tail -f 1000mFile</span></span><br></pre></td></tr></table></figure><p>再开启一个终端使用lsof命令查看这两个文件的状态。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> lsof |grep File</span></span><br><span class="line">tail      10105                root    3r      REG              253,0  524288000   51733031 /home/slions/500mFile</span><br><span class="line">tail      10106                root    3r      REG              253,0 1048576000   51738061 /home/slions/1000mFile</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>这个时候我们使用rm命令来删除了这两个文件。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> rm -rf 1000mFile 500mFile</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> du -ha .</span></span><br><span class="line">0       .</span><br><span class="line"><span class="meta">$</span><span class="bash"> df -h `<span class="built_in">pwd</span>`</span></span><br><span class="line">文件系统                 容量  已用  可用 已用% 挂载点</span><br><span class="line">/dev/mapper/centos-root   18G  4.0G   15G   22% /</span><br></pre></td></tr></table></figure><p>可以看到du已经显示为0，df无任何变化。通过lsof再看下进程状态。(sort -nrk 7是进行大小排序)</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> lsof |grep File |sort -nrk 7</span></span><br><span class="line">tail      10106                root    3r      REG              253,0 1048576000   51738061 /home/slions/1000mFile (deleted)</span><br><span class="line">tail      10105                root    3r      REG              253,0  524288000   51733031 /home/slions/500mFile (deleted)</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>这个deleted表示该已经删除了的文件，但是文件句柄未释放。</p><p>想要释放此句柄直接kill掉对应进程就好了。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> <span class="built_in">kill</span> -9 10106</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> <span class="built_in">kill</span> -9 10105</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> df -h `<span class="built_in">pwd</span>`</span></span><br><span class="line">文件系统                 容量  已用  可用 已用% 挂载点</span><br><span class="line">/dev/mapper/centos-root   18G  2.5G   16G   14% /</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>可以看到，df的已用容量对应减少了1.5G。</p><p>在日常的运维工作中，我们可以直接通过以下命令来快速定位未释放文件句柄的进程，从而进行解决。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> lsof |grep deleted</span></span><br></pre></td></tr></table></figure><h1 id="建议"><a href="#建议" class="headerlink" title="建议"></a>建议</h1><p>在日常运维过程中，如果我们需要删除比较大的文件 可以使用 <code>&gt; filename </code>，这种可以直接释放磁盘空间，使用 rm 如果有进程在访问文件，则有可能出现磁盘空间不释放的情况。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;&lt;code&gt;df&lt;/code&gt;与&lt;code&gt;du&lt;/code&gt;命令都是运维人员常用的检测存储空间大小的命令，平时我们并不太关注这两个命令的差别，但是经常会遇到这样一种场景，使用df查看到空间使用率已经非常高了，但使用du命令排查时发现不存在占用空间大的文件， 两者间的结果不</summary>
      
    
    
    
    
    <category term="linux" scheme="https://slions.github.io/tags/linux/"/>
    
  </entry>
  
  <entry>
    <title>磁盘分区工具</title>
    <link href="https://slions.github.io/2021/08/02/%E7%A3%81%E7%9B%98%E5%88%86%E5%8C%BA%E5%B7%A5%E5%85%B7/"/>
    <id>https://slions.github.io/2021/08/02/%E7%A3%81%E7%9B%98%E5%88%86%E5%8C%BA%E5%B7%A5%E5%85%B7/</id>
    <published>2021-08-02T12:40:24.000Z</published>
    <updated>2021-08-02T13:10:20.506Z</updated>
    
    <content type="html"><![CDATA[<p>在我们日常的运维工作中，磁盘分区是必备的一项技能，掌握了就可以更好的规划存储空间，提高资源的利用率。</p><p>常见的磁盘分区工具有<code>fdisk</code>、<code>parted</code>、<code>gdisk</code></p><p>使用方式有些许的差异，除了都支持交互型操作外，parted天生支持非交互的能力，而fdisk与gdisk需要我们来换种思路实现非交互式。</p><blockquote><p>关于这些工具交互式的操作命令比较简单，可以直接通过man手册来巩固，以下主要介绍下如何实现非交互式，后续使磁盘分区操作脚本化。</p></blockquote><h1 id="fdisk"><a href="#fdisk" class="headerlink" title="fdisk"></a>fdisk</h1><p>一般情况下，我们都是选择使用fdisk工具来进行分区，而常用的fdisk这个工具对分区是有大小限制的，它只能划分<strong>小于2T</strong>的磁盘，所以在划大于2T磁盘分区的时候fdisk就无法满足要求了。</p><p>fdisk工具用来分MBR磁盘上的区。要分GPT磁盘上的区，可以使用gdisk。parted工具对这两种格式的磁盘分区都支持。</p><p>fdisk操作全部是在内存中执行的，必须保存生效。保存后，内核还未识别该分区，可以查看/proc/partition目录下存在的文件，这些文件是能被内核识别的分区。运行partprobe或partx命令重新读取分区表让内核识别新的分区，内核识别后才可以格式化。而且分区结束时按w保存分区表有时候会失败，提示重启，这时候运行partprobe命令可以代替重启就生效。</p><h1 id="gdisk"><a href="#gdisk" class="headerlink" title="gdisk"></a>gdisk</h1><p>gdisk用来划分gpt分区，需要单独安装这个工具包。</p><h1 id="Parted"><a href="#Parted" class="headerlink" title="Parted"></a>Parted</h1><p>parted支持mbr格式和gpt格式的磁盘分区。它的强大在于可以一步到位而不需要不断的交互式输入(也可以交互式)。</p><p>parted分区工具是实时的，所以每一步操作都是直接写入磁盘而不是写进内存，它不像fdisk/gdisk还需要w命令将内存中的结果保存到磁盘中。</p><h1 id="fdisk实现非交互"><a href="#fdisk实现非交互" class="headerlink" title="fdisk实现非交互"></a>fdisk实现非交互</h1><p>fdisk实现非交互的原理是从标准输入中读取，每读取一行传递一次操作。</p><p>所以可以有两种方式：</p><ul><li>使用echo和管道传递</li><li>将操作写入到文件中，从文件中读取。</li></ul><p>例如：下面的命令创建了两个分区。使用默认值时传递空行即可。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> <span class="built_in">echo</span> -e <span class="string">&quot;n\np\n1\n\n+5G\nn\np\n2\n\n+1G\nw\n&quot;</span>  | fdisk /dev/sdb</span></span><br></pre></td></tr></table></figure><p>如果要传递的操作很多，则可以将它们写入到一个文件中，从文件中读取。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> <span class="built_in">echo</span> -e <span class="string">&quot;n\np\n1\n\n+5G\nn\np\n2\n\n+1G\nw\n&quot;</span> &gt;/tmp/a.txt</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> fdisk /dev/sdb &lt;/tmp/a.txt</span></span><br></pre></td></tr></table></figure><h1 id="gdisk实现非交互"><a href="#gdisk实现非交互" class="headerlink" title="gdisk实现非交互"></a>gdisk实现非交互</h1><p>原理同fdisk。</p><p>例如：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> <span class="built_in">echo</span> -e <span class="string">&quot;n\n1\n\n+3G\n\nw\nY\n&quot;</span> | gdisk /dev/sdb</span></span><br></pre></td></tr></table></figure><p>上面传递的各参数意义为：</p><p>新建分区，分区number为1，使用默认开始扇区位置，分区大小+3G，使用默认分区类型，保存，确认。</p><h1 id="parted实现非交互"><a href="#parted实现非交互" class="headerlink" title="parted实现非交互"></a>parted实现非交互</h1><p>parted命令只能一次非交互一个命令中的所有动作。如下所示：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">parted /dev/sdb mklabel msdos                 # 设置硬盘flag(msdos/gpt)</span><br><span class="line">parted /dev/sdb mkpart primary ext4 1 1000   # Mbr格式分区，分别是partition type/fstype/start/end</span><br><span class="line">parted /dev/sdb mkpart 1 ext4 1M 10240M      # gpt格式分区，分别是name/fstype/start/end</span><br><span class="line">parted /dev/sdb mkpart 1 10G 15G             # 省略fstype的交互式分区</span><br><span class="line">parted /dev/sdb rm 1                         # 删除分区</span><br><span class="line">parted /dev/sdb p                            # 输出信息</span><br></pre></td></tr></table></figure><p>如果不确定分区的起点大小，可以加上-s选项使用script模式，该模式下parted将回答一切默认值，如yes、no。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;在我们日常的运维工作中，磁盘分区是必备的一项技能，掌握了就可以更好的规划存储空间，提高资源的利用率。&lt;/p&gt;
&lt;p&gt;常见的磁盘分区工具有&lt;code&gt;fdisk&lt;/code&gt;、&lt;code&gt;parted&lt;/code&gt;、&lt;code&gt;gdisk&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;使用方式</summary>
      
    
    
    
    
    <category term="linux" scheme="https://slions.github.io/tags/linux/"/>
    
  </entry>
  
  <entry>
    <title>ceph mimic对接k8s 1.17.0(ceph-csi)</title>
    <link href="https://slions.github.io/2021/08/02/ceph%20mimic%E5%AF%B9%E6%8E%A5k8s%201.17.0/"/>
    <id>https://slions.github.io/2021/08/02/ceph%20mimic%E5%AF%B9%E6%8E%A5k8s%201.17.0/</id>
    <published>2021-08-02T03:57:37.000Z</published>
    <updated>2021-08-02T05:51:22.539Z</updated>
    
    <content type="html"><![CDATA[<p>最近测试ceph rbd在kubernetes的自动扩容问题，之前K8s v1.11.0时的策略是先找到目标卷，使用rbd resize命令对此卷扩容，找到挂载此卷的客户端宿主机，执行xfs_growfs等刷新文件系统的命令。查看网上资料k8s 在1.15版本后，ExpandInUsePersistentVolume功能被开启。意思大概就是不需要挂载到容器即可扩容PVC。按网上的手册设置相关参数并没啥用，发现github有人提到ceph-csi可以实现自动扩容pvc的功能。</p><p><img src="/doc_picture/ceph-1.png" alt="image-20210802120109616"></p><p>这里简单说下csi是啥，全称是Container Storage Interface，旨在能为容器编排引擎和存储系统间建立一套标准的存储调用接口，通过该接口能为容器编排引擎提供存储服务。</p><p>csi之前，k8s提供的存储服务通过一种“in-tree”的方式提供的，这种方式需要将存储提供者的代码逻辑放到K8S的代码库中运行，调用引擎与插件间属于强耦合。</p><h1 id="环境描述"><a href="#环境描述" class="headerlink" title="环境描述"></a>环境描述</h1><p>k8s版本：kubernetes  v1.17.0</p><p>ceph版本：ceph mimic</p><table><thead><tr><th><strong>主机名</strong></th><th>ip地址</th><th><strong>系统版本</strong>\内核版本</th><th><strong>角色</strong></th></tr></thead><tbody><tr><td>ceph1</td><td>192.168.186.10</td><td>centos 7.6\3.10.0-957.el7.x86_64</td><td>K8s_master,ceph mon,osd,mds</td></tr><tr><td>ceph2</td><td>192.168.186.11</td><td>centos 7.6\3.10.0-957.el7.x86_64</td><td>K8s_node,ceph mon,osd,mds</td></tr><tr><td>ceph3</td><td>192.168.186.12</td><td>centos 7.6\3.10.0-957.el7.x86_64</td><td>K8s_node,ceph mon,osd,mds</td></tr></tbody></table><h1 id="整体架构图"><a href="#整体架构图" class="headerlink" title="整体架构图"></a>整体架构图</h1><blockquote><p>根据ceph官方描述：<br>ceph-csi默认情况下使用RBD内核模块，这些模块可能不支持所有Ceph CRUSH可调参数或RBD图像功能。</p></blockquote><p><img src="/doc_picture/ceph-2.png" alt="image-20210802121043578"></p><h1 id="安装步骤"><a href="#安装步骤" class="headerlink" title="安装步骤"></a>安装步骤</h1><p>我本地已经提前安装好了kubernetes与ceph,以下仅叙述如何对接。</p><h2 id="1-创建存储池"><a href="#1-创建存储池" class="headerlink" title="1.    创建存储池"></a>1.    创建存储池</h2><p>ceph在L版本之后就不会创建默认的rbd池了，我们需要建立一个单独的存储池给kubernetes使用。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> ceph osd pool create kubernetes 64 64</span></span><br></pre></td></tr></table></figure><p>初始化新创建的池。</p><blockquote><p>这里的初始化池操作在jewel版本是不需要的，jewel之后的版本在创建了池后还需要开启对应的应用授权（rbd,cephfs,rgw）,命令为</p><p><code>ceph osd pool application enable &lt;pool-name&gt; &lt;app-name&gt;</code></p></blockquote><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> rbd pool init kubernetes</span></span><br></pre></td></tr></table></figure><p>这里查看创建的kubernetes池自动加入了rbd池。</p><p><img src="/doc_picture/ceph-3.png" alt="image-20210802121704441"></p><h2 id="2-配置ceph-csi"><a href="#2-配置ceph-csi" class="headerlink" title="2.    配置ceph-csi"></a>2.    配置ceph-csi</h2><p>设置ceph客户端身份验证。</p><blockquote><p>官方提供的命令是：</p><p><code>ceph auth get-or-create client.kubernetes mon &#39;profile rbd&#39; osd &#39;profile rbd pool=kubernetes&#39; mgr &#39;profile rbd pool=kubernetes&#39;</code></p></blockquote><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> ceph auth get-or-create client.kubernetes mon <span class="string">&#x27;allow r&#x27;</span> osd <span class="string">&#x27;allow rwx pool=kubernetes&#x27;</span> -o ceph.client.kubernetes.keyring</span></span><br></pre></td></tr></table></figure><p>生成文件中的key使用user的key，后面配置中是需要用到的</p><p><img src="/doc_picture/ceph-4.png" alt="image-20210802121940950"></p><h2 id="3-生成ceph-csi的configmap"><a href="#3-生成ceph-csi的configmap" class="headerlink" title="3.    生成ceph-csi的configmap"></a>3.    生成ceph-csi的configmap</h2><p><img src="/doc_picture/ceph-5.png" alt="image-20210802122044038"></p><p>这里一共有两个需要使用的信息，第一个是fsid(集群id)，第二个是监控节点信息。</p><blockquote><p>看到有人查询到的监控节点信息有2个版本（v1和v2），目前的ceph-csi只支持V1版本的协议，所以监控节点那里我们只能用v1的那个IP和端口号，我这里不需要改动</p></blockquote><p>编写对应的configmap。</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">ConfigMap</span></span><br><span class="line"><span class="attr">data:</span></span><br><span class="line">  <span class="attr">config.json:</span> <span class="string">|-</span></span><br><span class="line"><span class="string">    [</span></span><br><span class="line"><span class="string">      &#123;</span></span><br><span class="line"><span class="string">        &quot;clusterID&quot;: &quot;10594fb3-68f3-4c97-8e0b-df80ba2a6745&quot;,</span></span><br><span class="line"><span class="string">        &quot;monitors&quot;: [</span></span><br><span class="line"><span class="string">          &quot;192.168.186.10:6789&quot;,</span></span><br><span class="line"><span class="string">          &quot;192.168.186.11:6789&quot;,</span></span><br><span class="line"><span class="string">          &quot;192.168.186.12:6789&quot;</span></span><br><span class="line"><span class="string">        ]</span></span><br><span class="line"><span class="string">      &#125;</span></span><br><span class="line"><span class="string">    ]</span></span><br><span class="line"><span class="string"></span><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">ceph-csi-config</span></span><br></pre></td></tr></table></figure><p>部署：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> kubectl apply -f csi-config-map.yaml</span></span><br></pre></td></tr></table></figure><h2 id="4-生成ceph-csi认证的secret"><a href="#4-生成ceph-csi认证的secret" class="headerlink" title="4.    生成ceph-csi认证的secret"></a>4.    生成ceph-csi认证的secret</h2><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Secret</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">csi-rbd-secret</span></span><br><span class="line">  <span class="attr">namespace:</span> <span class="string">default</span></span><br><span class="line"><span class="attr">stringData:</span></span><br><span class="line">  <span class="attr">userID:</span> <span class="string">kubernetes</span></span><br><span class="line">  <span class="attr">userKey:</span> <span class="string">AQBEpRdf2MXxFxAA8JGQQhTX1XIHPSSbw72Gqw==</span></span><br></pre></td></tr></table></figure><p>这里就用到了之前生成的用户的用户id(kubernetes)和key</p><p>部署：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> kubectl apply -f csi-rbd-secret.yaml</span></span><br></pre></td></tr></table></figure><h2 id="5-配置ceph-csi插件"><a href="#5-配置ceph-csi插件" class="headerlink" title="5.    配置ceph-csi插件"></a>5.    配置ceph-csi插件</h2><p>这里的插件就是配置kubernetes上的rbac和提供存储功能的容器。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> kubectl apply -f csi-provisioner-rbac.yaml</span></span><br></pre></td></tr></table></figure><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">$</span> <span class="string">cat</span> <span class="string">csi-provisioner-rbac.yaml</span></span><br><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">ServiceAccount</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">rbd-csi-provisioner</span></span><br><span class="line"></span><br><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">ClusterRole</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">rbac.authorization.k8s.io/v1</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">rbd-external-provisioner-runner</span></span><br><span class="line"><span class="attr">aggregationRule:</span></span><br><span class="line">  <span class="attr">clusterRoleSelectors:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">matchLabels:</span></span><br><span class="line">        <span class="attr">rbac.rbd.csi.ceph.com/aggregate-to-rbd-external-provisioner-runner:</span> <span class="string">&quot;true&quot;</span></span><br><span class="line"><span class="attr">rules:</span> []</span><br><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">ClusterRole</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">rbac.authorization.k8s.io/v1</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">rbd-external-provisioner-runner-rules</span></span><br><span class="line">  <span class="attr">labels:</span></span><br><span class="line">    <span class="attr">rbac.rbd.csi.ceph.com/aggregate-to-rbd-external-provisioner-runner:</span> <span class="string">&quot;true&quot;</span></span><br><span class="line"><span class="attr">rules:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">apiGroups:</span> [<span class="string">&quot;&quot;</span>]</span><br><span class="line">    <span class="attr">resources:</span> [<span class="string">&quot;nodes&quot;</span>]</span><br><span class="line">    <span class="attr">verbs:</span> [<span class="string">&quot;get&quot;</span>, <span class="string">&quot;list&quot;</span>, <span class="string">&quot;watch&quot;</span>]</span><br><span class="line">  <span class="bullet">-</span> <span class="attr">apiGroups:</span> [<span class="string">&quot;&quot;</span>]</span><br><span class="line">    <span class="attr">resources:</span> [<span class="string">&quot;secrets&quot;</span>]</span><br><span class="line">    <span class="attr">verbs:</span> [<span class="string">&quot;get&quot;</span>, <span class="string">&quot;list&quot;</span>]</span><br><span class="line">  <span class="bullet">-</span> <span class="attr">apiGroups:</span> [<span class="string">&quot;&quot;</span>]</span><br><span class="line">    <span class="attr">resources:</span> [<span class="string">&quot;events&quot;</span>]</span><br><span class="line">    <span class="attr">verbs:</span> [<span class="string">&quot;list&quot;</span>, <span class="string">&quot;watch&quot;</span>, <span class="string">&quot;create&quot;</span>, <span class="string">&quot;update&quot;</span>, <span class="string">&quot;patch&quot;</span>]</span><br><span class="line">  <span class="bullet">-</span> <span class="attr">apiGroups:</span> [<span class="string">&quot;&quot;</span>]</span><br><span class="line">    <span class="attr">resources:</span> [<span class="string">&quot;persistentvolumes&quot;</span>]</span><br><span class="line">    <span class="attr">verbs:</span> [<span class="string">&quot;get&quot;</span>, <span class="string">&quot;list&quot;</span>, <span class="string">&quot;watch&quot;</span>, <span class="string">&quot;create&quot;</span>, <span class="string">&quot;update&quot;</span>, <span class="string">&quot;delete&quot;</span>, <span class="string">&quot;patch&quot;</span>]</span><br><span class="line">  <span class="bullet">-</span> <span class="attr">apiGroups:</span> [<span class="string">&quot;&quot;</span>]</span><br><span class="line">    <span class="attr">resources:</span> [<span class="string">&quot;persistentvolumeclaims&quot;</span>]</span><br><span class="line">    <span class="attr">verbs:</span> [<span class="string">&quot;get&quot;</span>, <span class="string">&quot;list&quot;</span>, <span class="string">&quot;watch&quot;</span>, <span class="string">&quot;update&quot;</span>]</span><br><span class="line">  <span class="bullet">-</span> <span class="attr">apiGroups:</span> [<span class="string">&quot;&quot;</span>]</span><br><span class="line">    <span class="attr">resources:</span> [<span class="string">&quot;persistentvolumeclaims/status&quot;</span>]</span><br><span class="line">    <span class="attr">verbs:</span> [<span class="string">&quot;update&quot;</span>, <span class="string">&quot;patch&quot;</span>]</span><br><span class="line">  <span class="bullet">-</span> <span class="attr">apiGroups:</span> [<span class="string">&quot;storage.k8s.io&quot;</span>]</span><br><span class="line">    <span class="attr">resources:</span> [<span class="string">&quot;storageclasses&quot;</span>]</span><br><span class="line">    <span class="attr">verbs:</span> [<span class="string">&quot;get&quot;</span>, <span class="string">&quot;list&quot;</span>, <span class="string">&quot;watch&quot;</span>]</span><br><span class="line">  <span class="bullet">-</span> <span class="attr">apiGroups:</span> [<span class="string">&quot;snapshot.storage.k8s.io&quot;</span>]</span><br><span class="line">    <span class="attr">resources:</span> [<span class="string">&quot;volumesnapshots&quot;</span>]</span><br><span class="line">    <span class="attr">verbs:</span> [<span class="string">&quot;get&quot;</span>, <span class="string">&quot;list&quot;</span>]</span><br><span class="line">  <span class="bullet">-</span> <span class="attr">apiGroups:</span> [<span class="string">&quot;snapshot.storage.k8s.io&quot;</span>]</span><br><span class="line">    <span class="attr">resources:</span> [<span class="string">&quot;volumesnapshotcontents&quot;</span>]</span><br><span class="line">    <span class="attr">verbs:</span> [<span class="string">&quot;create&quot;</span>, <span class="string">&quot;get&quot;</span>, <span class="string">&quot;list&quot;</span>, <span class="string">&quot;watch&quot;</span>, <span class="string">&quot;update&quot;</span>, <span class="string">&quot;delete&quot;</span>]</span><br><span class="line">  <span class="bullet">-</span> <span class="attr">apiGroups:</span> [<span class="string">&quot;snapshot.storage.k8s.io&quot;</span>]</span><br><span class="line">    <span class="attr">resources:</span> [<span class="string">&quot;volumesnapshotclasses&quot;</span>]</span><br><span class="line">    <span class="attr">verbs:</span> [<span class="string">&quot;get&quot;</span>, <span class="string">&quot;list&quot;</span>, <span class="string">&quot;watch&quot;</span>]</span><br><span class="line">  <span class="bullet">-</span> <span class="attr">apiGroups:</span> [<span class="string">&quot;storage.k8s.io&quot;</span>]</span><br><span class="line">    <span class="attr">resources:</span> [<span class="string">&quot;volumeattachments&quot;</span>]</span><br><span class="line">    <span class="attr">verbs:</span> [<span class="string">&quot;get&quot;</span>, <span class="string">&quot;list&quot;</span>, <span class="string">&quot;watch&quot;</span>, <span class="string">&quot;update&quot;</span>, <span class="string">&quot;patch&quot;</span>]</span><br><span class="line">  <span class="bullet">-</span> <span class="attr">apiGroups:</span> [<span class="string">&quot;storage.k8s.io&quot;</span>]</span><br><span class="line">    <span class="attr">resources:</span> [<span class="string">&quot;csinodes&quot;</span>]</span><br><span class="line">    <span class="attr">verbs:</span> [<span class="string">&quot;get&quot;</span>, <span class="string">&quot;list&quot;</span>, <span class="string">&quot;watch&quot;</span>]</span><br><span class="line">  <span class="bullet">-</span> <span class="attr">apiGroups:</span> [<span class="string">&quot;snapshot.storage.k8s.io&quot;</span>]</span><br><span class="line">    <span class="attr">resources:</span> [<span class="string">&quot;volumesnapshotcontents/status&quot;</span>]</span><br><span class="line">    <span class="attr">verbs:</span> [<span class="string">&quot;update&quot;</span>]</span><br><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">ClusterRoleBinding</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">rbac.authorization.k8s.io/v1</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">rbd-csi-provisioner-role</span></span><br><span class="line"><span class="attr">subjects:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">kind:</span> <span class="string">ServiceAccount</span></span><br><span class="line">    <span class="attr">name:</span> <span class="string">rbd-csi-provisioner</span></span><br><span class="line">    <span class="attr">namespace:</span> <span class="string">default</span></span><br><span class="line"><span class="attr">roleRef:</span></span><br><span class="line">  <span class="attr">kind:</span> <span class="string">ClusterRole</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">rbd-external-provisioner-runner</span></span><br><span class="line">  <span class="attr">apiGroup:</span> <span class="string">rbac.authorization.k8s.io</span></span><br><span class="line"></span><br><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Role</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">rbac.authorization.k8s.io/v1</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="comment"># replace with non-default namespace name</span></span><br><span class="line">  <span class="attr">namespace:</span> <span class="string">default</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">rbd-external-provisioner-cfg</span></span><br><span class="line"><span class="attr">rules:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">apiGroups:</span> [<span class="string">&quot;&quot;</span>]</span><br><span class="line">    <span class="attr">resources:</span> [<span class="string">&quot;configmaps&quot;</span>]</span><br><span class="line">    <span class="attr">verbs:</span> [<span class="string">&quot;get&quot;</span>, <span class="string">&quot;list&quot;</span>, <span class="string">&quot;watch&quot;</span>, <span class="string">&quot;create&quot;</span>, <span class="string">&quot;delete&quot;</span>]</span><br><span class="line">  <span class="bullet">-</span> <span class="attr">apiGroups:</span> [<span class="string">&quot;coordination.k8s.io&quot;</span>]</span><br><span class="line">    <span class="attr">resources:</span> [<span class="string">&quot;leases&quot;</span>]</span><br><span class="line">    <span class="attr">verbs:</span> [<span class="string">&quot;get&quot;</span>, <span class="string">&quot;watch&quot;</span>, <span class="string">&quot;list&quot;</span>, <span class="string">&quot;delete&quot;</span>, <span class="string">&quot;update&quot;</span>, <span class="string">&quot;create&quot;</span>]</span><br><span class="line"></span><br><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">RoleBinding</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">rbac.authorization.k8s.io/v1</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">rbd-csi-provisioner-role-cfg</span></span><br><span class="line">  <span class="comment"># replace with non-default namespace name</span></span><br><span class="line">  <span class="attr">namespace:</span> <span class="string">default</span></span><br><span class="line"><span class="attr">subjects:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">kind:</span> <span class="string">ServiceAccount</span></span><br><span class="line">    <span class="attr">name:</span> <span class="string">rbd-csi-provisioner</span></span><br><span class="line">    <span class="comment"># replace with non-default namespace name</span></span><br><span class="line">    <span class="attr">namespace:</span> <span class="string">default</span></span><br><span class="line"><span class="attr">roleRef:</span></span><br><span class="line">  <span class="attr">kind:</span> <span class="string">Role</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">rbd-external-provisioner-cfg</span></span><br><span class="line">  <span class="attr">apiGroup:</span> <span class="string">rbac.authorization.k8s.io</span></span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> kubectl apply -f csi-nodeplugin-rbac.yaml</span></span><br></pre></td></tr></table></figure><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">$</span> <span class="string">cat</span> <span class="string">csi-nodeplugin-rbac.yaml</span></span><br><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">ServiceAccount</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">rbd-csi-nodeplugin</span></span><br><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">ClusterRole</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">rbac.authorization.k8s.io/v1</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">rbd-csi-nodeplugin</span></span><br><span class="line"><span class="attr">aggregationRule:</span></span><br><span class="line">  <span class="attr">clusterRoleSelectors:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">matchLabels:</span></span><br><span class="line">        <span class="attr">rbac.rbd.csi.ceph.com/aggregate-to-rbd-csi-nodeplugin:</span> <span class="string">&quot;true&quot;</span></span><br><span class="line"><span class="attr">rules:</span> []</span><br><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">ClusterRole</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">rbac.authorization.k8s.io/v1</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">rbd-csi-nodeplugin-rules</span></span><br><span class="line">  <span class="attr">labels:</span></span><br><span class="line">    <span class="attr">rbac.rbd.csi.ceph.com/aggregate-to-rbd-csi-nodeplugin:</span> <span class="string">&quot;true&quot;</span></span><br><span class="line"><span class="attr">rules:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">apiGroups:</span> [<span class="string">&quot;&quot;</span>]</span><br><span class="line">    <span class="attr">resources:</span> [<span class="string">&quot;nodes&quot;</span>]</span><br><span class="line">    <span class="attr">verbs:</span> [<span class="string">&quot;get&quot;</span>]</span><br><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">ClusterRoleBinding</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">rbac.authorization.k8s.io/v1</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">rbd-csi-nodeplugin</span></span><br><span class="line"><span class="attr">subjects:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">kind:</span> <span class="string">ServiceAccount</span></span><br><span class="line">    <span class="attr">name:</span> <span class="string">rbd-csi-nodeplugin</span></span><br><span class="line">    <span class="attr">namespace:</span> <span class="string">default</span></span><br><span class="line"><span class="attr">roleRef:</span></span><br><span class="line">  <span class="attr">kind:</span> <span class="string">ClusterRole</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">rbd-csi-nodeplugin</span></span><br><span class="line">  <span class="attr">apiGroup:</span> <span class="string">rbac.authorization.k8s.io</span></span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> kubectl apply -f csi-rbdplugin-provisioner.yaml</span></span><br></pre></td></tr></table></figure><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">$</span> <span class="string">cat</span> <span class="string">csi-rbdplugin-provisioner.yaml</span></span><br><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Service</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">csi-rbdplugin-provisioner</span></span><br><span class="line">  <span class="attr">labels:</span></span><br><span class="line">    <span class="attr">app:</span> <span class="string">csi-metrics</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">selector:</span></span><br><span class="line">    <span class="attr">app:</span> <span class="string">csi-rbdplugin-provisioner</span></span><br><span class="line">  <span class="attr">ports:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">http-metrics</span></span><br><span class="line">      <span class="attr">port:</span> <span class="number">8080</span></span><br><span class="line">      <span class="attr">protocol:</span> <span class="string">TCP</span></span><br><span class="line">      <span class="attr">targetPort:</span> <span class="number">8680</span></span><br><span class="line"></span><br><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Deployment</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">apps/v1</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">csi-rbdplugin-provisioner</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">replicas:</span> <span class="number">3</span></span><br><span class="line">  <span class="attr">selector:</span></span><br><span class="line">    <span class="attr">matchLabels:</span></span><br><span class="line">      <span class="attr">app:</span> <span class="string">csi-rbdplugin-provisioner</span></span><br><span class="line">  <span class="attr">template:</span></span><br><span class="line">    <span class="attr">metadata:</span></span><br><span class="line">      <span class="attr">labels:</span></span><br><span class="line">        <span class="attr">app:</span> <span class="string">csi-rbdplugin-provisioner</span></span><br><span class="line">    <span class="attr">spec:</span></span><br><span class="line">      <span class="attr">serviceAccount:</span> <span class="string">rbd-csi-provisioner</span></span><br><span class="line">      <span class="attr">containers:</span></span><br><span class="line">        <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">csi-provisioner</span></span><br><span class="line">          <span class="attr">image:</span> <span class="string">quay.io/k8scsi/csi-provisioner:v1.6.0</span></span><br><span class="line">          <span class="attr">args:</span></span><br><span class="line">            <span class="bullet">-</span> <span class="string">&quot;--csi-address=$(ADDRESS)&quot;</span></span><br><span class="line">            <span class="bullet">-</span> <span class="string">&quot;--v=5&quot;</span></span><br><span class="line">            <span class="bullet">-</span> <span class="string">&quot;--timeout=150s&quot;</span></span><br><span class="line">            <span class="bullet">-</span> <span class="string">&quot;--retry-interval-start=500ms&quot;</span></span><br><span class="line">            <span class="bullet">-</span> <span class="string">&quot;--enable-leader-election=true&quot;</span></span><br><span class="line">            <span class="bullet">-</span> <span class="string">&quot;--leader-election-type=leases&quot;</span></span><br><span class="line">            <span class="bullet">-</span> <span class="string">&quot;--feature-gates=Topology=true&quot;</span></span><br><span class="line">          <span class="attr">env:</span></span><br><span class="line">            <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">ADDRESS</span></span><br><span class="line">              <span class="attr">value:</span> <span class="string">unix:///csi/csi-provisioner.sock</span></span><br><span class="line">          <span class="attr">imagePullPolicy:</span> <span class="string">&quot;IfNotPresent&quot;</span></span><br><span class="line">          <span class="attr">volumeMounts:</span></span><br><span class="line">            <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">socket-dir</span></span><br><span class="line">              <span class="attr">mountPath:</span> <span class="string">/csi</span></span><br><span class="line">        <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">csi-snapshotter</span></span><br><span class="line">          <span class="attr">image:</span> <span class="string">quay.io/k8scsi/csi-snapshotter:v2.1.0</span></span><br><span class="line">          <span class="attr">args:</span></span><br><span class="line">            <span class="bullet">-</span> <span class="string">&quot;--csi-address=$(ADDRESS)&quot;</span></span><br><span class="line">            <span class="bullet">-</span> <span class="string">&quot;--v=5&quot;</span></span><br><span class="line">            <span class="bullet">-</span> <span class="string">&quot;--timeout=150s&quot;</span></span><br><span class="line">            <span class="bullet">-</span> <span class="string">&quot;--leader-election=true&quot;</span></span><br><span class="line">          <span class="attr">env:</span></span><br><span class="line">            <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">ADDRESS</span></span><br><span class="line">              <span class="attr">value:</span> <span class="string">unix:///csi/csi-provisioner.sock</span></span><br><span class="line">          <span class="attr">imagePullPolicy:</span> <span class="string">&quot;IfNotPresent&quot;</span></span><br><span class="line">          <span class="attr">securityContext:</span></span><br><span class="line">            <span class="attr">privileged:</span> <span class="literal">true</span></span><br><span class="line">          <span class="attr">volumeMounts:</span></span><br><span class="line">            <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">socket-dir</span></span><br><span class="line">              <span class="attr">mountPath:</span> <span class="string">/csi</span></span><br><span class="line">        <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">csi-attacher</span></span><br><span class="line">          <span class="attr">image:</span> <span class="string">quay.io/k8scsi/csi-attacher:v2.1.1</span></span><br><span class="line">          <span class="attr">args:</span></span><br><span class="line">            <span class="bullet">-</span> <span class="string">&quot;--v=5&quot;</span></span><br><span class="line">            <span class="bullet">-</span> <span class="string">&quot;--csi-address=$(ADDRESS)&quot;</span></span><br><span class="line">            <span class="bullet">-</span> <span class="string">&quot;--leader-election=true&quot;</span></span><br><span class="line">            <span class="bullet">-</span> <span class="string">&quot;--retry-interval-start=500ms&quot;</span></span><br><span class="line">          <span class="attr">env:</span></span><br><span class="line">            <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">ADDRESS</span></span><br><span class="line">              <span class="attr">value:</span> <span class="string">/csi/csi-provisioner.sock</span></span><br><span class="line">          <span class="attr">imagePullPolicy:</span> <span class="string">&quot;IfNotPresent&quot;</span></span><br><span class="line">          <span class="attr">volumeMounts:</span></span><br><span class="line">            <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">socket-dir</span></span><br><span class="line">              <span class="attr">mountPath:</span> <span class="string">/csi</span></span><br><span class="line">        <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">csi-resizer</span></span><br><span class="line">          <span class="attr">image:</span> <span class="string">quay.io/k8scsi/csi-resizer:v0.5.0</span></span><br><span class="line">          <span class="attr">args:</span></span><br><span class="line">            <span class="bullet">-</span> <span class="string">&quot;--csi-address=$(ADDRESS)&quot;</span></span><br><span class="line">            <span class="bullet">-</span> <span class="string">&quot;--v=5&quot;</span></span><br><span class="line">            <span class="bullet">-</span> <span class="string">&quot;--csiTimeout=150s&quot;</span></span><br><span class="line">            <span class="bullet">-</span> <span class="string">&quot;--leader-election&quot;</span></span><br><span class="line">            <span class="bullet">-</span> <span class="string">&quot;--retry-interval-start=500ms&quot;</span></span><br><span class="line">          <span class="attr">env:</span></span><br><span class="line">            <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">ADDRESS</span></span><br><span class="line">              <span class="attr">value:</span> <span class="string">unix:///csi/csi-provisioner.sock</span></span><br><span class="line">          <span class="attr">imagePullPolicy:</span> <span class="string">&quot;IfNotPresent&quot;</span></span><br><span class="line">          <span class="attr">volumeMounts:</span></span><br><span class="line">            <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">socket-dir</span></span><br><span class="line">              <span class="attr">mountPath:</span> <span class="string">/csi</span></span><br><span class="line">        <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">csi-rbdplugin</span></span><br><span class="line">          <span class="attr">securityContext:</span></span><br><span class="line">            <span class="attr">privileged:</span> <span class="literal">true</span></span><br><span class="line">            <span class="attr">capabilities:</span></span><br><span class="line">              <span class="attr">add:</span> [<span class="string">&quot;SYS_ADMIN&quot;</span>]</span><br><span class="line">          <span class="comment"># for stable functionality replace canary with latest release version</span></span><br><span class="line">          <span class="attr">image:</span> <span class="string">quay.io/cephcsi/cephcsi:canary</span></span><br><span class="line">          <span class="attr">args:</span></span><br><span class="line">            <span class="bullet">-</span> <span class="string">&quot;--nodeid=$(NODE_ID)&quot;</span></span><br><span class="line">            <span class="bullet">-</span> <span class="string">&quot;--type=rbd&quot;</span></span><br><span class="line">            <span class="bullet">-</span> <span class="string">&quot;--controllerserver=true&quot;</span></span><br><span class="line">            <span class="bullet">-</span> <span class="string">&quot;--endpoint=$(CSI_ENDPOINT)&quot;</span></span><br><span class="line">            <span class="bullet">-</span> <span class="string">&quot;--v=5&quot;</span></span><br><span class="line">            <span class="bullet">-</span> <span class="string">&quot;--drivername=rbd.csi.ceph.com&quot;</span></span><br><span class="line">            <span class="bullet">-</span> <span class="string">&quot;--pidlimit=-1&quot;</span></span><br><span class="line">            <span class="bullet">-</span> <span class="string">&quot;--rbdhardmaxclonedepth=8&quot;</span></span><br><span class="line">            <span class="bullet">-</span> <span class="string">&quot;--rbdsoftmaxclonedepth=4&quot;</span></span><br><span class="line">          <span class="attr">env:</span></span><br><span class="line">            <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">POD_IP</span></span><br><span class="line">              <span class="attr">valueFrom:</span></span><br><span class="line">                <span class="attr">fieldRef:</span></span><br><span class="line">                  <span class="attr">fieldPath:</span> <span class="string">status.podIP</span></span><br><span class="line">            <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">NODE_ID</span></span><br><span class="line">              <span class="attr">valueFrom:</span></span><br><span class="line">                <span class="attr">fieldRef:</span></span><br><span class="line">                  <span class="attr">fieldPath:</span> <span class="string">spec.nodeName</span></span><br><span class="line">            <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">CSI_ENDPOINT</span></span><br><span class="line">              <span class="attr">value:</span> <span class="string">unix:///csi/csi-provisioner.sock</span></span><br><span class="line">          <span class="attr">imagePullPolicy:</span> <span class="string">&quot;IfNotPresent&quot;</span></span><br><span class="line">          <span class="attr">volumeMounts:</span></span><br><span class="line">            <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">socket-dir</span></span><br><span class="line">              <span class="attr">mountPath:</span> <span class="string">/csi</span></span><br><span class="line">            <span class="bullet">-</span> <span class="attr">mountPath:</span> <span class="string">/dev</span></span><br><span class="line">              <span class="attr">name:</span> <span class="string">host-dev</span></span><br><span class="line">            <span class="bullet">-</span> <span class="attr">mountPath:</span> <span class="string">/sys</span></span><br><span class="line">              <span class="attr">name:</span> <span class="string">host-sys</span></span><br><span class="line">            <span class="bullet">-</span> <span class="attr">mountPath:</span> <span class="string">/lib/modules</span></span><br><span class="line">              <span class="attr">name:</span> <span class="string">lib-modules</span></span><br><span class="line">              <span class="attr">readOnly:</span> <span class="literal">true</span></span><br><span class="line">            <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">ceph-csi-config</span></span><br><span class="line">              <span class="attr">mountPath:</span> <span class="string">/etc/ceph-csi-config/</span></span><br><span class="line">           <span class="comment"># - name: ceph-csi-encryption-kms-config</span></span><br><span class="line">           <span class="comment">#   mountPath: /etc/ceph-csi-encryption-kms-config/</span></span><br><span class="line">            <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">keys-tmp-dir</span></span><br><span class="line">              <span class="attr">mountPath:</span> <span class="string">/tmp/csi/keys</span></span><br><span class="line">        <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">liveness-prometheus</span></span><br><span class="line">          <span class="attr">image:</span> <span class="string">quay.io/cephcsi/cephcsi:canary</span></span><br><span class="line">          <span class="attr">args:</span></span><br><span class="line">            <span class="bullet">-</span> <span class="string">&quot;--type=liveness&quot;</span></span><br><span class="line">            <span class="bullet">-</span> <span class="string">&quot;--endpoint=$(CSI_ENDPOINT)&quot;</span></span><br><span class="line">            <span class="bullet">-</span> <span class="string">&quot;--metricsport=8680&quot;</span></span><br><span class="line">            <span class="bullet">-</span> <span class="string">&quot;--metricspath=/metrics&quot;</span></span><br><span class="line">            <span class="bullet">-</span> <span class="string">&quot;--polltime=60s&quot;</span></span><br><span class="line">            <span class="bullet">-</span> <span class="string">&quot;--timeout=3s&quot;</span></span><br><span class="line">          <span class="attr">env:</span></span><br><span class="line">            <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">CSI_ENDPOINT</span></span><br><span class="line">              <span class="attr">value:</span> <span class="string">unix:///csi/csi-provisioner.sock</span></span><br><span class="line">            <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">POD_IP</span></span><br><span class="line">              <span class="attr">valueFrom:</span></span><br><span class="line">                <span class="attr">fieldRef:</span></span><br><span class="line">                  <span class="attr">fieldPath:</span> <span class="string">status.podIP</span></span><br><span class="line">          <span class="attr">volumeMounts:</span></span><br><span class="line">            <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">socket-dir</span></span><br><span class="line">              <span class="attr">mountPath:</span> <span class="string">/csi</span></span><br><span class="line">          <span class="attr">imagePullPolicy:</span> <span class="string">&quot;IfNotPresent&quot;</span></span><br><span class="line">      <span class="attr">volumes:</span></span><br><span class="line">        <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">host-dev</span></span><br><span class="line">          <span class="attr">hostPath:</span></span><br><span class="line">            <span class="attr">path:</span> <span class="string">/dev</span></span><br><span class="line">        <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">host-sys</span></span><br><span class="line">          <span class="attr">hostPath:</span></span><br><span class="line">            <span class="attr">path:</span> <span class="string">/sys</span></span><br><span class="line">        <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">lib-modules</span></span><br><span class="line">          <span class="attr">hostPath:</span></span><br><span class="line">            <span class="attr">path:</span> <span class="string">/lib/modules</span></span><br><span class="line">        <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">socket-dir</span></span><br><span class="line">          <span class="attr">emptyDir:</span> &#123;</span><br><span class="line">            <span class="attr">medium:</span> <span class="string">&quot;Memory&quot;</span></span><br><span class="line">          &#125;</span><br><span class="line">        <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">ceph-csi-config</span></span><br><span class="line">          <span class="attr">configMap:</span></span><br><span class="line">            <span class="attr">name:</span> <span class="string">ceph-csi-config</span></span><br><span class="line">        <span class="comment">#- name: ceph-csi-encryption-kms-config</span></span><br><span class="line">        <span class="comment">#  configMap:</span></span><br><span class="line">        <span class="comment">#    name: ceph-csi-encryption-kms-config</span></span><br><span class="line">        <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">keys-tmp-dir</span></span><br><span class="line">          <span class="attr">emptyDir:</span> &#123;</span><br><span class="line">            <span class="attr">medium:</span> <span class="string">&quot;Memory&quot;</span></span><br><span class="line">          &#125;</span><br></pre></td></tr></table></figure><p>上面yaml文件中注释的部分是之前测试报错没有找到cm，官方文档没有创建此文件，这里注释掉无影响。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> kubectl  apply  -f  csi-rbdplugin.yaml</span></span><br></pre></td></tr></table></figure><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">$</span> <span class="string">cat</span> <span class="string">csi-rbdplugin.yaml</span></span><br><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">DaemonSet</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">apps/v1</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">csi-rbdplugin</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">selector:</span></span><br><span class="line">    <span class="attr">matchLabels:</span></span><br><span class="line">      <span class="attr">app:</span> <span class="string">csi-rbdplugin</span></span><br><span class="line">  <span class="attr">template:</span></span><br><span class="line">    <span class="attr">metadata:</span></span><br><span class="line">      <span class="attr">labels:</span></span><br><span class="line">        <span class="attr">app:</span> <span class="string">csi-rbdplugin</span></span><br><span class="line">    <span class="attr">spec:</span></span><br><span class="line">      <span class="attr">serviceAccount:</span> <span class="string">rbd-csi-nodeplugin</span></span><br><span class="line">      <span class="attr">hostNetwork:</span> <span class="literal">true</span></span><br><span class="line">      <span class="attr">hostPID:</span> <span class="literal">true</span></span><br><span class="line">      <span class="comment"># to use e.g. Rook orchestrated cluster, and mons&#x27; FQDN is</span></span><br><span class="line">      <span class="comment"># resolved through k8s service, set dns policy to cluster first</span></span><br><span class="line">      <span class="attr">dnsPolicy:</span> <span class="string">ClusterFirstWithHostNet</span></span><br><span class="line">      <span class="attr">containers:</span></span><br><span class="line">        <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">driver-registrar</span></span><br><span class="line">          <span class="comment"># This is necessary only for systems with SELinux, where</span></span><br><span class="line">          <span class="comment"># non-privileged sidecar containers cannot access unix domain socket</span></span><br><span class="line">          <span class="comment"># created by privileged CSI driver container.</span></span><br><span class="line">          <span class="attr">securityContext:</span></span><br><span class="line">            <span class="attr">privileged:</span> <span class="literal">true</span></span><br><span class="line">          <span class="attr">image:</span> <span class="string">quay.io/k8scsi/csi-node-driver-registrar:v1.3.0</span></span><br><span class="line">          <span class="attr">args:</span></span><br><span class="line">            <span class="bullet">-</span> <span class="string">&quot;--v=5&quot;</span></span><br><span class="line">            <span class="bullet">-</span> <span class="string">&quot;--csi-address=/csi/csi.sock&quot;</span></span><br><span class="line">            <span class="bullet">-</span> <span class="string">&quot;--kubelet-registration-path=/data/kubelet/plugins/rbd.csi.ceph.com/csi.sock&quot;</span></span><br><span class="line">          <span class="attr">lifecycle:</span></span><br><span class="line">            <span class="attr">preStop:</span></span><br><span class="line">              <span class="attr">exec:</span></span><br><span class="line">                <span class="attr">command:</span> [</span><br><span class="line">                  <span class="string">&quot;/bin/sh&quot;</span>, <span class="string">&quot;-c&quot;</span>,</span><br><span class="line">                  <span class="string">&quot;rm -rf /registration/rbd.csi.ceph.com \</span></span><br><span class="line"><span class="string">                  /registration/rbd.csi.ceph.com-reg.sock&quot;</span></span><br><span class="line">                ]</span><br><span class="line">          <span class="attr">env:</span></span><br><span class="line">            <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">KUBE_NODE_NAME</span></span><br><span class="line">              <span class="attr">valueFrom:</span></span><br><span class="line">                <span class="attr">fieldRef:</span></span><br><span class="line">                  <span class="attr">fieldPath:</span> <span class="string">spec.nodeName</span></span><br><span class="line">          <span class="attr">volumeMounts:</span></span><br><span class="line">            <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">socket-dir</span></span><br><span class="line">              <span class="attr">mountPath:</span> <span class="string">/csi</span></span><br><span class="line">            <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">registration-dir</span></span><br><span class="line">              <span class="attr">mountPath:</span> <span class="string">/registration</span></span><br><span class="line">        <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">csi-rbdplugin</span></span><br><span class="line">          <span class="attr">securityContext:</span></span><br><span class="line">            <span class="attr">privileged:</span> <span class="literal">true</span></span><br><span class="line">            <span class="attr">capabilities:</span></span><br><span class="line">              <span class="attr">add:</span> [<span class="string">&quot;SYS_ADMIN&quot;</span>]</span><br><span class="line">            <span class="attr">allowPrivilegeEscalation:</span> <span class="literal">true</span></span><br><span class="line">          <span class="comment"># for stable functionality replace canary with latest release version</span></span><br><span class="line">          <span class="attr">image:</span> <span class="string">quay.io/cephcsi/cephcsi:canary</span></span><br><span class="line">          <span class="attr">args:</span></span><br><span class="line">            <span class="bullet">-</span> <span class="string">&quot;--nodeid=$(NODE_ID)&quot;</span></span><br><span class="line">            <span class="bullet">-</span> <span class="string">&quot;--type=rbd&quot;</span></span><br><span class="line">            <span class="bullet">-</span> <span class="string">&quot;--nodeserver=true&quot;</span></span><br><span class="line">            <span class="bullet">-</span> <span class="string">&quot;--endpoint=$(CSI_ENDPOINT)&quot;</span></span><br><span class="line">            <span class="bullet">-</span> <span class="string">&quot;--v=5&quot;</span></span><br><span class="line">            <span class="bullet">-</span> <span class="string">&quot;--drivername=rbd.csi.ceph.com&quot;</span></span><br><span class="line">            <span class="comment"># If topology based provisioning is desired, configure required</span></span><br><span class="line">            <span class="comment"># node labels representing the nodes topology domain</span></span><br><span class="line">            <span class="comment"># and pass the label names below, for CSI to consume and advertize</span></span><br><span class="line">            <span class="comment"># its equivalent topology domain</span></span><br><span class="line">            <span class="comment"># - &quot;--domainlabels=failure-domain/region,failure-domain/zone&quot;</span></span><br><span class="line">          <span class="attr">env:</span></span><br><span class="line">            <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">POD_IP</span></span><br><span class="line">              <span class="attr">valueFrom:</span></span><br><span class="line">                <span class="attr">fieldRef:</span></span><br><span class="line">                  <span class="attr">fieldPath:</span> <span class="string">status.podIP</span></span><br><span class="line">            <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">NODE_ID</span></span><br><span class="line">              <span class="attr">valueFrom:</span></span><br><span class="line">                <span class="attr">fieldRef:</span></span><br><span class="line">                  <span class="attr">fieldPath:</span> <span class="string">spec.nodeName</span></span><br><span class="line">            <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">CSI_ENDPOINT</span></span><br><span class="line">              <span class="attr">value:</span> <span class="string">unix:///csi/csi.sock</span></span><br><span class="line">          <span class="attr">imagePullPolicy:</span> <span class="string">&quot;IfNotPresent&quot;</span></span><br><span class="line">          <span class="attr">volumeMounts:</span></span><br><span class="line">            <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">socket-dir</span></span><br><span class="line">              <span class="attr">mountPath:</span> <span class="string">/csi</span></span><br><span class="line">            <span class="bullet">-</span> <span class="attr">mountPath:</span> <span class="string">/dev</span></span><br><span class="line">              <span class="attr">name:</span> <span class="string">host-dev</span></span><br><span class="line">            <span class="bullet">-</span> <span class="attr">mountPath:</span> <span class="string">/sys</span></span><br><span class="line">              <span class="attr">name:</span> <span class="string">host-sys</span></span><br><span class="line">            <span class="bullet">-</span> <span class="attr">mountPath:</span> <span class="string">/run/mount</span></span><br><span class="line">              <span class="attr">name:</span> <span class="string">host-mount</span></span><br><span class="line">            <span class="bullet">-</span> <span class="attr">mountPath:</span> <span class="string">/lib/modules</span></span><br><span class="line">              <span class="attr">name:</span> <span class="string">lib-modules</span></span><br><span class="line">              <span class="attr">readOnly:</span> <span class="literal">true</span></span><br><span class="line">            <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">ceph-csi-config</span></span><br><span class="line">              <span class="attr">mountPath:</span> <span class="string">/etc/ceph-csi-config/</span></span><br><span class="line">           <span class="comment"># - name: ceph-csi-encryption-kms-config</span></span><br><span class="line">           <span class="comment">#   mountPath: /etc/ceph-csi-encryption-kms-config/</span></span><br><span class="line">            <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">plugin-dir</span></span><br><span class="line">              <span class="attr">mountPath:</span> <span class="string">/data/kubelet/plugins</span></span><br><span class="line">              <span class="attr">mountPropagation:</span> <span class="string">&quot;Bidirectional&quot;</span></span><br><span class="line">            <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">mountpoint-dir</span></span><br><span class="line">              <span class="attr">mountPath:</span> <span class="string">/data/kubelet/pods</span></span><br><span class="line">              <span class="attr">mountPropagation:</span> <span class="string">&quot;Bidirectional&quot;</span></span><br><span class="line">            <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">keys-tmp-dir</span></span><br><span class="line">              <span class="attr">mountPath:</span> <span class="string">/tmp/csi/keys</span></span><br><span class="line">        <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">liveness-prometheus</span></span><br><span class="line">          <span class="attr">securityContext:</span></span><br><span class="line">            <span class="attr">privileged:</span> <span class="literal">true</span></span><br><span class="line">          <span class="attr">image:</span> <span class="string">quay.io/cephcsi/cephcsi:canary</span></span><br><span class="line">          <span class="attr">args:</span></span><br><span class="line">            <span class="bullet">-</span> <span class="string">&quot;--type=liveness&quot;</span></span><br><span class="line">            <span class="bullet">-</span> <span class="string">&quot;--endpoint=$(CSI_ENDPOINT)&quot;</span></span><br><span class="line">            <span class="bullet">-</span> <span class="string">&quot;--metricsport=8680&quot;</span></span><br><span class="line">            <span class="bullet">-</span> <span class="string">&quot;--metricspath=/metrics&quot;</span></span><br><span class="line">            <span class="bullet">-</span> <span class="string">&quot;--polltime=60s&quot;</span></span><br><span class="line">            <span class="bullet">-</span> <span class="string">&quot;--timeout=3s&quot;</span></span><br><span class="line">          <span class="attr">env:</span></span><br><span class="line">            <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">CSI_ENDPOINT</span></span><br><span class="line">              <span class="attr">value:</span> <span class="string">unix:///csi/csi.sock</span></span><br><span class="line">            <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">POD_IP</span></span><br><span class="line">              <span class="attr">valueFrom:</span></span><br><span class="line">                <span class="attr">fieldRef:</span></span><br><span class="line">                  <span class="attr">fieldPath:</span> <span class="string">status.podIP</span></span><br><span class="line">          <span class="attr">volumeMounts:</span></span><br><span class="line">            <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">socket-dir</span></span><br><span class="line">              <span class="attr">mountPath:</span> <span class="string">/csi</span></span><br><span class="line">          <span class="attr">imagePullPolicy:</span> <span class="string">&quot;IfNotPresent&quot;</span></span><br><span class="line">      <span class="attr">volumes:</span></span><br><span class="line">        <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">socket-dir</span></span><br><span class="line">          <span class="attr">hostPath:</span></span><br><span class="line">            <span class="attr">path:</span> <span class="string">/data/kubelet/plugins/rbd.csi.ceph.com</span></span><br><span class="line">            <span class="attr">type:</span> <span class="string">DirectoryOrCreate</span></span><br><span class="line">        <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">plugin-dir</span></span><br><span class="line">          <span class="attr">hostPath:</span></span><br><span class="line">            <span class="attr">path:</span> <span class="string">/data/kubelet/plugins</span></span><br><span class="line">            <span class="attr">type:</span> <span class="string">Directory</span></span><br><span class="line">        <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">mountpoint-dir</span></span><br><span class="line">          <span class="attr">hostPath:</span></span><br><span class="line">            <span class="attr">path:</span> <span class="string">/data/kubelet/pods</span></span><br><span class="line">            <span class="attr">type:</span> <span class="string">DirectoryOrCreate</span></span><br><span class="line">        <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">registration-dir</span></span><br><span class="line">          <span class="attr">hostPath:</span></span><br><span class="line">            <span class="attr">path:</span> <span class="string">/data/kubelet/plugins_registry/</span></span><br><span class="line">            <span class="attr">type:</span> <span class="string">Directory</span></span><br><span class="line">        <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">host-dev</span></span><br><span class="line">          <span class="attr">hostPath:</span></span><br><span class="line">            <span class="attr">path:</span> <span class="string">/dev</span></span><br><span class="line">        <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">host-sys</span></span><br><span class="line">          <span class="attr">hostPath:</span></span><br><span class="line">            <span class="attr">path:</span> <span class="string">/sys</span></span><br><span class="line">        <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">host-mount</span></span><br><span class="line">          <span class="attr">hostPath:</span></span><br><span class="line">            <span class="attr">path:</span> <span class="string">/run/mount</span></span><br><span class="line">        <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">lib-modules</span></span><br><span class="line">          <span class="attr">hostPath:</span></span><br><span class="line">            <span class="attr">path:</span> <span class="string">/lib/modules</span></span><br><span class="line">        <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">ceph-csi-config</span></span><br><span class="line">          <span class="attr">configMap:</span></span><br><span class="line">            <span class="attr">name:</span> <span class="string">ceph-csi-config</span></span><br><span class="line">        <span class="comment">#- name: ceph-csi-encryption-kms-config</span></span><br><span class="line">        <span class="comment">#  configMap:</span></span><br><span class="line">        <span class="comment">#    name: ceph-csi-encryption-kms-config</span></span><br><span class="line">        <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">keys-tmp-dir</span></span><br><span class="line">          <span class="attr">emptyDir:</span> &#123;</span><br><span class="line">            <span class="attr">medium:</span> <span class="string">&quot;Memory&quot;</span></span><br><span class="line">          &#125;</span><br><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="comment"># This is a service to expose the liveness metrics</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Service</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">csi-metrics-rbdplugin</span></span><br><span class="line">  <span class="attr">labels:</span></span><br><span class="line">    <span class="attr">app:</span> <span class="string">csi-metrics</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">ports:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">http-metrics</span></span><br><span class="line">      <span class="attr">port:</span> <span class="number">8080</span></span><br><span class="line">      <span class="attr">protocol:</span> <span class="string">TCP</span></span><br><span class="line">      <span class="attr">targetPort:</span> <span class="number">8680</span></span><br><span class="line">  <span class="attr">selector:</span></span><br><span class="line">    <span class="attr">app:</span> <span class="string">csi-rbdplugin</span></span><br></pre></td></tr></table></figure><p>查看容器是否正常运行。</p><p><img src="/doc_picture/ceph-6.png" alt="image-20210802123641214"></p><h2 id="6-使用ceph块设备"><a href="#6-使用ceph块设备" class="headerlink" title="6.    使用ceph块设备"></a>6.    使用ceph块设备</h2><h3 id="创建storageclass"><a href="#创建storageclass" class="headerlink" title="创建storageclass"></a>创建storageclass</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> kubectl  apply  -f  csi-rbd-sc-filesystem.yaml</span></span><br></pre></td></tr></table></figure><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">$</span> <span class="string">cat</span> <span class="string">csi-rbd-sc-filesystem.yaml</span></span><br><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">storage.k8s.io/v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">StorageClass</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">   <span class="attr">name:</span> <span class="string">csi-rbd-sc-filesystem</span></span><br><span class="line"><span class="attr">provisioner:</span> <span class="string">rbd.csi.ceph.com</span></span><br><span class="line"><span class="attr">parameters:</span></span><br><span class="line">   <span class="attr">clusterID:</span> <span class="string">10594fb3-68f3-4c97-8e0b-df80ba2a6745</span></span><br><span class="line">   <span class="attr">imageFeatures:</span> <span class="string">layering</span></span><br><span class="line">   <span class="attr">pool:</span> <span class="string">kubernetes</span></span><br><span class="line">   <span class="attr">csi.storage.k8s.io/provisioner-secret-name:</span> <span class="string">csi-rbd-secret</span></span><br><span class="line">   <span class="attr">csi.storage.k8s.io/provisioner-secret-namespace:</span> <span class="string">default</span></span><br><span class="line">   <span class="attr">csi.storage.k8s.io/node-stage-secret-name:</span> <span class="string">csi-rbd-secret</span></span><br><span class="line">   <span class="attr">csi.storage.k8s.io/node-stage-secret-namespace:</span> <span class="string">default</span></span><br><span class="line">   <span class="attr">csi.storage.k8s.io/controller-expand-secret-name:</span> <span class="string">csi-rbd-secret</span></span><br><span class="line">   <span class="attr">csi.storage.k8s.io/controller-expand-secret-namespace:</span> <span class="string">default</span></span><br><span class="line">   <span class="attr">csi.storage.k8s.io/fstype:</span> <span class="string">ext4</span></span><br><span class="line"><span class="attr">reclaimPolicy:</span> <span class="string">Delete</span></span><br><span class="line"><span class="attr">allowVolumeExpansion:</span> <span class="literal">true</span></span><br><span class="line"><span class="attr">mountOptions:</span></span><br><span class="line">   <span class="bullet">-</span> <span class="string">discard</span></span><br></pre></td></tr></table></figure><p>其中：</p><p> <code>csi.storage.k8s.io/controller-expand-secret-name: csi-rbd-secret</code></p><p><code>csi.storage.k8s.io/controller-expand-secret-namespace: default</code></p><p><code>allowVolumeExpansion: true</code></p><p>以上三个参数都是在ceph-csi支持动态扩容时需要具备的参数</p><h3 id="创建pvc"><a href="#创建pvc" class="headerlink" title="创建pvc"></a>创建pvc</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> kubectl  apply  -f  filesystem-pvc2.yaml</span></span><br></pre></td></tr></table></figure><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">$</span> <span class="string">cat</span> <span class="string">filesystem-pvc2.yaml</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">PersistentVolumeClaim</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">filesystem-pvc-2</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">accessModes:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">ReadWriteOnce</span></span><br><span class="line">  <span class="attr">volumeMode:</span> <span class="string">Filesystem</span></span><br><span class="line">  <span class="attr">resources:</span></span><br><span class="line">    <span class="attr">requests:</span></span><br><span class="line">      <span class="attr">storage:</span> <span class="string">200Mi</span></span><br><span class="line">  <span class="attr">storageClassName:</span> <span class="string">csi-rbd-sc-filesystem</span></span><br></pre></td></tr></table></figure><blockquote><p>这里除了filesystem还可以设置成block，区别就是文件系统是直接挂在文件夹下，block相当于是裸设备，有些场景服务为了性能会直接操作裸磁盘就可以用到了。</p></blockquote><p>这个时候查看卷就创建出来了。</p><p><img src="/doc_picture/ceph-7.png" alt="image-20210802124356580"></p><h2 id="存储盘扩容测试"><a href="#存储盘扩容测试" class="headerlink" title="存储盘扩容测试"></a>存储盘扩容测试</h2><p>以上面创建出来的200Mi的filesystem-pvc-2卷举例，查看pvc与pv都是200M。</p><p><img src="/doc_picture/ceph-8.png" alt="image-20210802124511152"></p><p><img src="/doc_picture/ceph-9.png" alt="image-20210802124518604"></p><p>在线修改pvc中的大小，200M修改为700M，保存退出。</p><p><img src="/doc_picture/ceph-10.png" alt="image-20210802124539312"></p><p>查看此时的pvc与pv状态</p><p><img src="/doc_picture/ceph-11.png" alt="image-20210802124656194"></p><p><img src="/doc_picture/ceph-12.png" alt="image-20210802124713132"></p><p>会发现pv已经变为了700M,pvc没有改变，查看pvc的详细信息。</p><p><img src="/doc_picture/ceph-13.png" alt="image-20210802124732583"></p><p>状态栏中已经说的很清楚了，重启文件系统就可以生效了（客户端）。</p><h2 id="扩容缺陷"><a href="#扩容缺陷" class="headerlink" title="扩容缺陷"></a>扩容缺陷</h2><p>原先的卷空间如果扩容到1G及以下会按照实际申请大小来创建，如果申请扩容大小超出1G会自动以GB为单位补全，如下例子：</p><p>将之前创建的filesystem-pvc-2扩容到1.2G</p><p><img src="/doc_picture/ceph-14.png" alt="image-20210802133810652"></p><p>保存退出，查看pv的大小，补为了2G</p><p><img src="/doc_picture/ceph-15.png" alt="image-20210802133825273"></p><p>查看ceph端的rbd大小也是2G</p><p><img src="/doc_picture/ceph-16.png" alt="image-20210802133841028"></p><h1 id="参考文档"><a href="#参考文档" class="headerlink" title="参考文档"></a>参考文档</h1><p>Kubernetes存储介绍系列 ——CSI plugin设计：<a href="http://newto.me/k8s-csi-design/">http://newto.me/k8s-csi-design/</a></p><p>Kubernetes 兼容 CSI 做的工作： <a href="https://www.kubernetes.org.cn/4618.html">https://www.kubernetes.org.cn/4618.html</a></p><p>kubernetes部署csi:     <a href="#using-ceph-block-devices">https://docs.ceph.com/docs/master/rbd/rbd-kubernetes/#using-ceph-block-devices</a></p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;最近测试ceph rbd在kubernetes的自动扩容问题，之前K8s v1.11.0时的策略是先找到目标卷，使用rbd resize命令对此卷扩容，找到挂载此卷的客户端宿主机，执行xfs_growfs等刷新文件系统的命令。查看网上资料k8s 在1.15版本后，Expan</summary>
      
    
    
    
    
    <category term="kubernetes storage" scheme="https://slions.github.io/tags/kubernetes-storage/"/>
    
    <category term="ceph" scheme="https://slions.github.io/tags/ceph/"/>
    
  </entry>
  
  <entry>
    <title>创建开机自启程序（下篇）</title>
    <link href="https://slions.github.io/2021/08/01/%E5%88%9B%E5%BB%BA%E5%BC%80%E6%9C%BA%E8%87%AA%E5%90%AF%E7%A8%8B%E5%BA%8F%EF%BC%88%E4%B8%8B%E7%AF%87%EF%BC%89/"/>
    <id>https://slions.github.io/2021/08/01/%E5%88%9B%E5%BB%BA%E5%BC%80%E6%9C%BA%E8%87%AA%E5%90%AF%E7%A8%8B%E5%BA%8F%EF%BC%88%E4%B8%8B%E7%AF%87%EF%BC%89/</id>
    <published>2021-08-01T07:27:11.000Z</published>
    <updated>2021-08-01T11:02:06.354Z</updated>
    
    <content type="html"><![CDATA[<a href="/2021/08/01/%E5%88%9B%E5%BB%BA%E5%BC%80%E6%9C%BA%E8%87%AA%E5%90%AF%E7%A8%8B%E5%BA%8F%EF%BC%88%E4%B8%8A%E7%AF%87%EF%BC%89/" title="上篇">上篇</a>讲述了通过rc.local可以实现程序的开机自启，这篇说说centOS7以后的推荐方式`systemd service`。<h1 id="systemd-service"><a href="#systemd-service" class="headerlink" title="systemd service"></a>systemd service</h1><p>systemd Service是systemd提供的用于管理服务启动、停止和相关操作的功能，它极大的简化了服务管理的配置过程，用户只需要配置几项指令即可。</p><p>systemd service是systemd所管理的其中一项内容。实际上，systemd service是Systemd Unit的一种，除了Service，systemd还有其他几种类型的unit，比如service、socket、slice、scope、target等等。在这里，暂时了解两项内容：</p><ul><li><p>Service类型，定义服务程序的启动、停止、重启等操作和进程相关属性</p></li><li><p>Target类型，主要目的是对Service(也可以是其它Unit)进行分组、归类，可以包含一个或多个Service Unit(也可以是其它Unit)</p></li></ul><blockquote><p>systemd管理服务的一些亮点：</p><ol><li>用户可以直接在Service配置文件中定义CGroup相关指令来对该服务程序做资源限制。</li><li>用户可以选择Journal日志而非采用rsyslog，这意味着用户可以不用单独去配置rsyslog，而且可以直接通过systemctl或journalctl命令来查看某服务的日志信息。当然，该功能并不适用于所有情况，比如用户需要管理日志时</li><li>Systemd Service还有其它一些特性，比如可以动态修改服务管理配置文件，比如可以并行启动非依赖的服务，从而加速开机过程等等。</li></ol></blockquote><h1 id="systemd服务配置文件存放路径"><a href="#systemd服务配置文件存放路径" class="headerlink" title="systemd服务配置文件存放路径"></a>systemd服务配置文件存放路径</h1><p>systemd 默认从目录<code>/etc/systemd/system/</code>读取配置文件。里面存放的大部分文件都是符号链接，真正的配置文件存放在<code>/usr/lib/systemd/system/</code>，如果用户需要，可以将服务配置文件手动存放至用户配置目录<code>/etc/systemd/system</code>下。该目录下的服务配置文件可以是普通.service文件，也可以是链接至<code>/usr/lib/systemd/system</code>目录下服务配置文件的软链接。</p><p>位于<code>/usr/lib/systemd/system</code>下的服务配置文件:</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">[root@slions_pc1 ~]# ls -l /usr/lib/systemd/system/*.service|head -10</span><br><span class="line">-rw-r--r--. 1 root root  275 11月 14 2018 /usr/lib/systemd/system/abrt-ccpp.service</span><br><span class="line">-rw-r--r--. 1 root root  380 11月 14 2018 /usr/lib/systemd/system/abrtd.service</span><br><span class="line">-rw-r--r--. 1 root root  361 11月 14 2018 /usr/lib/systemd/system/abrt-oops.service</span><br><span class="line">-rw-r--r--. 1 root root  266 11月 14 2018 /usr/lib/systemd/system/abrt-pstoreoops.service</span><br><span class="line">-rw-r--r--. 1 root root  262 11月 14 2018 /usr/lib/systemd/system/abrt-vmcore.service</span><br><span class="line">-rw-r--r--. 1 root root  311 11月 14 2018 /usr/lib/systemd/system/abrt-xorg.service</span><br><span class="line">-rw-r--r--. 1 root root  275 10月 31 2018 /usr/lib/systemd/system/arp-ethers.service</span><br><span class="line">-rw-r--r--. 1 root root  222 10月 31 2018 /usr/lib/systemd/system/atd.service</span><br><span class="line">-rw-r--r--. 1 root root 1384 8月   8 2019 /usr/lib/systemd/system/auditd.service</span><br><span class="line">lrwxrwxrwx. 1 root root   14 5月  21 16:45 /usr/lib/systemd/system/autovt@.service -&gt; getty@.service</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>下面这些目录(*.target.wants)定义各种类型下需要运行的服务：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">[root@slions_pc1 ~]# ls -1dF /etc/systemd/system/*</span><br><span class="line">/etc/systemd/system/basic.target.wants/</span><br><span class="line">/etc/systemd/system/dbus-org.freedesktop.NetworkManager.service@</span><br><span class="line">/etc/systemd/system/dbus-org.freedesktop.nm-dispatcher.service@</span><br><span class="line">/etc/systemd/system/default.target@</span><br><span class="line">/etc/systemd/system/default.target.wants/</span><br><span class="line">/etc/systemd/system/getty.target.wants/</span><br><span class="line">/etc/systemd/system/local-fs.target.wants/</span><br><span class="line">/etc/systemd/system/multi-user.target.wants/</span><br><span class="line">/etc/systemd/system/network-online.target.wants/</span><br><span class="line">/etc/systemd/system/sockets.target.wants/</span><br><span class="line">/etc/systemd/system/sysinit.target.wants/</span><br><span class="line">/etc/systemd/system/system-update.target.wants/</span><br><span class="line">/etc/systemd/system/vmtoolsd.service.requires/</span><br></pre></td></tr></table></figure><p>/etc/systemd/system/multi-user.target.wants下的服务配置文件，几乎都是软链接</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">[root@slions_pc1 ~]# ls -l /etc/systemd/system/multi-user.target.wants/ | awk &#x27;&#123;print $9,$10,$11&#125;&#x27;</span><br><span class="line">abrt-ccpp.service -&gt; /usr/lib/systemd/system/abrt-ccpp.service</span><br><span class="line">abrtd.service -&gt; /usr/lib/systemd/system/abrtd.service</span><br><span class="line">abrt-oops.service -&gt; /usr/lib/systemd/system/abrt-oops.service</span><br><span class="line">abrt-vmcore.service -&gt; /usr/lib/systemd/system/abrt-vmcore.service</span><br><span class="line">abrt-xorg.service -&gt; /usr/lib/systemd/system/abrt-xorg.service</span><br><span class="line">atd.service -&gt; /usr/lib/systemd/system/atd.service</span><br><span class="line">auditd.service -&gt; /usr/lib/systemd/system/auditd.service</span><br><span class="line">chronyd.service -&gt; /usr/lib/systemd/system/chronyd.service</span><br><span class="line">crond.service -&gt; /usr/lib/systemd/system/crond.service</span><br><span class="line">irqbalance.service -&gt; /usr/lib/systemd/system/irqbalance.service</span><br><span class="line">kdump.service -&gt; /usr/lib/systemd/system/kdump.service</span><br><span class="line">libstoragemgmt.service -&gt; /usr/lib/systemd/system/libstoragemgmt.service</span><br><span class="line">mdmonitor.service -&gt; /usr/lib/systemd/system/mdmonitor.service</span><br></pre></td></tr></table></figure><h1 id="systemd-service文件格式"><a href="#systemd-service文件格式" class="headerlink" title="systemd service文件格式"></a>systemd service文件格式</h1><p>基本的配置文件格式如下：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">[Unit]</span><br><span class="line">Description = some descriptions</span><br><span class="line">Documentation = man:xxx(8) man:xxx_config(5)</span><br><span class="line">Requires = xxx1.target xxx2.target</span><br><span class="line">After = yyy1.target yyy2.target</span><br><span class="line"></span><br><span class="line">[Service]</span><br><span class="line">Type = &lt;TYPE&gt;</span><br><span class="line">ExecStart = &lt;CMD_for_START&gt;</span><br><span class="line">ExecStop = &lt;CMD_for_STOP&gt;</span><br><span class="line">ExecReload = &lt;CMD_for_RELOAD&gt;</span><br><span class="line">Restart = &lt;WHEN_TO_RESTART&gt;</span><br><span class="line">RestartSec = &lt;TIME&gt;</span><br><span class="line"></span><br><span class="line">[Install]</span><br><span class="line">WantedBy = xxx.target yy.target</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>一个.Service配置文件分为三部分：</p><ul><li>Unit：定义该服务作为Unit角色时相关的属性</li><li>Service：定义本服务相关的属性</li><li>Install：定义本服务在设置服务开机自启动时相关的属性。换句话说，只有在创建/移除服务配置文件的软链接时，Install段才会派上用场。这一配置段不是必须的，<strong>当未配置[Install]时，设置开机自启动或禁止开机自启动的操作将无任何效果</strong></li></ul><h1 id="参考文档"><a href="#参考文档" class="headerlink" title="参考文档"></a>参考文档</h1><p><a href="http://www.jinbuguo.com/systemd/systemd.service.html">http://www.jinbuguo.com/systemd/systemd.service.html</a></p><p><a href="https://www.junmajinlong.com/linux/systemd/service_1/">https://www.junmajinlong.com/linux/systemd/service_1/</a></p>]]></content>
    
    
      
      
    <summary type="html">&lt;a href=&quot;/2021/08/01/%E5%88%9B%E5%BB%BA%E5%BC%80%E6%9C%BA%E8%87%AA%E5%90%AF%E7%A8%8B%E5%BA%8F%EF%BC%88%E4%B8%8A%E7%AF%87%EF%BC%89/&quot; title=&quot;上</summary>
      
    
    
    
    
    <category term="linux" scheme="https://slions.github.io/tags/linux/"/>
    
  </entry>
  
  <entry>
    <title>创建开机自启程序（上篇）</title>
    <link href="https://slions.github.io/2021/08/01/%E5%88%9B%E5%BB%BA%E5%BC%80%E6%9C%BA%E8%87%AA%E5%90%AF%E7%A8%8B%E5%BA%8F%EF%BC%88%E4%B8%8A%E7%AF%87%EF%BC%89/"/>
    <id>https://slions.github.io/2021/08/01/%E5%88%9B%E5%BB%BA%E5%BC%80%E6%9C%BA%E8%87%AA%E5%90%AF%E7%A8%8B%E5%BA%8F%EF%BC%88%E4%B8%8A%E7%AF%87%EF%BC%89/</id>
    <published>2021-08-01T04:58:46.000Z</published>
    <updated>2021-08-01T10:34:17.335Z</updated>
    
    <content type="html"><![CDATA[<p>在我们实现如何创建开机自启程序前，先简单了解下linux的启动过程。</p><h1 id="linux启动过程"><a href="#linux启动过程" class="headerlink" title="linux启动过程"></a>linux启动过程</h1><p>Linux 系统的启动，从计算机开机通电自检开始，一直到登陆系统，需要经历多个过程。</p><h2 id="CentOS-RHEL6"><a href="#CentOS-RHEL6" class="headerlink" title="CentOS/RHEL6"></a>CentOS/RHEL6</h2><ol><li>服务器加电，加载 BIOS 信息，BIOS 进行系统检测。依照 BIOS 设定，找到第一个可以启动的设备（一般是硬盘）；</li><li>读取第一个启动设备的 MBR (主引导记录），加载 MBR 中的 Boot Loader（启动引导程序，最为常见的是 GRUB）。</li><li>依据 Boot Loader 的设置加载内核，内核会再进行一遍系统检测。系统一般会采用内核检测硬件的信息，而不一定采用 Bios 的自检信息。内核在检测硬件的同时，还会通过加载动态模块的形式加载硬件的驱动。</li><li>内核启动系统的第一个进程，也就是 /sbin/init。</li><li>由 /sbin/init 进程调用 /etc/init/rcS.conf 配置文件，通过这个配置文件调用 /etc/rc.d/rc.sysinit 配置文件。而 /etc/rc.d/rc.sysinit 配置文件是用来进行系统初始化的，主要用于配置计算机的初始环境。</li><li>还是通过 /etc/init/rcS.conf 配置文件调用 /etc/inittab 配置文件。通过 /etc/inittab 配置文件来确定系统的默认运行级别。</li><li>确定默认运行级别后，调用 /etc/init/rc.conf 配置文件。</li><li>通过 /etc/init/rc.conf 配置文件调用并执行 /etc/rc.d/rc 脚本，并传入运行级别参数。</li><li>/etc/rc.d/rc 确定传入的运行级别，然后运行相应的运行级别目录 /etc/rc[0-6].d/ 中的脚本。</li><li>/etc/rc[0-6].d/ 目录中的脚本依据设定好的优先级依次启动和关闭。</li><li>最后执行 /etc/rc.d/rc.local 中的程序。</li><li>如果是字符界面启动，就可以看到登录界面了。如果是图形界面启动，就会调用相应的 X Window 接口。</li></ol><p><img src="/doc_picture/2-1Q02310563a22.jpg" alt="img"></p><h2 id="CentOS-RHEL7"><a href="#CentOS-RHEL7" class="headerlink" title="CentOS/RHEL7"></a>CentOS/RHEL7</h2><ol><li><p>服务器加电，加载 BIOS 信息，BIOS 进行系统检测。依照 BIOS 设定，找到第一个可以启动的设备（一般是硬盘）；</p></li><li><p>读取第一个启动设备的 MBR (主引导记录），加载 MBR 中的 OSLoader（启动引导程序GRUB2）。</p></li><li><p>OSLoader 加载其相关配置 , 并显示相关的配置 菜单来引导用户选择相关操作(/etc/grub.d,/etc/default/grub,/boot/grub2/grub.cfg)</p></li><li><p>在用户选择后 ( 或 timeout 后 ),GRUB2 将加载 内核及 initramfs 至内存中 .initramfs 属于一个 img 的虚拟磁盘。</p><p>initramfs 包含了动态的内核模块 , 初始化脚本及非常多的硬件驱动等 , 在 RHEL7 中 initramfs 自身即包含了一个完整的可用系统。（/etc/dracut.conf）</p></li><li><p>GRUB2 将控制系统切换到 kernel, 通过对 GRUB2 的控制可以添加各种 kernel 的选项 . 并将 这些选项同时传递至内存中 , 影响 kernel 及 initramfs 的运行 . (/etc/grub.d,/etc/default/grub,/boot/grub2/gr ub.cfg)</p></li><li><p>kernel 在启动后将初始化所有的硬件 , 通过 initramfs 找到硬件相关的驱动程序 , 而后从 initramfs 中执行 PID 1 的 /sbin/init 命令 ( 最高 进程 ). 在 RHEL 中 init 属 于 /lib/systemd/systemd 的软连接 , 以及一个 udev 进程来自动建立已经存在的硬件的设备。（/etc/fstab）</p></li><li><p>由 initramfs 建立的内存的根分区 /sysroot( 物理 ‘/‘ 分区 ) 在成功挂载之后，将切 换到此根分区上 , 并将 systemd 重新执行安装至真实的根分区中。</p></li><li><p> systemd 开始查找所有的服务 , 开始执行 ( 停止 ) 相关的服务 , 以符合该服务的配置并解决相关的依赖问题。（/etc/systemd/system/default.target）</p></li></ol><blockquote><p>Systemd引入了并行启动的概念，它会为每个需要启动的守护进程建立一个套接字，这些套接字对于使用它们的进程来说是抽象的，这样它们可以允许不同守护进程之间进行交互。Systemd会创建新进程并为每个进程分配一个控制组（cgroup）。处于不同控制组的进程之间可以通过内核来互相通信。</p><p>从 7.x 版本开始，引入了 systemd 来管理服务，/etc/rc.local 是为向前兼容而保留。</p></blockquote><h1 id="开机自启"><a href="#开机自启" class="headerlink" title="开机自启"></a>开机自启</h1><p>我们现在大多都在使用CentOS/RHEL7中，实现开机启动程序主要有两种方法：</p><ul><li><p>在/etc/rc.local脚本文件中编写启动程序的脚本</p></li><li><p>把要启动的程序配置成自定义的systemd服务（推荐）</p></li></ul><h2 id="通过rc-local实现开机自启程序"><a href="#通过rc-local实现开机自启程序" class="headerlink" title="通过rc.local实现开机自启程序"></a>通过rc.local实现开机自启程序</h2><p>/etc/rc.local是/etc/rc.d/rc.local的软链接</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@slions_pc1 home]# ll /etc/rc.local</span><br><span class="line">lrwxrwxrwx. 1 root root 13 5月  21 16:45 /etc/rc.local -&gt; rc.d/rc.local</span><br></pre></td></tr></table></figure><h3 id="rc-local配置文件解读"><a href="#rc-local配置文件解读" class="headerlink" title="rc.local配置文件解读"></a>rc.local配置文件解读</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">[root@slions_pc1 home]# cat /etc/rc.local</span><br><span class="line"><span class="meta">#</span><span class="bash">!/bin/bash</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> THIS FILE IS ADDED FOR COMPATIBILITY PURPOSES</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 添加此文件是为了兼容。</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> It is highly advisable to create own systemd services or udev rules</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> to run scripts during boot instead of using this file.</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 强烈建议创建自己的systemd服务或udev规则，以便在引导期间运行脚本，而不是使用此文件。</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> In contrast to previous versions due to parallel execution during boot</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> this script will NOT be run after all other services.</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 请注意，必须运行<span class="string">&#x27;chmod+x/etc/rc.d/rc.local&#x27;</span>，以确保在引导期间执行此脚本。</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> Please note that you must run <span class="string">&#x27;chmod +x /etc/rc.d/rc.local&#x27;</span> to ensure</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> that this script will be executed during boot.</span></span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash">默认会touch这个文件，每次系统启动时都会touch这个文件，这个文件的修改时间就是系统的启动时间</span></span><br><span class="line">touch /var/lock/subsys/local</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>rc.local本质上是一个shell脚本文件，可以把启动时需要执行的命令写在里面，启动时将按顺序执行。</p><h3 id="rc-local编写注意"><a href="#rc-local编写注意" class="headerlink" title="rc.local编写注意"></a>rc.local编写注意</h3><ul><li>rc.local脚本在操作系统启动时只执行一次</li><li>环境变量的问题<ul><li>在rc.local脚本中执行程序时是没有环境变量的，如果执行的程序需要环境变量，可以在脚本中设置环境变量</li></ul></li><li>命令需写绝对路径</li><li>不要让rc.local挂起<ul><li>rc.local是一个脚本，是按顺序执行的，执行完一个程序后才会执行下一个程序，如果某程序不是后台程序，就应该加&amp;让程序运行在后台，否则rc.local会挂起。</li></ul></li><li>切记chmod+x/etc/rc.d/rc.local赋予执行权限</li></ul><h2 id="通过system实现开机自启程序"><a href="#通过system实现开机自启程序" class="headerlink" title="通过system实现开机自启程序"></a>通过system实现开机自启程序</h2><h3 id="systemd可管理的服务"><a href="#systemd可管理的服务" class="headerlink" title="systemd可管理的服务"></a>systemd可管理的服务</h3><p>操作系统使用systemd后，所有用户进程都是systemd的后代进程。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line">[root@slions_pc1 ~]# pstree -p</span><br><span class="line">systemd(1)─┬─NetworkManager(8829)─┬─&#123;NetworkManager&#125;(8881)</span><br><span class="line">           │                      └─&#123;NetworkManager&#125;(8883)</span><br><span class="line">           ├─VGAuthService(8844)</span><br><span class="line">           ├─abrt-dbus(14768)─┬─&#123;abrt-dbus&#125;(14802)</span><br><span class="line">           │                  └─&#123;abrt-dbus&#125;(14813)</span><br><span class="line">           ├─abrt-watch-log(8832)</span><br><span class="line">           ├─abrtd(8830)</span><br><span class="line">           ├─atd(8850)</span><br><span class="line">           ├─auditd(8793)───&#123;auditd&#125;(8794)</span><br><span class="line">           ├─chronyd(8869)</span><br><span class="line">           ├─crond(8853)</span><br><span class="line">           ├─dbus-daemon(8821)───&#123;dbus-daemon&#125;(8827)</span><br><span class="line">           ├─irqbalance(8834)</span><br><span class="line">           ├─login(8866)───bash(14656)</span><br><span class="line">           ├─lsmd(8849)</span><br><span class="line">           ├─lvmetad(4420)</span><br><span class="line">           ├─master(9397)─┬─pickup(9415)</span><br><span class="line">           │              └─qmgr(9416)</span><br><span class="line">           ├─polkitd(8818)─┬─&#123;polkitd&#125;(8826)</span><br><span class="line">           │               ├─&#123;polkitd&#125;(8828)</span><br><span class="line">           │               ├─&#123;polkitd&#125;(8833)</span><br><span class="line">           │               ├─&#123;polkitd&#125;(8839)</span><br><span class="line">           │               ├─&#123;polkitd&#125;(8840)</span><br><span class="line">           │               └─&#123;polkitd&#125;(8841)</span><br><span class="line">           ├─rngd(8835)</span><br><span class="line">           ├─rpcbind(8824)</span><br><span class="line">           ├─rsyslogd(9174)─┬─&#123;rsyslogd&#125;(9237)</span><br><span class="line">           │                └─&#123;rsyslogd&#125;(9258)</span><br><span class="line">           ├─smartd(8820)</span><br><span class="line">           ├─sshd(9171)─┬─sshd(19034)───bash(19040)───pstree(19161)</span><br><span class="line">           │            └─sshd(19038)───sftp-server(19075)</span><br><span class="line">           ├─systemd-journal(4398)</span><br><span class="line">           ├─systemd-logind(8846)</span><br><span class="line">           ├─systemd-udevd(4427)</span><br><span class="line">           ├─tuned(9172)─┬─&#123;tuned&#125;(9926)</span><br><span class="line">           │             ├─&#123;tuned&#125;(9928)</span><br><span class="line">           │             ├─&#123;tuned&#125;(9984)</span><br><span class="line">           │             └─&#123;tuned&#125;(10117)</span><br><span class="line">           └─vmtoolsd(8845)───&#123;vmtoolsd&#125;(8935)</span><br></pre></td></tr></table></figure><p>虽然从进程树关系来看，所有进程都直接或间接地受到systemd的管理，但是，并非所有systemd的子进程都受Systemd Unit管理单元的管理。只有那些由systemd方式启动的服务进程(比如systemctl命令启动)才受到Systemd Unit管理单元的监控和管理。为了简化描述，后面均直接以『systemd管理』来描述受systemd unit管理单元的管理。</p><p>比如，用户可以通过下面两种方式启动Nginx服务进程：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">nginx                    # (1)</span><br><span class="line">systemctl start nginx    # (2)</span><br></pre></td></tr></table></figure><p>但systemd只能监控、管理第(2)种方式启动的nginx服务。比如第一种方式启动的nginx，无法使用systemctl stop nginx来停止。</p><h3 id="systemd管理服务的命令"><a href="#systemd管理服务的命令" class="headerlink" title="systemd管理服务的命令"></a>systemd管理服务的命令</h3><p><code>systemctl</code>是 Systemd 的主命令，用于管理系统。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 启动、停止服务:</span></span><br><span class="line">systemctl start Service_Name1 Service_Name2</span><br><span class="line">systemctl stop Service_Name</span><br><span class="line"><span class="meta">#</span><span class="bash"> 开机启动服务：</span></span><br><span class="line">systemctl enable Service_Name</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 有时候，该命令可能没有响应，服务停不下来。这时候就不得不<span class="string">&quot;杀进程&quot;</span>了，向正在运行的进程发出<span class="built_in">kill</span>信号。</span></span><br><span class="line">systemctl kill Service_Name</span><br><span class="line"><span class="meta">#</span><span class="bash"> 服务重载、重启相关操作：</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 重载服务：服务未运行时不做任何事</span></span><br><span class="line">systemctl reload Service_Name</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 重启服务：服务已运行时重启之，服务未运行时启动之</span></span><br><span class="line">systemctl restart Service_Name</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 服务已运行时重启之，未运行时不启动之</span></span><br><span class="line">systemctl try-restart Service_Name</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 服务已运行时，如果支持reload，则reload，如果不支持则restart</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 服务未运行时，启动之</span></span><br><span class="line">systemctl reload-or-restart Service_Name</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 服务已运行时，如果支持reload，则reload，如果不支持则restart</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 服务未运行时，不做任何事</span></span><br><span class="line">systemctl reload-or-try-restart Service_Name</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 查看服务状态</span></span><br><span class="line">systemctl status Service_Name</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 检查服务是否active: 服务是否已启动</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 至少一个服务active时，返回0，否则返回非0退出状态码</span></span><br><span class="line">systemctl is-active Service_Name1 Service_Name2</span><br><span class="line">systemctl --quiet is-active Service_Name  # 静默模式</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 检查服务是否failed: 服务启动命令退出状态码非0或启动超时</span></span><br><span class="line">systemctl is-failed Service_Name</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>因为篇幅问题，<a href="/2021/08/01/%E5%88%9B%E5%BB%BA%E5%BC%80%E6%9C%BA%E8%87%AA%E5%90%AF%E7%A8%8B%E5%BA%8F%EF%BC%88%E4%B8%8B%E7%AF%87%EF%BC%89/" title="下篇继续介绍如何编写systemd service">下篇继续介绍如何编写systemd service</a></p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;在我们实现如何创建开机自启程序前，先简单了解下linux的启动过程。&lt;/p&gt;
&lt;h1 id=&quot;linux启动过程&quot;&gt;&lt;a href=&quot;#linux启动过程&quot; class=&quot;headerlink&quot; title=&quot;linux启动过程&quot;&gt;&lt;/a&gt;linux启动过程&lt;/h1&gt;&lt;p&gt;</summary>
      
    
    
    
    
    <category term="linux" scheme="https://slions.github.io/tags/linux/"/>
    
  </entry>
  
  <entry>
    <title>alias与rm的最佳实践</title>
    <link href="https://slions.github.io/2021/08/01/alias%E4%B8%8Erm%E7%9A%84%E6%9C%80%E4%BD%B3%E5%AE%9E%E8%B7%B5/"/>
    <id>https://slions.github.io/2021/08/01/alias%E4%B8%8Erm%E7%9A%84%E6%9C%80%E4%BD%B3%E5%AE%9E%E8%B7%B5/</id>
    <published>2021-08-01T03:37:38.000Z</published>
    <updated>2021-08-01T04:56:54.058Z</updated>
    
    <content type="html"><![CDATA[<p>不论是刚入职的运维萌新，还是工作多年的运维老油条，当在服务器上敲下rm时必须要保证命令的准确，在技术贴吧中也常常有人调侃到不想干了就<code>rm -rf /</code>与删库跑路等等的段子，如何让<code>rm</code>的操作更有保障呢，以下会提供一种解题思路。</p><h1 id="alias的用法"><a href="#alias的用法" class="headerlink" title="alias的用法"></a>alias的用法</h1><p>Linux alias命令用于设置指令的别名。</p><p>语法：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">alias[别名]=[指令名称]</span><br></pre></td></tr></table></figure><p>使用不带参数的alias将列出当前shell环境下所有的已定义的别名。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">[root@slions_pc1 home]# alias</span><br><span class="line">alias cp=&#x27;cp -i&#x27;</span><br><span class="line">alias egrep=&#x27;egrep --color=auto&#x27;</span><br><span class="line">alias fgrep=&#x27;fgrep --color=auto&#x27;</span><br><span class="line">alias grep=&#x27;grep --color=auto&#x27;</span><br><span class="line">alias l.=&#x27;ls -d .* --color=auto&#x27;</span><br><span class="line">alias ll=&#x27;ls -l --color=auto&#x27;</span><br><span class="line">alias ls=&#x27;ls --color=auto&#x27;</span><br><span class="line">alias mv=&#x27;mv -i&#x27;</span><br><span class="line">alias rm=&#x27;rm -i&#x27;</span><br><span class="line">alias which=&#x27;alias | /usr/bin/which --tty-only --read-alias --show-dot --show-tilde&#x27;</span><br></pre></td></tr></table></figure><p>另外需要说明的是，当别名和命令同名时，将优先执行别名(否则别名就没有意义了)，这可以从which的结果中看出：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@slions_pc1 home]# which rm</span><br><span class="line">alias rm=&#x27;rm -i&#x27;</span><br><span class="line">        /usr/bin/rm</span><br></pre></td></tr></table></figure><p>如果定义的命名名称和原始命令同名(例如定义的别名 ls=’ls -l’ )，此时如果想要明确使用原始命令，可以删除别名或者使用绝对路径或者使用转义符来还原命令。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[root@slions_pc1 home]# \ls</span><br><span class="line">slions</span><br><span class="line">[root@slions_pc1 home]# /usr/bin/ls</span><br><span class="line">slions</span><br></pre></td></tr></table></figure><p>alias命令是临时定义别名，要定义长久生效的别名就将别名定义语句写入<code>/etc/profile</code>或<code>~/.bash_profile</code>或<code>~/.bashrc</code>，第一个对所有用户有效，后面两个对对应用户有效。修改后记得使用source来重新调取这些配置文件。</p><p>使用unalias可以临时取消别名。</p><h1 id="alias与rm"><a href="#alias与rm" class="headerlink" title="alias与rm"></a>alias与rm</h1><p>如何让alias与rm配合起来能减少误操作引发的影响呢，整体思路就是rm变为mv操作，后续给备份目录增加个时效性机制，保障磁盘空间的可用性。</p><h2 id="alias的bug"><a href="#alias的bug" class="headerlink" title="alias的bug"></a>alias的bug</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">[root@slions_pc1 home]# ls</span><br><span class="line">slions</span><br><span class="line">[root@slions_pc1 home]# touch testfile</span><br><span class="line">[root@slions_pc1 home]# alias rmm=&#x27;cp $@ /tmp/backup;rm $@&#x27;</span><br><span class="line">[root@slions_pc1 home]# rmm testfile</span><br><span class="line">cp: 在&quot;/tmp/backup&quot; 后缺少了要操作的目标文件</span><br><span class="line">Try &#x27;cp --help&#x27; for more information.</span><br><span class="line">rm：是否删除普通空文件 &quot;testfile&quot;？y</span><br><span class="line">[root@slions_pc1 home]# ls</span><br><span class="line">slions</span><br><span class="line">[root@slions_pc1 home]# ls /tmp/backup/</span><br></pre></td></tr></table></figure><p>该别名的目的是删除文件时先备份到<code>/tmp/backup</code>目录下，然后再删除。<strong>按照man bash里的说明，别名rmm只是第一个cp命令的别名，分号后的rm不是别名的一部分，而是紧跟在别名后的下一行命令。当执行别名rmm时，首先读取别名到分号位置处，然后进行别名扩展，执行完别名命令后，再执行分号后的rm命令。</strong></p><p>上面的命令没有达到预期效果，问题出在cp的参数”$@”，该变量本表示提供的所有参数，但由于cp命令后使用分号分隔并定义了另一个命令，这使得执行别名命令时，参数无法传递到cp命令上，而只能传递到最后一个命令rm上，也就是说cp后的”$@”是空值。</p><p>可以测试下：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[root@slions_pc1 home]# touch testfile</span><br><span class="line">[root@slions_pc1 home]# alias rmm=&#x27;echo cp $@ /tmp/backup;echo rm $@&#x27;</span><br><span class="line">[root@slions_pc1 home]# rmm testfile</span><br><span class="line">cp /tmp/backup</span><br><span class="line">rm testfile</span><br></pre></td></tr></table></figure><p>从上面的结果中看到cp后的”$@”根本就没有进行扩展，而是空值。</p><p>那如果别名定义语句中没有使用分号或其他方法定义额外的命令，而是只有一个命令呢？别名一定就能正确工作吗？</p><p>测试会发现也有问题：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@slions_pc1 home]# alias rmm=&#x27;echo cp $@ /tmp/backup&#x27;</span><br><span class="line">[root@slions_pc1 home]# rmm testfile</span><br><span class="line">cp /tmp/backup testfile</span><br></pre></td></tr></table></figure><p>之所以无法正常工作，是因为<code>/tmp/backup</code>也是”$@”的一部分，且是”$@”中最前面的参数。</p><p>从上面的分析可以知道，alias是有其缺陷的，它只适合进行简单的命令和参数替换、补全，想要实现复杂的命令替代有点难度。因此man bash中建议尽量使用<strong>函数</strong>来取代别名。</p><h2 id="alias最佳实践"><a href="#alias最佳实践" class="headerlink" title="alias最佳实践"></a>alias最佳实践</h2><p>例如，为了让rm安全执行，使用以下方法定义别名：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[root@slions_pc1 home]# alias rmm=&#x27;move()&#123; /bin/mv -f $@ /tmp/backup; &#125;;move $@&#x27;</span><br><span class="line">[root@slions_pc1 home]# rmm testfile</span><br><span class="line">[root@slions_pc1 home]# ls</span><br><span class="line">slions</span><br><span class="line">[root@slions_pc1 home]# ls /tmp/backup/</span><br><span class="line">testfile</span><br></pre></td></tr></table></figure><p>因为执行别名时的参数只能传递给最后一个命令即move函数，但”$@”代表的参数可以传递给函数，让函数中的”$@”得到正确的扩展，于是整个别名都能合理且正确地执行。</p><p>或者直接定义一个shell function替代rm。例如向/etc/profile.d/rm.sh文件中写入：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">function rm()&#123; [ -d /tmp/rmbackup ] || mkdir /tmp/rmbackup;/bin/mv -f $@ /tmp/rmbackup; &#125;</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@slions_pc1 home]# chmod +x /etc/profile.d/rm.sh</span><br><span class="line">[root@slions_pc1 home]# source /etc/profile.d/rm.sh</span><br></pre></td></tr></table></figure><p>如此，执行rm命令时，便会执行此处定义的rm函数，使得rm变得更安全。但注意，这样的函数默认无法直接在脚本中使用，除非使用 <code>export -f function_name </code>导出函数，使其可以被子shell继承。</p><p>所以，可在/etc/profile.d/rm.sh文件的尾部加上导出语句：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">function rm()&#123; [ -d /tmp/rmbackup ] || mkdir /tmp/rmbackup;/bin/mv -f $@ /tmp/rmbackup; &#125;</span><br><span class="line">export -f rm</span><br></pre></td></tr></table></figure><p>如果function名和命令名相同，则默认优先执行function，除非使用command明确指定。例如上面定义了rm函数，如果想执行rm命令，除了使用/bin/rm，还可以如下操作：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">command rm testfile</span><br></pre></td></tr></table></figure><p>最后，如果是**在shell脚本里涉及到rm命令，那么更建议在每次rm之前先cd到那个目录下，然后再rm相对路径，这样至少能保证不出现符号”/“**。当然，最重要的是习惯和责任心，切勿忙中出错。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;不论是刚入职的运维萌新，还是工作多年的运维老油条，当在服务器上敲下rm时必须要保证命令的准确，在技术贴吧中也常常有人调侃到不想干了就&lt;code&gt;rm -rf /&lt;/code&gt;与删库跑路等等的段子，如何让&lt;code&gt;rm&lt;/code&gt;的操作更有保障呢，以下会提供一种解题思路。</summary>
      
    
    
    
    
    <category term="linux" scheme="https://slions.github.io/tags/linux/"/>
    
  </entry>
  
  <entry>
    <title>linux用户与组管理（下篇）</title>
    <link href="https://slions.github.io/2021/07/31/linux%E7%94%A8%E6%88%B7%E4%B8%8E%E7%BB%84%E7%AE%A1%E7%90%86%EF%BC%88%E4%B8%8B%E7%AF%87%EF%BC%89/"/>
    <id>https://slions.github.io/2021/07/31/linux%E7%94%A8%E6%88%B7%E4%B8%8E%E7%BB%84%E7%AE%A1%E7%90%86%EF%BC%88%E4%B8%8B%E7%AF%87%EF%BC%89/</id>
    <published>2021-07-31T07:27:52.000Z</published>
    <updated>2021-07-31T08:17:10.390Z</updated>
    
    <content type="html"><![CDATA[<h1 id="用户和组管理命令"><a href="#用户和组管理命令" class="headerlink" title="用户和组管理命令"></a>用户和组管理命令</h1><h2 id="useradd-和-adduser"><a href="#useradd-和-adduser" class="headerlink" title="useradd 和 adduser"></a>useradd 和 adduser</h2><p>adduser是useradd的一个软链接。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@slions_pc1 ~]# ls -l /usr/sbin/adduser</span><br><span class="line">lrwxrwxrwx. 1 root root 7 5月  21 16:45 /usr/sbin/adduser -&gt; useradd</span><br></pre></td></tr></table></figure><p>useradd用法：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line">useradd [options] login_name</span><br><span class="line">选项说明：</span><br><span class="line">-b：指定家目录的basedir，默认为/home目录</span><br><span class="line">-d：指定用户家目录，不写时默认为/home/user_name</span><br><span class="line">-m：要创建家目录时，若家目录不存在则自动创建，若不指定该项且/etc/login.defs中的CREATE_HOME未启用时将不会创建家目录</span><br><span class="line">-M：显式指明不要创建家目录，会覆盖/etc/login.defs中的CREATE_HOME设置</span><br><span class="line"> </span><br><span class="line">-g：指定用户主组，要求组已存在</span><br><span class="line">-G：指定用户的辅助组，多个组以逗号分隔</span><br><span class="line">-N：明确指明不要创建和用户名同名的组名</span><br><span class="line">-U：明确指明要创建一个和用户名同名的组，并将用户加入到此组中</span><br><span class="line"></span><br><span class="line">-o：允许创建一个重复UID的用户，只有和-u选项同时使用时才生效</span><br><span class="line">-r：创建一个系统用户。useradd命令不会为此选项的系统用户创建家目录，除非明确使用-m选项</span><br><span class="line">-s：指定用户登录的shell，默认留空。此时将选择/etc/default/useradd中的SHELL变量设置</span><br><span class="line">-u：指定用户uid，默认uid必须唯一，除非使用了-o选项</span><br><span class="line">-c：用户的注释信息 </span><br><span class="line"></span><br><span class="line">-k：指定骨架目录(skeleton)</span><br><span class="line">-K：修改/etc/login.defs文件中有关于用户的配置项，不能修改组相关的配置。设置方式为KEY=VALUE，如-K UID_MIN=100</span><br><span class="line">-D：修改useradd创建用户时的默认选项，就修改/etc/default/useradd文件</span><br><span class="line">-e：帐户过期时间，格式为&quot;YYYY-MM-DD&quot;</span><br><span class="line">-f：密码过期后，该账号还能存活多久才被禁用，设置为0表示密码过期立即禁用帐户，设置为-1表示禁用此功能</span><br><span class="line">-l：不要将用户的信息写入到lastlog和faillog文件中。默认情况下，用户信息会写入到这两个文件中</span><br><span class="line"></span><br><span class="line">useradd -D [options]</span><br><span class="line">修改/etc/default/useradd文件</span><br><span class="line">选项说明：不加任何选项时会列出默认属性</span><br><span class="line">-b, --base-dir BASE_DIR</span><br><span class="line">-e, --expiredate EXPIRE_DATE</span><br><span class="line">-f, --inactive INACTIVE</span><br><span class="line">-g, --gid GROUP</span><br><span class="line">-s, --shell SHELL</span><br></pre></td></tr></table></figure><p>useradd创建用户时，默认会自动创建一个和用户名相同的用户组，这是<code>/etc/login.defs</code>中的USERGROUP_ENAB变量控制的。</p><p>useradd创建普通用户时，不加任何和家目录相关的选项时，是否创建家目录是由<code>/etc/login.defs</code>中的CREATE_HOME变量控制的。</p><h2 id="批量创建用户-newusers"><a href="#批量创建用户-newusers" class="headerlink" title="批量创建用户 newusers"></a>批量创建用户 newusers</h2><p>newusers用于批量创建或修改已有用户信息。在创建用户时，它会读取<code>/etc/login.defs</code>文件中的配置项。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">[root@slions_pc1 ~]# newusers -h</span><br><span class="line">用法：newusers [选项]</span><br><span class="line"></span><br><span class="line">选项：</span><br><span class="line">  -c, --crypt-method 方法        加密方法(NONE DES MD5 SHA256 SHA512 中的一个)</span><br><span class="line">  -h, --help                    显示此帮助信息并推出</span><br><span class="line">  -r, --system                  创建系统帐号</span><br><span class="line">  -R, --root CHROOT_DIR         chroot 到的目录</span><br><span class="line">  -s, --sha-rounds              SHA* 加密算法中的 SHA 旁边的数字</span><br></pre></td></tr></table></figure><p>newusers命令从file中或标准输入中读取要创建或修改用户的信息，文件中每行格式都一样(同passwd格式)，一行代表一个用户。格式如下：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">name:passwd:uid:gid:note:homedir:shell</span><br></pre></td></tr></table></figure><p>newusers首先尝试创建或修改所有指定的用户，然后将信息写入到user和group的文件中。如果尝试创建或修改用户过程中发生错误，则所有动作都将回滚，但如果在写入过程中发生错误，则写入成功的不会回滚，这将可能导致文件的不一致性。要检查用户、组文件的一致性，可以使用showdow-utils包提供的grpck和pwck命令。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">[root@slions_pc1 ~]# cat batch_adduser</span><br><span class="line">s1:123456:7295:7295::/home/s1:/bin/bash</span><br><span class="line">s2:123456:::::/bin/bash</span><br><span class="line">[root@slions_pc1 ~]# newusers -c SHA512 batch_adduser</span><br><span class="line">[root@slions_pc1 ~]# tail -2 /etc/passwd</span><br><span class="line">s1:x:7295:7295::/home/s1:/bin/bash</span><br><span class="line">s2:x:7296:7296:::/bin/bash</span><br><span class="line">[root@slions_pc1 ~]# tail -2 /etc/shadow</span><br><span class="line">s1:$6$OSCVQmJiFP/U4CbD$72ZkAJNKs4ehMgfxJR..tqNuy7yKHINycOiB/.lW4ANBtuIMuIcsphgw8mcfkR7A1tvhKifG6vmPbc8VjmfmV.:18839:0:99999:7:::</span><br><span class="line">s2:$6$/7nes/BRVe5pw7Gw$qV8.mVjf4Mpv9zVwjsKDSmtmx8qCwZwX03hmsbuFM.CEHJ.X76pX6Css8dvIAE87j7GcihAJN8lSn3Lg.KD.Q.:18839:0:99999:7:::</span><br><span class="line"></span><br></pre></td></tr></table></figure><h2 id="创建组-groupadd"><a href="#创建组-groupadd" class="headerlink" title="创建组 groupadd"></a>创建组 groupadd</h2><p>用法：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">[root@slions_pc1 ~]# groupadd -h</span><br><span class="line">用法：groupadd [选项] 组</span><br><span class="line"></span><br><span class="line">选项:</span><br><span class="line">  -f, --force           如果组已经存在则成功退出</span><br><span class="line">                        并且如果 GID 已经存在则取消 -g</span><br><span class="line">  -g, --gid GID                 为新组使用 GID</span><br><span class="line">  -h, --help                    显示此帮助信息并推出</span><br><span class="line">  -K, --key KEY=VALUE           不使用 /etc/login.defs 中的默认值</span><br><span class="line">  -o, --non-unique              允许创建有重复 GID 的组</span><br><span class="line">  -p, --password PASSWORD       为新组使用此加密过的密码</span><br><span class="line">  -r, --system                  创建一个系统账户</span><br><span class="line">  -R, --root CHROOT_DIR         chroot 到的目录</span><br></pre></td></tr></table></figure><h2 id="修改密码-passwd"><a href="#修改密码-passwd" class="headerlink" title="修改密码 passwd"></a>修改密码 passwd</h2><p>修改密码的工具。默认passwd命令不允许为用户创建空密码。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">[root@slions_pc1 ~]# passwd  --help</span><br><span class="line">用法: passwd [选项...] &lt;帐号名称&gt;</span><br><span class="line">  -k, --keep-tokens       保持身份验证令牌不过期</span><br><span class="line">  -d, --delete            删除已命名帐号的密码(只有根用户才能进行此操作)</span><br><span class="line">  -l, --lock              锁定指名帐户的密码(仅限 root 用户)</span><br><span class="line">  -u, --unlock            解锁指名账户的密码(仅限 root 用户)</span><br><span class="line">  -e, --expire            终止指名帐户的密码(仅限 root 用户)</span><br><span class="line">  -f, --force             强制执行操作</span><br><span class="line">  -x, --maximum=DAYS      密码的最长有效时限(只有根用户才能进行此操作)</span><br><span class="line">  -n, --minimum=DAYS      密码的最短有效时限(只有根用户才能进行此操作)</span><br><span class="line">  -w, --warning=DAYS      在密码过期前多少天开始提醒用户(只有根用户才能进行此操作)</span><br><span class="line">  -i, --inactive=DAYS     当密码过期后经过多少天该帐号会被禁用(只有根用户才能进行此操作)</span><br><span class="line">  -S, --status            报告已命名帐号的密码状态(只有根用户才能进行此操作)</span><br><span class="line">  --stdin                 从标准输入读取令牌(只有根用户才能进行此操作)</span><br></pre></td></tr></table></figure><h2 id="批量修改密码-chpasswd"><a href="#批量修改密码-chpasswd" class="headerlink" title="批量修改密码 chpasswd"></a>批量修改密码 chpasswd</h2><p>以批处理模式从标准输入中获取提供的用户和密码来修改用户密码，可以一次修改多个用户密码。也就是说不用交互。适用于一次性创建了多个用户时为他们提供密码。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">[root@slions_pc1 ~]# chpasswd --help</span><br><span class="line">用法：chpasswd [选项]</span><br><span class="line"></span><br><span class="line">选项：</span><br><span class="line">  -c, --crypt-method 方法        加密方法(NONE DES MD5 SHA256 SHA512 中的一个)</span><br><span class="line">  -e, --encrypted               提供的密码已经加密</span><br><span class="line">  -h, --help                    显示此帮助信息并推出</span><br><span class="line">  -m, --md5             使用 MD5 算法加密明文密码</span><br><span class="line">  -R, --root CHROOT_DIR         chroot 到的目录</span><br><span class="line">  -s, --sha-rounds              SHA* 加密算法中的 SHA 旁边的数字</span><br></pre></td></tr></table></figure><p>chpasswd会读取<code>/etc/login.defs</code>中的相关配置，修改成功后会将密码信息写入到密码文件中。</p><p>该命令的修改密码的处理方式是先在内存中修改，如果所有用户的密码都能设置成功，然后才写入到磁盘密码文件中。在内存中修改过程中出错，则所有修改都回滚，但若在写入密码文件过程中出错，则成功的不会回滚。</p><p>修改单个用户密码。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> <span class="built_in">echo</span> <span class="string">&quot;s1:123456&quot;</span> | chpasswd -c SHA512</span></span><br></pre></td></tr></table></figure><p>修改多个用户密码，则提供的每个用户对都要分行。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> <span class="built_in">echo</span>  -e <span class="string">&#x27;s1:123456\ns2:123456&#x27;</span> | chpasswd</span></span><br></pre></td></tr></table></figure><p>更方便的是写入到文件中，每行一个用户密码对。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> cat /tmp/passwdfile</span></span><br><span class="line">s1:123456</span><br><span class="line">s2:123456</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">$</span><span class="bash"> chapasswd -c SHA512 &lt;/tmp/passwdfile</span></span><br></pre></td></tr></table></figure><h2 id="chage"><a href="#chage" class="headerlink" title="chage"></a>chage</h2><p>chage命令主要修改或查看和密码时间相关的内容。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">-l：列出指定用户密码相关信息</span><br><span class="line">-E：指定帐户(不是密码)过期时间，所以是强锁定，如果指定为0，则立即过期，即直接锁定该用户</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">[root@slions_pc1 ~]# chage -l s1|column -t</span><br><span class="line">最近一次密码修改时间            ：7月    31,  2021</span><br><span class="line">密码过期时间                    ：从不</span><br><span class="line">密码失效时间                    ：从不</span><br><span class="line">帐户过期时间                    ：从不</span><br><span class="line">两次改变密码之间相距的最小天数  ：0</span><br><span class="line">两次改变密码之间相距的最大天数  ：99999</span><br><span class="line">在密码过期之前警告的天数        ：7</span><br><span class="line">[root@slions_pc1 ~]# chage -E 0 s1</span><br><span class="line">[root@slions_pc1 ~]# chage -l s1|column -t</span><br><span class="line">最近一次密码修改时间            ：7月    31,  2021</span><br><span class="line">密码过期时间                    ：从不</span><br><span class="line">密码失效时间                    ：从不</span><br><span class="line">帐户过期时间                    ：1月    01,  1970</span><br><span class="line">两次改变密码之间相距的最小天数  ：0</span><br><span class="line">两次改变密码之间相距的最大天数  ：99999</span><br><span class="line">在密码过期之前警告的天数        ：7</span><br><span class="line"></span><br></pre></td></tr></table></figure><h2 id="删除用户-userdel"><a href="#删除用户-userdel" class="headerlink" title="删除用户 userdel"></a>删除用户 userdel</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">userdel [options] login_name</span><br><span class="line">-r：递归删除家目录，默认不删除家目录。</span><br><span class="line">-f：强制删除用户，即使这个用户正处于登录状态。同时也会强制删除家目录。</span><br></pre></td></tr></table></figure><p>一般不直接删除家目录，即不用-r，可以vim /etc/passwd，将不需要的用户直接注释掉。</p><h2 id="删除组-groupdel"><a href="#删除组-groupdel" class="headerlink" title="删除组 groupdel"></a>删除组 groupdel</h2><p>如果要删除的组是某用户的主组，需要先删除主组中的用户。</p><h2 id="修改帐户属性信息-usermod"><a href="#修改帐户属性信息-usermod" class="headerlink" title="修改帐户属性信息 usermod"></a>修改帐户属性信息 usermod</h2><p>修改帐户属性信息。必须要确保在执行该命令的时候，待修改的用户没有在执行进程。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">usermod [options] login</span><br><span class="line">选项说明：</span><br><span class="line">-l：修改用户名，仅仅只是改用户名，其他的一切都不会改动(uid、家目录等)</span><br><span class="line">-u：新的uid，新的uid必须唯一，除非同时使用了-o选项</span><br><span class="line">-g：修改用户主组，可以是以gid或组名。对于那些以旧组为所属组的文件(除原家目录)，需要重新手动修改其所属组</span><br><span class="line">-m：移动家目录内容到新的位置，该选项只在和-d选项一起使用时才生效</span><br><span class="line">-d：修改用户的家目录位置，若不存在则自动创建。默认旧的家目录不会删除</span><br><span class="line">    如果同时指定了-m选项，则旧的家目录中的内容会移到新家目录</span><br><span class="line">    如果当前用户家目录不存在或没有家目录，则也不会创建新的家目录</span><br><span class="line">-o：允许用户使用非唯一的UID</span><br><span class="line">-s：修改用的shell，留空则选择默认shell</span><br><span class="line">-c：修改用户注释信息</span><br><span class="line"></span><br><span class="line">-a：将用户以追加的方式加入到辅助组中，只能和-G选项一起使用</span><br><span class="line">-G：将用户加入指定的辅助组中，若此处未列出某组，而此前该用户又是该组成员，则会删除该组中此成员</span><br><span class="line"></span><br><span class="line">-L：锁定用户的密码，将在/etc/shadow的密码列加上前缀&quot;!&quot;或&quot;!!&quot;</span><br><span class="line">-U：解锁用户的密码，解锁的方式是移除shadow文件密码列的前缀&quot;!&quot;或&quot;!!&quot;</span><br><span class="line">-e：帐户过期时间，时间格式为&quot;YYYY-MM-DD&quot;，如果给一个空的参数，则立即禁用该帐户</span><br><span class="line">-f：密码过期后多少天，帐户才过期被禁用，0表示密码过期帐户立即禁用，-1表示禁用该功能</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>同样，还有groupmod修改组信息，用法非常简单，几乎也用不上，不多说了。</p><h2 id="vipw和vigr"><a href="#vipw和vigr" class="headerlink" title="vipw和vigr"></a>vipw和vigr</h2><p>vipw和vigr是编辑用户和组文件的工具，vipw可以修改/etc/passwd和/etc/shadow，vigr可以修改/etc/group和/etc/gshadow，用这两个工具比较安全，在修改的时候会检查文件的一致性。</p><p>删除用户出错时，提示用户正在被进程占用。可以使用vi编辑/etc/paswd和/etc/shadow文件将该用户对应的行删除掉。也可以使用vipw和vipw -s来分别编辑/etc/paswd和/etc/shadow文件。它们的作用是一样的。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;用户和组管理命令&quot;&gt;&lt;a href=&quot;#用户和组管理命令&quot; class=&quot;headerlink&quot; title=&quot;用户和组管理命令&quot;&gt;&lt;/a&gt;用户和组管理命令&lt;/h1&gt;&lt;h2 id=&quot;useradd-和-adduser&quot;&gt;&lt;a href=&quot;#useradd-和-ad</summary>
      
    
    
    
    
    <category term="linux" scheme="https://slions.github.io/tags/linux/"/>
    
  </entry>
  
  <entry>
    <title>linux用户与组管理（上篇）</title>
    <link href="https://slions.github.io/2021/07/30/linux%E7%94%A8%E6%88%B7%E4%B8%8E%E7%BB%84%E7%AE%A1%E7%90%86%EF%BC%88%E4%B8%8A%E7%AF%87%EF%BC%89/"/>
    <id>https://slions.github.io/2021/07/30/linux%E7%94%A8%E6%88%B7%E4%B8%8E%E7%BB%84%E7%AE%A1%E7%90%86%EF%BC%88%E4%B8%8A%E7%AF%87%EF%BC%89/</id>
    <published>2021-07-30T04:34:26.000Z</published>
    <updated>2021-07-31T07:32:03.374Z</updated>
    
    <content type="html"><![CDATA[<h1 id="基本概念"><a href="#基本概念" class="headerlink" title="基本概念"></a>基本概念</h1><p>Linux 继承了UNIX 对用户的优秀支持，其属于多用户的操作系统。在linux中用户与组都是一种身份认证资源。</p><p>每个用户都有用户名、用户的唯一编号uid、所属组及其默认的shell，可能还有密码、家目录、附属组、注释信息等。每个组也有自己的名称、组唯一编号gid。一般来说，gid和uid都是相同的，但可以根据自己的实际需求来设置。组分为主组(primary group)和辅助组(secondary group)两种，用户一定会属于某个主组，也可以同时加入多个辅助组。</p><p>在Linux中，用户按权限来分类，可以分为3类：</p><ul><li><p>超级管理员</p><p>超级管理员是最高权限者，它的uid为0，默认超级管理员用户名为root。</p></li><li><p>系统用户</p><p>由系统或程序自行建立的账户被称为系统用户，特点是他们具有某些特权但又不需要登录操作系统。他们的uid范围从201到999，centos6的uid范围是1到499，出于安全考虑，它们一般不用来登录，所以它们的shell一般是/sbin/nologin，而且大多数时候它们是没有家目录的。</p></li><li><p>普通用户</p><p>普通用户是权限受到限制的用户，默认只能执行/bin、/usr/bin、/usr/local/bin和自身家目录下的命令。它们的uid从1000开始。尽管普通用户权限收到限制，但是它对自身家目录下的文件是有所有权限的。</p></li></ul><p>默认root用户的家目录为/root，其他用户的家目录一般在/home下以用户名命名的目录中。</p><h1 id="用户管理文件"><a href="#用户管理文件" class="headerlink" title="用户管理文件"></a>用户管理文件</h1><h2 id="用户文件"><a href="#用户文件" class="headerlink" title="用户文件"></a>用户文件</h2><p><code>/etc/passwd</code>文件里记录的是操作系统中用户的信息，这里面记录了几行就表示系统中有几个系统用户。它的格式大致如下：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">[root@slions_pc1 ~]# cat /etc/passwd</span><br><span class="line">root:x:0:0:root:/root:/bin/bash</span><br><span class="line">bin:x:1:1:bin:/bin:/sbin/nologin</span><br><span class="line">daemon:x:2:2:daemon:/sbin:/sbin/nologin</span><br><span class="line">adm:x:3:4:adm:/var/adm:/sbin/nologin</span><br><span class="line">lp:x:4:7:lp:/var/spool/lpd:/sbin/nologin</span><br><span class="line">sync:x:5:0:sync:/sbin:/bin/sync</span><br><span class="line">shutdown:x:6:0:shutdown:/sbin:/sbin/shutdown</span><br><span class="line">halt:x:7:0:halt:/sbin:/sbin/halt</span><br><span class="line">mail:x:8:12:mail:/var/spool/mail:/sbin/nologin</span><br><span class="line">operator:x:11:0:operator:/root:/sbin/nologin</span><br><span class="line">games:x:12:100:games:/usr/games:/sbin/nologin</span><br><span class="line">ftp:x:14:50:FTP User:/var/ftp:/sbin/nologin</span><br><span class="line">nobody:x:99:99:Nobody:/:/sbin/nologin</span><br><span class="line">systemd-network:x:192:192:systemd Network Management:/:/sbin/nologin</span><br><span class="line">dbus:x:81:81:System message bus:/:/sbin/nologin</span><br><span class="line">polkitd:x:999:998:User for polkitd:/:/sbin/nologin</span><br><span class="line">libstoragemgmt:x:998:997:daemon account for libstoragemgmt:/var/run/lsm:/sbin/nologin</span><br><span class="line">abrt:x:173:173::/etc/abrt:/sbin/nologin</span><br><span class="line">rpc:x:32:32:Rpcbind Daemon:/var/lib/rpcbind:/sbin/nologin</span><br><span class="line">sshd:x:74:74:Privilege-separated SSH:/var/empty/sshd:/sbin/nologin</span><br><span class="line">postfix:x:89:89::/var/spool/postfix:/sbin/nologin</span><br><span class="line">ntp:x:38:38::/etc/ntp:/sbin/nologin</span><br><span class="line">chrony:x:997:995::/var/lib/chrony:/sbin/nologin</span><br><span class="line">tcpdump:x:72:72::/:/sbin/nologin</span><br></pre></td></tr></table></figure><p>/etc/passwd 内容总共分为 7 个区域 ,以“ :” 作为区域的分隔符。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">用户名:x:uid:gid:用户注释信息:家目录:使用的shell类型</span><br></pre></td></tr></table></figure><p>第一列：用户名。区分大小写，账户名可以以字母 , 数字 , 英文句号 ‘.’, 下 划线 ‘_’, 连字符 ‘-‘ 等连和使用，账户名必须唯一</p><p>第二列：x。在以前老版本的系统上，第二列是存放用户密码的，但是密码和用户信息放在一起不便于管理(密钥要保证其特殊属性)，所以后来将密码单独放在另一个文件/etc/shadow中，这里就都写成x了。</p><p>第三列：uid。UID 号应该唯一，UID 号 0-999 为保留 UID</p><p>第四列：gid。</p><p>第五列：用户注释信息。</p><p>第六列：用户家目录，普通账户主目录默认建立在 /home 下。</p><p>第七列：用户的默认shell，虽然叫shell，但其实可以是任意一个可执行程序或脚本。</p><h2 id="密码文件"><a href="#密码文件" class="headerlink" title="密码文件"></a>密码文件</h2><p><code>/etc/shadow</code>文件管理着用户的密码 , 格式大致为：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">[root@slions_pc1 ~]# cat /etc/shadow</span><br><span class="line">root:$6$JifzPmKZFWt52T4M$2s.UGoczIsSGCFKiqw/KhfWsiLKgwAHqY3dq8jLsHg4/RGZ1NoSKhmgfeEgZiPLJVWJbafyoVLiVRptCi4KIs1::0:99999:7:::</span><br><span class="line">bin:*:17834:0:99999:7:::</span><br><span class="line">daemon:*:17834:0:99999:7:::</span><br><span class="line">adm:*:17834:0:99999:7:::</span><br><span class="line">lp:*:17834:0:99999:7:::</span><br><span class="line">sync:*:17834:0:99999:7:::</span><br><span class="line">shutdown:*:17834:0:99999:7:::</span><br><span class="line">halt:*:17834:0:99999:7:::</span><br><span class="line">mail:*:17834:0:99999:7:::</span><br><span class="line">operator:*:17834:0:99999:7:::</span><br><span class="line">games:*:17834:0:99999:7:::</span><br><span class="line">ftp:*:17834:0:99999:7:::</span><br><span class="line">nobody:*:17834:0:99999:7:::</span><br><span class="line">systemd-network:!!:18768::::::</span><br><span class="line">dbus:!!:18768::::::</span><br><span class="line">polkitd:!!:18768::::::</span><br><span class="line">libstoragemgmt:!!:18768::::::</span><br><span class="line">abrt:!!:18768::::::</span><br><span class="line">rpc:!!:18768:0:99999:7:::</span><br><span class="line">sshd:!!:18768::::::</span><br><span class="line">postfix:!!:18768::::::</span><br><span class="line">ntp:!!:18768::::::</span><br><span class="line">chrony:!!:18768::::::</span><br><span class="line">tcpdump:!!:18768::::::</span><br></pre></td></tr></table></figure><p>其有 9 个区域，每个区域的作用如下 :</p><p>第一列：用户名。( 与 /etc/passwd 一致 )<br>第二列：加密后的密码。但是这一列是有玄机的，有些特殊的字符表示特殊的意义。</p><ul><li><p>①.该列留空，即”::”，表示该用户没有密码。</p></li><li><p>②.该列为”!”，即”:!:”，表示该用户被锁，被锁将无法登陆，但是可能其他的登录方式是不受限制的，如ssh key的方式，su的方式。</p></li><li><p>③.该列为”<em>”，即”:</em>:”，也表示该用户被锁，和”!”效果是一样的。</p></li><li><p>④.该列以”!”或”!!”开头，则也表示该用户被锁。</p></li><li><p>⑤.该列为”!!”，即”:!!:”，表示该用户从来没设置过密码。</p></li><li><p>⑥.如果格式为”$id$salt$hashed”，则表示该用户密码正常。其中$id$的id表示密码的加密算法，$1$表示使用MD5算法，$2a$表示使用Blowfish算法，”$2y$”是另一算法长度的Blowfish,”$5$”表示SHA-256算法，而”$6$”表示SHA-512算法，可见上面的结果中都是使用sha-512算法的。$5$和$6$这两种算法的破解难度远高于MD5。$salt$是加密时使用的salt，$hashed才是真正的密码部分。</p></li></ul><p>第三列：密码自新纪元 (1970-1-1) 起到用户前一次修 改密码的天数。<br>第四列：密码最少使用期限(天数)。密码前次与下一次修改的时间间隔 , 一般为“０”位不设定，可随时修改。<br>第五列：密码最大使用期限(天数)。超过了它不一定密码就失效，可能下一个字段设置了过期后的宽限天数。设置为空时将永不过期，后面设置的提醒和警告将失效。root等一些用户的已经默认设置为了99999，表示永不过期。如果值设置小于最短使用期限，用户将不能修改密码。<br>第六列：密码过期前多少天就开始提醒用户密码将要过期。空或0将不提醒。<br>第七列：密码过期后宽限的天数，在宽限时间内用户无法使用原密码登录，必须改密码或者联系管理员。设置为空表示没有强制的宽限时间，可以过期后的任意时间内修改密码。<br>第八列：帐号过期时间。从1970年1月1日开始计算天数。设置为空帐号将永不过期，不能设置为0。不同于密码过期，密码过期后账户还有效，改密码后还能登录；帐号过期后帐号失效，修改密码重设密码都无法使用该帐号。<br>第九列：保留字段。</p><h2 id="组文件"><a href="#组文件" class="headerlink" title="组文件"></a>组文件</h2><p><code>/etc/group</code>包含了组信息。格式大致为：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line">[root@slions_pc1 ~]# cat /etc/group</span><br><span class="line">root:x:0:</span><br><span class="line">bin:x:1:</span><br><span class="line">daemon:x:2:</span><br><span class="line">sys:x:3:</span><br><span class="line">adm:x:4:</span><br><span class="line">tty:x:5:</span><br><span class="line">disk:x:6:</span><br><span class="line">lp:x:7:</span><br><span class="line">mem:x:8:</span><br><span class="line">kmem:x:9:</span><br><span class="line">wheel:x:10:</span><br><span class="line">cdrom:x:11:</span><br><span class="line">mail:x:12:postfix</span><br><span class="line">man:x:15:</span><br><span class="line">dialout:x:18:</span><br><span class="line">floppy:x:19:</span><br><span class="line">games:x:20:</span><br><span class="line">tape:x:33:</span><br><span class="line">video:x:39:</span><br><span class="line">ftp:x:50:</span><br><span class="line">lock:x:54:</span><br><span class="line">audio:x:63:</span><br><span class="line">nobody:x:99:</span><br><span class="line">users:x:100:</span><br><span class="line">utmp:x:22:</span><br><span class="line">utempter:x:35:</span><br><span class="line">input:x:999:</span><br><span class="line">systemd-journal:x:190:</span><br><span class="line">systemd-network:x:192:</span><br><span class="line">dbus:x:81:</span><br><span class="line">polkitd:x:998:</span><br><span class="line">libstoragemgmt:x:997:</span><br><span class="line">ssh_keys:x:996:</span><br><span class="line">abrt:x:173:</span><br><span class="line">rpc:x:32:</span><br><span class="line">sshd:x:74:</span><br><span class="line">slocate:x:21:</span><br><span class="line">postdrop:x:90:</span><br><span class="line">postfix:x:89:</span><br><span class="line">ntp:x:38:</span><br><span class="line">chrony:x:995:</span><br><span class="line">tcpdump:x:72:</span><br><span class="line">stapusr:x:156:</span><br><span class="line">stapsys:x:157:</span><br><span class="line">stapdev:x:158:</span><br><span class="line">cgred:x:994:</span><br><span class="line">docker:x:993:</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>每行一个组，每一行3个冒号共4列属性：</p><p>第一列：组名。<br>第二列：占位符。<br>第三列：gid。<br>第四列：该组下的user列表，这些user成员以该组做为辅助组，多个成员使用逗号隔开。</p><h2 id="框架目录"><a href="#框架目录" class="headerlink" title="框架目录"></a>框架目录</h2><p><code>/etc/skel</code>框架目录中的文件是每次新建用户时，都会复制到新用户家目录里的文件。默认只有3个环境配置文件，可以修改这里面的内容，或者添加几个文件在骨架目录中，以后新建用户时就会自动获取到这些环境和文件。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[root@slions_pc2 ~]# ls -lA /etc/skel/</span><br><span class="line">总用量 12</span><br><span class="line">-rw-r--r--. 1 root root  18 10月 31 2018 .bash_logout</span><br><span class="line">-rw-r--r--. 1 root root 193 10月 31 2018 .bash_profile</span><br><span class="line">-rw-r--r--. 1 root root 231 10月 31 2018 .bashrc</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>删除家目录下这些文件，会导致某些设置出现问题。例如删除”.bashrc”这个文件，会导致提示符变异的问题(-bash-4.2$)。</p><h2 id="创建用户限制文件"><a href="#创建用户限制文件" class="headerlink" title="创建用户限制文件"></a>创建用户限制文件</h2><p><code>/etc/login.defs</code>设置用户帐号限制的文件。该文件里的配置对root用户无效。</p><p>如果/etc/shadow文件里有相同的选项，则以/etc/shadow里的设置为准，也就是说/etc/shadow的配置优先级高于/etc/login.defs。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br></pre></td><td class="code"><pre><span class="line">[root@slions_pc2 ~]# cat /etc/login.defs</span><br><span class="line"><span class="meta">#</span><span class="bash"></span></span><br><span class="line"><span class="bash"><span class="comment"># Please note that the parameters in this configuration file control the</span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> behavior of the tools from the shadow-utils component. None of these</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> tools uses the PAM mechanism, and the utilities that use PAM (such as the</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> passwd <span class="built_in">command</span>) should therefore be configured elsewhere. Refer to</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> /etc/pam.d/system-auth <span class="keyword">for</span> more information.</span></span><br><span class="line"><span class="meta">#</span><span class="bash"></span></span><br><span class="line"><span class="bash"></span><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> *REQUIRED*</span></span><br><span class="line"><span class="meta">#</span><span class="bash">   Directory <span class="built_in">where</span> mailboxes reside, _or_ name of file, relative to the</span></span><br><span class="line"><span class="meta">#</span><span class="bash">   home directory.  If you _do_ define both, MAIL_DIR takes precedence.</span></span><br><span class="line"><span class="meta">#</span><span class="bash">   QMAIL_DIR is <span class="keyword">for</span> Qmail</span></span><br><span class="line"><span class="meta">#</span><span class="bash"></span></span><br><span class="line"><span class="bash"><span class="comment">#QMAIL_DIR      Maildir           # QMAIL_DIR是Qmail邮件的目录，所以可以不设置它</span></span></span><br><span class="line">MAIL_DIR        /var/spool/mail   # 默认邮件根目录，即信箱</span><br><span class="line"><span class="meta">#</span><span class="bash">MAIL_FILE      .mail             <span class="comment"># mail文件的格式是.mail</span></span></span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> Password aging controls:</span></span><br><span class="line"><span class="meta">#</span><span class="bash"></span></span><br><span class="line"><span class="bash"><span class="comment">#       PASS_MAX_DAYS   Maximum number of days a password may be used.</span></span></span><br><span class="line"><span class="meta">#</span><span class="bash">       PASS_MIN_DAYS   Minimum number of days allowed between password changes.</span></span><br><span class="line"><span class="meta">#</span><span class="bash">       PASS_MIN_LEN    Minimum acceptable password length.</span></span><br><span class="line"><span class="meta">#</span><span class="bash">       PASS_WARN_AGE   Number of days warning given before a password expires.</span></span><br><span class="line"><span class="meta">#</span><span class="bash"></span></span><br><span class="line"><span class="bash"><span class="comment"># Password aging controls:</span></span></span><br><span class="line">PASS_MAX_DAYS   99999         # 密码最大有效期(天)</span><br><span class="line">PASS_MIN_DAYS   0             # 两次密码修改之间最小时间间隔</span><br><span class="line">PASS_MIN_LEN    5             # 密码最短长度</span><br><span class="line">PASS_WARN_AGE   7             # 密码过期前给警告信息的时间</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 控制useradd创建用户时自动选择的uid范围</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> Min/max values <span class="keyword">for</span> automatic uid selection <span class="keyword">in</span> useradd</span></span><br><span class="line"><span class="meta">#</span><span class="bash"></span></span><br><span class="line"><span class="bash">UID_MIN                  1000</span></span><br><span class="line">UID_MAX                 60000</span><br><span class="line"><span class="meta">#</span><span class="bash"> System accounts</span></span><br><span class="line">SYS_UID_MIN               201</span><br><span class="line">SYS_UID_MAX               999</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 控制groupadd创建组时自动选择的gid范围</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> Min/max values <span class="keyword">for</span> automatic gid selection <span class="keyword">in</span> groupadd</span></span><br><span class="line"><span class="meta">#</span><span class="bash"></span></span><br><span class="line"><span class="bash">GID_MIN                  1000</span></span><br><span class="line">GID_MAX                 60000</span><br><span class="line"><span class="meta">#</span><span class="bash"> System accounts</span></span><br><span class="line">SYS_GID_MIN               201</span><br><span class="line">SYS_GID_MAX               999</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"></span></span><br><span class="line"><span class="bash"><span class="comment"># If defined, this command is run when removing a user.</span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> It should remove any at/cron/<span class="built_in">print</span> <span class="built_in">jobs</span> etc. owned by</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> the user to be removed (passed as the first argument).</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 设置此项后，在删除用户时，将自动删除用户拥有的at/cron/<span class="built_in">print</span>等job</span></span><br><span class="line"><span class="meta">#</span><span class="bash">USERDEL_CMD    /usr/sbin/userdel_local</span></span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"></span></span><br><span class="line"><span class="bash"><span class="comment"># If useradd should create home directories for users by default</span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> On RH systems, we <span class="keyword">do</span>. This option is overridden with the -m flag on</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> useradd <span class="built_in">command</span> line.</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 控制useradd添加用户时是否默认创建家目录，useradd -m选项会覆盖此处设置</span></span><br><span class="line">CREATE_HOME     yes</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> The permission mask is initialized to this value. If not specified,</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> the permission mask will be initialized to 022.</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 设置创建家目录时的<span class="built_in">umask</span>值，若不指定则默认为022</span></span><br><span class="line">UMASK           077</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> This enables userdel to remove user groups <span class="keyword">if</span> no members exist.</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 设置此项表示当组中没有成员时自动删除该组</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 且useradd是否同时创建同用户名的主组。</span></span><br><span class="line">USERGROUPS_ENAB yes</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> Use SHA512 to encrypt password.</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 设置用户和组密码的加密算法</span></span><br><span class="line">ENCRYPT_METHOD SHA512</span><br></pre></td></tr></table></figure><p>/etc/login.defs中的设置控制的是shadow-utils包中的组件，也就是说，该组件中的工具执行操作时会读取该文件中的配置。该组件中包含下面的程序：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">/usr/bin/gpasswd      ：administer /etc/group and /etc/gshadow</span><br><span class="line">/usr/bin/newgrp       ：log in to a new group，可用来修改gid，哪怕是正在登陆的会话也可以修改</span><br><span class="line">/usr/bin/sg           ：execute command as different group ID</span><br><span class="line">/usr/sbin/groupadd    ：添加组</span><br><span class="line">/usr/sbin/groupdel    ：删除组</span><br><span class="line">/usr/sbin/groupmems   ：管理当前用户的主组中的成员，root用户则可以指定要管理的组</span><br><span class="line">/usr/sbin/groupmod    ：modify a group definition on the system</span><br><span class="line">/usr/sbin/grpck       ：verify integrity of group files</span><br><span class="line">/usr/sbin/grpconv     ：无视它</span><br><span class="line">/usr/sbin/grpunconv   ：无视它</span><br><span class="line">/usr/sbin/pwconv      ：无视它</span><br><span class="line">/usr/sbin/pwunconv    ：无视它</span><br><span class="line">/usr/sbin/adduser     ：是useradd的一个软链接，添加用户</span><br><span class="line">/usr/sbin/chpasswd    ：update passwords in batch mode</span><br><span class="line">/usr/sbin/newusers    ：update and create new users in batch</span><br><span class="line">/usr/sbin/pwck        ：verify integrity of passsword files</span><br><span class="line">/usr/sbin/useradd     ：添加用户</span><br><span class="line">/usr/sbin/userdel     ：删除用户</span><br><span class="line">/usr/sbin/usermod     ：重定义用户信息</span><br><span class="line">/usr/sbin/vigr        ：edit the group and shadow-group file</span><br><span class="line">/usr/sbin/vipw        ：edit the password and shadow-password file</span><br><span class="line">/usr/bin/lastlog      ：输出所有用户或给定用户最近登录信息</span><br><span class="line"></span><br></pre></td></tr></table></figure><h2 id="创建用户时默认配置文件"><a href="#创建用户时默认配置文件" class="headerlink" title="创建用户时默认配置文件"></a>创建用户时默认配置文件</h2><p><code>/etc/default/useradd</code>。useradd -D修改的就是此文件。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">[root@slions_pc1 ~]# cat /etc/default/useradd</span><br><span class="line">[root@xuexi ~]# cat /etc/default/useradd  </span><br><span class="line"><span class="meta">#</span><span class="bash"> useradd defaults file</span></span><br><span class="line">GROUP=100       # 在useradd使用-N或/etc/login.defs中USERGROUPS_ENAB=no时表示创建</span><br><span class="line">                # 用户时不创建同用户名的主组(primary group)，此时新建的用户将默认以</span><br><span class="line">                # 此组为主组，网上关于该设置的很多说明都是错的，具体可看man useradd</span><br><span class="line">                # 的-g选项或useradd -D的-g选项</span><br><span class="line">HOME=/home      # 把用户的家目录建在/home中</span><br><span class="line">INACTIVE=-1     # 是否启用帐号过期设置(是帐号过期不是密码过期)，-1表示不启用</span><br><span class="line">EXPIRE=         # 帐号过期时间，不设置表示不启用</span><br><span class="line">SHELL=/bin/bash # 新建用户默认的shell类型</span><br><span class="line">SKEL=/etc/skel  # 指定框架目录，前文的/etc/skel就在这里</span><br><span class="line">CREATE_MAIL_SPOOL=yes  # 是否创建用户mail缓冲</span><br><span class="line"></span><br></pre></td></tr></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;基本概念&quot;&gt;&lt;a href=&quot;#基本概念&quot; class=&quot;headerlink&quot; title=&quot;基本概念&quot;&gt;&lt;/a&gt;基本概念&lt;/h1&gt;&lt;p&gt;Linux 继承了UNIX 对用户的优秀支持，其属于多用户的操作系统。在linux中用户与组都是一种身份认证资源。&lt;/p&gt;
</summary>
      
    
    
    
    
    <category term="linux" scheme="https://slions.github.io/tags/linux/"/>
    
  </entry>
  
  <entry>
    <title>harbor集群系统负载升高原因分析</title>
    <link href="https://slions.github.io/2021/07/19/harbor%E9%9B%86%E7%BE%A4%E7%B3%BB%E7%BB%9F%E8%B4%9F%E8%BD%BD%E5%8D%87%E9%AB%98%E5%8E%9F%E5%9B%A0%E5%88%86%E6%9E%90/"/>
    <id>https://slions.github.io/2021/07/19/harbor%E9%9B%86%E7%BE%A4%E7%B3%BB%E7%BB%9F%E8%B4%9F%E8%BD%BD%E5%8D%87%E9%AB%98%E5%8E%9F%E5%9B%A0%E5%88%86%E6%9E%90/</id>
    <published>2021-07-19T10:07:15.000Z</published>
    <updated>2021-07-19T11:00:44.903Z</updated>
    
    <content type="html"><![CDATA[<h1 id="问题现象"><a href="#问题现象" class="headerlink" title="问题现象"></a>问题现象</h1><p>接到客户反馈，说是创建不了新的服务了，查看相关的Event日志，发现此服务拉取不了images，后台查看harbor节点的状态，发现问题，harbor集群的VIP丢失，并且在主机上执行命令都比较卡，top查看平均负载发现非常高。</p><p><img src="/doc_picture/harbor-1.png" alt="image-20210719181131471"></p><h1 id="问题分析"><a href="#问题分析" class="headerlink" title="问题分析"></a>问题分析</h1><p>首先我们需要排查什么原因导致harbor节点这个卡顿，负载一直升高。</p><blockquote><p>平均负载是指单位时间内，系统处于可运行状态和不可中断状态的平均进程数，也就是平均活跃进程数。所以，它不仅包括了正在使用CPU的进程，还包括等待CPU和等待I/O的进程。</p></blockquote><p>系统平均负载升高的原因主要有三种：</p><ul><li>CPU密集型进程，使用大量CPU会导致平均负载升高，此时CPU使用率跟平均负载是一致的；</li><li>I/O密集型进程，等待I/O也会导致平均负载升高，但CPU使用率不一定很高；</li><li>大量等待CPU的进程调度也会导致平均负载升高，此时CPU使用率也会比较高。</li></ul><p>首先sar -u 观察CPU情况。</p><p><img src="/doc_picture/harbor-2.png" alt="image-20210719182405728"></p><p>cpu使用率非常低，大部分为idle，说明没有进程在等待cpu资源。</p><p>sar -b 观察IO情况 IO设备的读写tps都几乎为0。</p><p><img src="/doc_picture/harbor-3.png" alt="image-20210719182429006"></p><p>发现并不存在CPU/IO密集型的进程后，执行了下df操作，发现命令hang死，另外开一个终端，通过strace去分析df命令的系统调用及信号情况，可以明显发现df是在系统调用尝试获取目录/harborimages的stat信息时挂起。</p><p><img src="/doc_picture/harbor-4.png" alt="image-20210719182846933"></p><p>通过ps aux抓取系统运行的df进程信息（状态为D+(无法中断的休眠状态)）：</p><p><img src="/doc_picture/harbor-5.png" alt="image-20210719182903747"></p><p>通过ps aux查看内存和cpu占用最多的5个进程发现了问题，没有cpu占用特别大的进程，但是有一个状态为Dl的进程（无法中断的休眠状态/多线程，克隆线程）</p><p><img src="/doc_picture/harbor-6.png" alt="image-20210719182922163"></p><p>我们再查看下此进程的线程状态：</p><p><img src="/doc_picture/harbor-7.png" alt="image-20210719182944057"></p><p>发现有一堆不可中断的线程，而且越来越多，cpu使用率都为0，此时可以定位问题了，有大量进程读写请求一直获取不到资源，从而进程一直是不可中断状态。造成负载很高。平均负载升高导致机器性能降低，内部的keepalived服务心跳机制超时，最后使得VIP丢失。</p><p><img src="/doc_picture/harbor-8.png" alt="image-20210719184236890"></p><p>registry是harbor中负责存储镜像文件的组件，同时负责处理镜像的pull/push命令。此服务会将宿主机的/harborimages作为挂载目录，为了保障可用性我们后端使用了客户现成的nas来挂载到了/harborimages目录，之前执行df hang住的地方正是这个目录。 又在本地创了个测试目录，使用之前的nas地址挂载发现异常。此时回想起来，前一天客户发通知说要进行xxx区机器网络进行调整，大概率是由于这个引起了。联系客户方运维协助排查，最后发现nas服务器因为重启过导致防火墙开启了。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;问题现象&quot;&gt;&lt;a href=&quot;#问题现象&quot; class=&quot;headerlink&quot; title=&quot;问题现象&quot;&gt;&lt;/a&gt;问题现象&lt;/h1&gt;&lt;p&gt;接到客户反馈，说是创建不了新的服务了，查看相关的Event日志，发现此服务拉取不了images，后台查看harbor节点的状</summary>
      
    
    
    
    
    <category term="harbor" scheme="https://slions.github.io/tags/harbor/"/>
    
  </entry>
  
  <entry>
    <title>glusterfs可用性测试</title>
    <link href="https://slions.github.io/2021/07/18/glusterfs%E5%8F%AF%E7%94%A8%E6%80%A7%E6%B5%8B%E8%AF%95/"/>
    <id>https://slions.github.io/2021/07/18/glusterfs%E5%8F%AF%E7%94%A8%E6%80%A7%E6%B5%8B%E8%AF%95/</id>
    <published>2021-07-18T10:58:16.000Z</published>
    <updated>2021-07-18T11:44:07.694Z</updated>
    
    <content type="html"><![CDATA[<h1 id="环境描述"><a href="#环境描述" class="headerlink" title="环境描述"></a>环境描述</h1><table><thead><tr><th><strong>主机名</strong></th><th>ip地址</th><th><strong>系统环境</strong></th><th><strong>角色</strong></th></tr></thead><tbody><tr><td>k8s-1</td><td>IP:192.168.186.10</td><td>cpu:x2  mem:2GB  disk:/dev/sdb 40GB</td><td>K8s_master,Gluster_master,Heketi_master</td></tr><tr><td>K8s-2</td><td>IP:192.168.186.11</td><td>cpu:x2  mem:2GB  disk:/dev/sdb 40GB</td><td>K8s_node,Gluster_node</td></tr><tr><td>K8s-3</td><td>IP:192.168.186.12</td><td>cpu:x2  mem:2GB  disk:/dev/sdb 40GB</td><td>K8s_master,Gluster_node</td></tr></tbody></table><h1 id="集群扩容"><a href="#集群扩容" class="headerlink" title="集群扩容"></a>集群扩容</h1><h2 id="添加新磁盘"><a href="#添加新磁盘" class="headerlink" title="添加新磁盘"></a>添加新磁盘</h2><blockquote><p>添加设备时，请记住将设备添加为一组。例如，如果创建的卷使用副本为2，则应将device添加到两个节点（每个节点一个device）。如果使用副本3，则将device添加到三个节点。</p></blockquote><h3 id="命令行方式"><a href="#命令行方式" class="headerlink" title="命令行方式"></a>命令行方式</h3><p>假设在k8s-3上增加磁盘，查看k8s-3部署的pod name及IP：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-1 ~]# kubectl get po -o wide -l glusterfs-node</span><br><span class="line">NAME             READY      STATUS    RESTARTS   AGE           IP            NODE</span><br><span class="line">glusterfs-5npwn   1/1       Running   0          20h       192.168.186.10   k8s-1</span><br><span class="line">glusterfs-8zfzq   1/1       Running   0          20h       192.168.186.11   k8s-2</span><br><span class="line">glusterfs-bd5dx   1/1       Running   0          20h       192.168.186.12   k8s-3</span><br></pre></td></tr></table></figure><p>在k8s-3上确认新添加的盘符：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">Disk /dev/sdc: 42.9 GB, 42949672960 bytes, 83886080 sectors</span><br><span class="line">Units = sectors of 1 * 512 = 512 bytes</span><br><span class="line">Sector size (logical/physical): 512 bytes / 512 bytes</span><br><span class="line">I/O size (minimum/optimal): 512 bytes / 512 bytes</span><br></pre></td></tr></table></figure><p>使用heketi-cli查看cluster ID和所有node ID：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-1 ~]# heketi-cli cluster list</span><br><span class="line">Clusters:</span><br><span class="line">Id:5dec5676c731498c2bdf996e110a3e5e [file][block]</span><br><span class="line">[root@k8s-1 ~]# heketi-cli cluster info 5dec5676c731498c2bdf996e110a3e5e</span><br><span class="line">Cluster id: 5dec5676c731498c2bdf996e110a3e5e</span><br><span class="line">Nodes:</span><br><span class="line">0f00835397868d3591f45432e432ba38</span><br><span class="line">d38819746cab7d567ba5f5f4fea45d91</span><br><span class="line">fb181b0cef571e9af7d84d2ecf534585</span><br><span class="line">Volumes:</span><br><span class="line">32146a51be9f980c14bc86c34f67ebd5</span><br><span class="line">56d636b452d31a9d4cb523d752ad0891</span><br><span class="line">828dc2dfaa00b7213e831b91c6213ae4</span><br><span class="line">b9c68075c6f20438b46db892d15ed45a</span><br><span class="line">Block: true</span><br><span class="line">File: true</span><br></pre></td></tr></table></figure><p>找到对应的k8s-3的node ID：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-1 ~]# heketi-cli node info 0f00835397868d3591f45432e432ba38</span><br><span class="line">Node Id: 0f00835397868d3591f45432e432ba38</span><br><span class="line">State: online</span><br><span class="line">Cluster Id: 5dec5676c731498c2bdf996e110a3e5e</span><br><span class="line">Zone: 1</span><br><span class="line">Management Hostname: k8s-node02</span><br><span class="line">Storage Hostname: 192.168.186.12</span><br><span class="line">Devices:</span><br><span class="line">Id:82af8e5f2fb2e1396f7c9e9f7698a178   Name:/dev/sdb            State:online    Size (GiB):39      Used (GiB):25      Free (GiB):14      Bricks:4</span><br></pre></td></tr></table></figure><p>添加磁盘至GFS集群的k8s-3：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-1 ~]# heketi-cli device add --name=/dev/sdc --node=0f00835397868d3591f45432e432ba38</span><br><span class="line">Device added successfully</span><br></pre></td></tr></table></figure><p>查看结果：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-1 ~]# heketi-cli node info 0f00835397868d3591f45432e432ba38</span><br><span class="line">Node Id: 0f00835397868d3591f45432e432ba38</span><br><span class="line">State: online</span><br><span class="line">Cluster Id: 5dec5676c731498c2bdf996e110a3e5e</span><br><span class="line">Zone: 1</span><br><span class="line">Management Hostname: k8s-3</span><br><span class="line">Storage Hostname: 192.168.186.12</span><br><span class="line">Devices:</span><br><span class="line">Id:5539e74bc2955e7c70b3a20e72c04615   Name:/dev/sdc            State:online    Size (GiB):39      Used (GiB):0       Free (GiB):39      Bricks:0       </span><br><span class="line">Id:82af8e5f2fb2e1396f7c9e9f7698a178   Name:/dev/sdb            State:online    Size (GiB):39      Used (GiB):25      Free (GiB):14      Bricks:4</span><br></pre></td></tr></table></figure><h3 id="拓扑文件方式"><a href="#拓扑文件方式" class="headerlink" title="拓扑文件方式"></a>拓扑文件方式</h3><p>当一次添加多个设备的一种更简单的方法是将新设备添加到用于设置群集的拓扑文件(topology.json)中的节点描述中。然后重新运行该命令以加载新拓扑。下面是我们向节点添加新的/dev/sdc磁盘的示例：</p><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br></pre></td><td class="code"><pre><span class="line">cat &gt;topology.json&lt;&lt;EOF</span><br><span class="line">&#123;</span><br><span class="line">  <span class="attr">&quot;clusters&quot;</span>: [</span><br><span class="line">    &#123;</span><br><span class="line">      <span class="attr">&quot;nodes&quot;</span>: [</span><br><span class="line">        &#123;</span><br><span class="line">          <span class="attr">&quot;node&quot;</span>: &#123;</span><br><span class="line">            <span class="attr">&quot;hostnames&quot;</span>: &#123;</span><br><span class="line">              <span class="attr">&quot;manage&quot;</span>: [</span><br><span class="line">                <span class="string">&quot;k8s-1&quot;</span></span><br><span class="line">              ],</span><br><span class="line">              <span class="attr">&quot;storage&quot;</span>: [</span><br><span class="line">                <span class="string">&quot;192.168.186.10&quot;</span></span><br><span class="line">              ]</span><br><span class="line">            &#125;,</span><br><span class="line">            <span class="attr">&quot;zone&quot;</span>: <span class="number">1</span></span><br><span class="line">          &#125;,</span><br><span class="line">          <span class="attr">&quot;devices&quot;</span>: [</span><br><span class="line">            <span class="string">&quot;/dev/sdb&quot;</span>,</span><br><span class="line">            <span class="string">&quot;/dev/sdc&quot;</span></span><br><span class="line">          ]</span><br><span class="line">        &#125;,</span><br><span class="line">        &#123;</span><br><span class="line">          <span class="attr">&quot;node&quot;</span>: &#123;</span><br><span class="line">            <span class="attr">&quot;hostnames&quot;</span>: &#123;</span><br><span class="line">              <span class="attr">&quot;manage&quot;</span>: [</span><br><span class="line">                <span class="string">&quot;k8s-2&quot;</span></span><br><span class="line">              ],</span><br><span class="line">              <span class="attr">&quot;storage&quot;</span>: [</span><br><span class="line">                <span class="string">&quot;192.168.186.11&quot;</span></span><br><span class="line">              ]</span><br><span class="line">            &#125;,</span><br><span class="line">            <span class="attr">&quot;zone&quot;</span>: <span class="number">1</span></span><br><span class="line">          &#125;,</span><br><span class="line">          <span class="attr">&quot;devices&quot;</span>: [</span><br><span class="line">            <span class="string">&quot;/dev/sdb&quot;</span>,</span><br><span class="line">            <span class="string">&quot;/dev/sdc&quot;</span></span><br><span class="line">          ]</span><br><span class="line">        &#125;,</span><br><span class="line">        &#123;</span><br><span class="line">          <span class="attr">&quot;node&quot;</span>: &#123;</span><br><span class="line">            <span class="attr">&quot;hostnames&quot;</span>: &#123;</span><br><span class="line">              <span class="attr">&quot;manage&quot;</span>: [</span><br><span class="line">                <span class="string">&quot;k8s-3&quot;</span></span><br><span class="line">              ],</span><br><span class="line">              <span class="attr">&quot;storage&quot;</span>: [</span><br><span class="line">                <span class="string">&quot;192.168.186.12&quot;</span></span><br><span class="line">              ]</span><br><span class="line">            &#125;,</span><br><span class="line">            <span class="attr">&quot;zone&quot;</span>: <span class="number">1</span></span><br><span class="line">          &#125;,</span><br><span class="line">          <span class="attr">&quot;devices&quot;</span>: [</span><br><span class="line">            <span class="string">&quot;/dev/sdb&quot;</span>,</span><br><span class="line">            <span class="string">&quot;/dev/sdc&quot;</span></span><br><span class="line">          ]                                                                                    </span><br><span class="line">        &#125;</span><br><span class="line">      ]</span><br><span class="line">    &#125;</span><br><span class="line">  ]</span><br><span class="line">&#125;</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure><p>heketi加载拓扑配置：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> heketi-cli topology load --json=topology.json</span></span><br><span class="line">   Creating cluster ... ID: 224a5a6555fa5c0c930691111c63e863</span><br><span class="line">     Allowing file volumes on cluster.</span><br><span class="line">     Allowing block volumes on cluster.</span><br><span class="line">     Creating node 192.168.186.10 ... ID: 7946b917b91a579c619ba51d9129aeb0</span><br><span class="line">Found device /dev/sdb</span><br><span class="line">Adding device /dev/sdc ... OK</span><br><span class="line">     Creating node 192.168.186.11 ... ID: 5d10e593e89c7c61f8712964387f959c</span><br><span class="line">Found device /dev/sdb</span><br><span class="line">            Adding device /dev/sdc ... OK</span><br><span class="line">     Creating node 192.168.186.12 ... ID: de620cb2c313a5461d5e0a6ae234c553</span><br><span class="line">Found device /dev/sdb</span><br><span class="line">            Adding device /dev/sdc ... OK</span><br></pre></td></tr></table></figure><h2 id="添加新节点"><a href="#添加新节点" class="headerlink" title="添加新节点"></a>添加新节点</h2><p>假设将k8s-4，IP为192.168.186.13的加入glusterfs集群，并将该节点的/dev/sdb,/dev/sdc加入到集群。</p><p>先给node加标签，之后会自动创建pod：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-1 kubernetes]# kubectl label node k8s-4 storagenode=glusterfs</span><br><span class="line">node/k8s-4 labeled</span><br><span class="line"></span><br><span class="line">[root@k8s-1 kubernetes]# kubectl  get pod -o wide -l glusterfs-node</span><br><span class="line">NAME        READY     STATUS        RESTARTS   AGE       IP          NODE</span><br><span class="line">glusterfs-5npwn   1/1     Running     0     21h       192.168.186.11        k8s-2</span><br><span class="line">glusterfs-8zfzq   1/1       Running      0    21h      192.168.186.10        k8s-1</span><br><span class="line">glusterfs-96w74   0/1  ContainerCreating   0   2m     192.168.186.13         k8s-4</span><br><span class="line">glusterfs-bd5dx   1/1     Running       0      21h       192.168.186.12     k8s-3</span><br></pre></td></tr></table></figure><p>进入任意节点的gfs服务容器执行peer probe，加入新节点：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-1 kubernetes]# kubectl exec -ti glusterfs-5npwn -- gluster peer probe 192.168.186.13</span><br><span class="line">peer probe: success.</span><br></pre></td></tr></table></figure><p>将新节点纳入heketi数据库统一管理：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-1 kubernetes]# heketi-cli cluster list</span><br><span class="line">Clusters:</span><br><span class="line">Id:5dec5676c731498c2bdf996e110a3e5e [file][block]</span><br><span class="line"></span><br><span class="line">[root@k8s-1 kubernetes]# heketi-cli node add --zone=1 --cluster=5dec5676c731498c2bdf996e110a3e5e --management-host-name=k8s-4 --storage-host-name=192.168.186.13</span><br><span class="line">Node information:</span><br><span class="line">Id: 150bc8c458a70310c6137e840619758c</span><br><span class="line">State: online</span><br><span class="line">Cluster Id: 5dec5676c731498c2bdf996e110a3e5e</span><br><span class="line">Zone: 1</span><br><span class="line">Management Hostname k8s-4</span><br><span class="line">Storage Hostname 192.168.186.13</span><br></pre></td></tr></table></figure><p>将新节点的磁盘加入到集群中，参考上面的两种方式之一即可。</p><h1 id="存储卷扩容"><a href="#存储卷扩容" class="headerlink" title="存储卷扩容"></a>存储卷扩容</h1><h2 id="扩容"><a href="#扩容" class="headerlink" title="扩容"></a>扩容</h2><p>扩容volume可使用命令（单位为G）：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> heketi-cli volume expand --volume=volumeID --expand-size=10</span></span><br></pre></td></tr></table></figure><h1 id="集群缩容"><a href="#集群缩容" class="headerlink" title="集群缩容"></a>集群缩容</h1><p>Heketi也支持降低存储容量。这可以通过删除device，节点和集群来实现。可以使用API或使用heketi-cli执行这些更改。</p><blockquote><p> heketi删除device的前提是device没有被使用（Used为0）</p></blockquote><p>以下是如何从Heketi删除没有device被使用的命令：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> heketi-cli topology info</span></span><br><span class="line">Cluster Id: 6fe4dcffb9e077007db17f737ed999fe </span><br><span class="line">Volumes:</span><br><span class="line"></span><br><span class="line">    Nodes:</span><br><span class="line">  </span><br><span class="line">        Node Id: 61d019bb0f717e04ecddfefa5555bc41</span><br><span class="line">        State: online</span><br><span class="line">        Cluster Id: 6fe4dcffb9e077007db17f737ed999fe</span><br><span class="line">        Zone: 1</span><br><span class="line">        Management Hostname: k8s-3</span><br><span class="line">        Storage Hostname: 192.168.186.12</span><br><span class="line">        Devices:</span><br><span class="line">                Id:e4805400ffa45d6da503da19b26baad6   Name:/dev/sdb            State:online    Size (GiB):40     Used (GiB):0       Free (GiB):40</span><br><span class="line">                        Bricks:</span><br><span class="line">                Id:ecc3c65e4d22abf3980deba4ae90238c   Name:/dev/sdc            State:online    Size (GiB):40     Used (GiB):0       Free (GiB):40</span><br><span class="line">                        Bricks:</span><br><span class="line">  </span><br><span class="line">        Node Id: e97d77d0191c26089376c78202ee2f20</span><br><span class="line">        State: online</span><br><span class="line">        Cluster Id: 6fe4dcffb9e077007db17f737ed999fe</span><br><span class="line">        Zone: 2</span><br><span class="line">        Management Hostname: k8s-4</span><br><span class="line">        Storage Hostname: 192.168.186.13</span><br><span class="line">        Devices:</span><br><span class="line">                Id:3dc3b3f0dfd749e8dc4ee98ed2cc4141   Name:/dev/sdb            State:online    Size (GiB):40     Used (GiB):0       Free (GiB):40</span><br><span class="line">                        Bricks:</span><br><span class="line">                Id:4122bdbbe28017944a44e42b06755b1c   Name:/dev/sdc            State:online    Size (GiB):40     Used (GiB):0       Free (GiB)40</span><br><span class="line">                        Bricks:</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> d=`heketi-cli topology info | grep Size | awk <span class="string">&#x27;&#123;print $1&#125;&#x27;</span> | cut -d: -f 2`</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> <span class="keyword">for</span> i <span class="keyword">in</span> <span class="variable">$d</span> ; <span class="keyword">do</span></span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash"> heketi-cli device delete <span class="variable">$i</span></span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash"> <span class="keyword">done</span></span></span><br><span class="line">Device e4805400ffa45d6da503da19b26baad6 deleted</span><br><span class="line">Device ecc3c65e4d22abf3980deba4ae90238c deleted</span><br><span class="line">Device 3dc3b3f0dfd749e8dc4ee98ed2cc4141 deleted</span><br><span class="line">Device 4122bdbbe28017944a44e42b06755b1c deleted</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> heketi-cli node delete <span class="variable">$node1</span></span></span><br><span class="line">Node 61d019bb0f717e04ecddfefa5555bc41 deleted</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> heketi-cli node delete <span class="variable">$node2</span></span></span><br><span class="line">Node e97d77d0191c26089376c78202ee2f20 deleted</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> heketi-cli cluster delete <span class="variable">$cluster</span></span></span><br><span class="line">Cluster 6fe4dcffb9e077007db17f737ed999fe deleted</span><br></pre></td></tr></table></figure><h1 id="可用性测试"><a href="#可用性测试" class="headerlink" title="可用性测试"></a>可用性测试</h1><h2 id="添加节点"><a href="#添加节点" class="headerlink" title="添加节点"></a>添加节点</h2><p>通过restful api添加一台glusterfs主机，可以正常使用，前提是在添加之前要在新节点安装好glusterfs和lvm，加载 dm_thin_pool 模块，开启相关端口，给新节点打glusterfs的tag(daemonset用)，集群内节点可以互相解析域名。</p><h2 id="关闭节点"><a href="#关闭节点" class="headerlink" title="关闭节点"></a>关闭节点</h2><p>测试高可用中的坑。</p><p>三个glusterfs节点，关闭一台，客户端可读可写。</p><p>三个glusterfs节点，关闭两台，客户端可读不可写。</p><p>关闭虚机后发现heketi中还是显示节点在线，bricks不会同步到新增的虚机上，尝试把那台关了的机器剔除，发现bricks同步到了新增的节点。具体操作如下：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">[root@redhat1 /]# heketi-cli -s http://redhat1:30080 node list</span><br><span class="line">Id:05ac57499e3fbc0f8ec5a3301fac92c7Cluster:43c28e6f4b4e05f58ebd3b6f158982cd</span><br><span class="line">Id:5e10a4c264a2fc13ecdf8d2482ba7281Cluster:43c28e6f4b4e05f58ebd3b6f158982cd</span><br><span class="line">Id:b1e3ba52f6e8c82e8b3e512798348876Cluster:43c28e6f4b4e05f58ebd3b6f158982cd</span><br><span class="line">Id:f1bd8cd0f52b2d91b13e79799a34c2edCluster:43c28e6f4b4e05f58ebd3b6f158982cd</span><br><span class="line">f1bd8cd0f52b2d91b13e79799a34c2ed为已关闭的节点</span><br><span class="line">5e10a4c264a2fc13ecdf8d2482ba7281为新添加的节点</span><br><span class="line"> </span><br><span class="line">[root@redhat1 /]# heketi-cli -s http://redhat1:30080 node disable f1bd8cd0f52b2d91b13e79799a34c2ed</span><br><span class="line">Node f1bd8cd0f52b2d91b13e79799a34c2ed is now offline</span><br><span class="line"></span><br><span class="line">[root@redhat1 /]#heketi-cli-s http://redhat1:30080 node remove f1bd8cd0f52b2d91b13e79799a34c2ed</span><br><span class="line">Node f1bd8cd0f52b2d91b13e79799a34c2ed is now removed</span><br><span class="line"></span><br><span class="line">[root@redhat1 /]#heketi-cli -s http://redhat1:30080 device delete ff257d2350f05f7f5ebaa2853e5815e8</span><br><span class="line">Error: Failed to delete device /dev/sdb with id ff257d2350f05f7f5ebaa2853e5815e8 on host redhat3: error dialing backend: dial tcp 192.168.186.12:10250: connect: no route to host</span><br><span class="line"></span><br><span class="line">[root@redhat1 /]# heketi-cli -s http://redhat1:30080 node delete f1bd8cd0f52b2d91b13e79799a34c2ed</span><br><span class="line">Error: Unable to delete node [f1bd8cd0f52b2d91b13e79799a34c2ed] because it contains devices</span><br></pre></td></tr></table></figure><p>报错是因为关机了连不上节点，此时再查看关闭的哪个节点上bricks已经没了，同步到了新增加的节点上。</p><blockquote><p> 在旧节点从群集中完全清除之前，新增的节点不能和原先关闭的节点共用同样的标识。</p></blockquote><h2 id="硬盘损坏"><a href="#硬盘损坏" class="headerlink" title="硬盘损坏"></a>硬盘损坏</h2><p>本地三台gfs节点，每台挂载一块裸盘，把其中一块盘给删除模拟磁盘损坏，此时heketi中还能看到device，并且正在使用，登录那台节点执行partprobe更新下磁盘后发现lv和vg没有了，pv会存在残留数据，重启此节点后残留数据消失，heketi端还显示device正在使用。</p><p>手动删除device报以下错误</p><p>Error: Failed to remove device, error: No Replacement was found for resource</p><p>此错误是因为存储设备当前没有达到副本数要求的三个。</p><p><strong>解决方案：</strong></p><p>添加device后删除原先的device即可。</p><h2 id="brick损坏"><a href="#brick损坏" class="headerlink" title="brick损坏"></a>brick损坏</h2><p>手动删除gfs节点的一个brick:</p><p><img src="/doc_picture/gfs-test-1.png" alt="image-20210718192918628"></p><p>查看当前volume的状态，此时brick显示离线:</p><p><img src="/doc_picture/gfs-test-2.png" alt="image-20210718192931819"></p><p><strong>解决方案：</strong></p><p>先从volume端删除此brick</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> gluster volume remove-brick &lt;volume-name&gt;  replica &lt;count&gt; force</span></span><br></pre></td></tr></table></figure><blockquote><p>replica  2参数，开始我们创建卷时复制数是3，现在变为2。</p></blockquote><p><img src="/doc_picture/gfs-test-3.jpg" alt="img"> </p><p>此时查看volume状态，brick已经删除，volume变为2副本</p><p><img src="/doc_picture/gfs-test-4.jpg" alt="img"> </p><p>重新添加回此brick:</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> gluster volume add-brick &lt;volume-name&gt;  replica &lt;count&gt; force</span></span><br></pre></td></tr></table></figure><p><img src="/doc_picture/gfs-test-5.jpg" alt="img"> </p><p>查看volume状态，brick已经添加回来了</p><p><img src="/doc_picture/gfs-test-6.jpg" alt="img"> </p><p>查看此brick中数据已恢复。</p><p>上述方法是模拟其中一个brick故障，如果此卷可以重启的话可以快速重启尝试恢复。</p><h2 id="创建大于剩余空间的卷"><a href="#创建大于剩余空间的卷" class="headerlink" title="创建大于剩余空间的卷"></a>创建大于剩余空间的卷</h2><p>创建不出来，报错</p><p><img src="/doc_picture/gfs-test-7.png" alt="image-20210718193341163"></p><h2 id="扩容volume"><a href="#扩容volume" class="headerlink" title="扩容volume"></a>扩容volume</h2><p>提前申请一个1g大小的pvc并且挂载到应用服务，写一些数据，然后将应用服务停掉，扩容pvc到2G大小，再把应用服务开启，测试写，因为扩容volume想当于是在原先子卷的情况下又加了一个子卷，但是就算是新加入的子卷有剩余空间，glusterfs的hash寻址机制也会一直读写老的子卷（找之前的文件hash值），尝试给卷做rebalance操作来触发数据均衡操作，（扩容后会自动进行rebalance），没有效果。</p><p>创建存储盘使应提前预估好使用量大小。</p><p><img src="/doc_picture/gfs-test-8.png" alt="image-20210718193421419"></p><p><img src="/doc_picture/gfs-test-9.png" alt="image-20210718193430193"></p><p><img src="/doc_picture/gfs-test-10.png" alt="image-20210718193443985"></p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;环境描述&quot;&gt;&lt;a href=&quot;#环境描述&quot; class=&quot;headerlink&quot; title=&quot;环境描述&quot;&gt;&lt;/a&gt;环境描述&lt;/h1&gt;&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;&lt;strong&gt;主机名&lt;/strong&gt;&lt;/th&gt;
&lt;th&gt;ip地址&lt;/th&gt;
</summary>
      
    
    
    
    
    <category term="glusterfs" scheme="https://slions.github.io/tags/glusterfs/"/>
    
    <category term="kubernetes storage" scheme="https://slions.github.io/tags/kubernetes-storage/"/>
    
  </entry>
  
  <entry>
    <title>glusterfs回收站功能</title>
    <link href="https://slions.github.io/2021/07/16/glusterfs%E5%9B%9E%E6%94%B6%E7%AB%99%E5%8A%9F%E8%83%BD/"/>
    <id>https://slions.github.io/2021/07/16/glusterfs%E5%9B%9E%E6%94%B6%E7%AB%99%E5%8A%9F%E8%83%BD/</id>
    <published>2021-07-15T16:24:50.000Z</published>
    <updated>2021-07-15T16:33:07.228Z</updated>
    
    <content type="html"><![CDATA[<h1 id="功能简述"><a href="#功能简述" class="headerlink" title="功能简述"></a>功能简述</h1><p>glusterfs有一个类似windows回收站的功能，可以帮助用户获取和恢复临时被删除的数据。每个块都会保留一个隐藏的目录.trash，它将会被用于存放被从各个块删除的文件。这个translator以后还会增强功能来支持被删除文件的恢复。</p><p>回收站的目录名应该是可配置的。trash translator也会被用于内部操作比如自卷的自修复以及再平衡。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> gluster volume <span class="built_in">set</span> &lt;VOLNAME&gt; features.trash &lt;on/off&gt;</span></span><br></pre></td></tr></table></figure><p>此命令可用于启用卷中的Trash translator,如果设置为on，则在卷启动命令期间，将在卷内的每个brick块中创建.trashcan目录。默认情况下，translator在卷启动期间加载，但仍然不起作用。在此选项的帮助下禁用垃圾桶将不会从卷中删除垃圾邮件目录或甚至其内容。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> gluster volume <span class="built_in">set</span> &lt;VOLNAME&gt; features.trash-dir &lt;name&gt;</span></span><br></pre></td></tr></table></figure><p>此命令用于将垃圾目录重新配置为用户指定的名称。参数是有效的目录名称。目录将在这个名字下面的每个brick内创建。如果用户没有指定，translator将创建默认名称为“.trashcan”的垃圾桶目录。只有当Trash translator开启时才可使用。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> gluster volume <span class="built_in">set</span> &lt;VOLNAME&gt; features.trash-max-filesize &lt;size&gt;</span></span><br></pre></td></tr></table></figure><p>此命令可用于根据大小过滤进入垃圾目录的文件。大小超过rash_max_filesize的文件将直接删除/截断。大小值后可以跟乘性后缀，例如KB（= 1024字节），MB（= 1024 * 1024字节）和GB（= 1024 * 1024 * 1024字节）。默认大小设置为5MB。考虑到垃圾目录占用了glusterfs卷空间这一事实，垃圾邮件功能的实现方式是，即使此选项设置为大于1GB的某个值，它也可以直接删除/截断大于1GB的文件。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> gluster volume <span class="built_in">set</span> &lt;VOLNAME&gt; features.trash-internal-op &lt;on/off&gt;</span></span><br></pre></td></tr></table></figure><p>此命令可用于为内部操作（例如自愈和重新平衡）启用垃圾桶。默认设置为关闭。</p><h1 id="简单测试"><a href="#简单测试" class="headerlink" title="简单测试"></a>简单测试</h1><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">[root@gluster1 /]# gluster volume info</span><br><span class="line"> </span><br><span class="line">Volume Name: gv1</span><br><span class="line">Type: Distribute</span><br><span class="line">Volume ID: 58bf037f-5b56-4cf6-8dab-9e9944800b61</span><br><span class="line">Status: Started</span><br><span class="line">Number of Bricks: 2</span><br><span class="line">Transport-type: tcp</span><br><span class="line">Bricks:</span><br><span class="line">Brick1: gluster1:/storage/brick1</span><br><span class="line">Brick2: mystorage2:/storage/brick1</span><br><span class="line">Options Reconfigured:</span><br><span class="line">features.trash: on</span><br><span class="line">performance.readdir-ahead: on</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>启用gv1卷中的Trash translator：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@gluster1 /]# gluster volume set gv1 features.trash on</span><br><span class="line">volume set: success</span><br></pre></td></tr></table></figure><p>进入到挂载目录进行删除操作:</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[root@gluster1 /]# cd /mnt/</span><br><span class="line">[root@gluster1 mnt]# ls</span><br><span class="line">aa  bb  cc  ddd</span><br><span class="line">[root@gluster1 mnt]# rm -rf cc</span><br></pre></td></tr></table></figure><p>查看目录发现有带时间戳的文件:</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">[root@gluster1 mnt]# ls -la</span><br><span class="line">total 12</span><br><span class="line">drwxr-xr-x   4 root root 4096 May 21 06:03 .</span><br><span class="line">dr-xr-xr-x. 23 root root 4096 May 20 17:23 ..</span><br><span class="line">-rwxr-xr-x   1 root root    0 May 20 00:36 aa</span><br><span class="line">-rwxr-xr-x   1 root root    0 May 20 00:36 bb</span><br><span class="line">-rwxr-xr-x   1 root root    0 May 20 00:36 ddd</span><br><span class="line">drwsr-sr-x   3 root root 4096 May 20 23:22 .trashcan</span><br><span class="line">[root@gluster1 mnt]# cd .trashcan/</span><br><span class="line">[root@gluster1 .trashcan]# ls</span><br><span class="line">cc_2019-05-20_151208  internal_op</span><br></pre></td></tr></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;功能简述&quot;&gt;&lt;a href=&quot;#功能简述&quot; class=&quot;headerlink&quot; title=&quot;功能简述&quot;&gt;&lt;/a&gt;功能简述&lt;/h1&gt;&lt;p&gt;glusterfs有一个类似windows回收站的功能，可以帮助用户获取和恢复临时被删除的数据。每个块都会保留一个隐藏的目录</summary>
      
    
    
    
    
    <category term="glusterfs" scheme="https://slions.github.io/tags/glusterfs/"/>
    
  </entry>
  
  <entry>
    <title>k8s-v1.11使用glusterfs</title>
    <link href="https://slions.github.io/2021/07/15/k8s-v1.11%E4%BD%BF%E7%94%A8glusterfs/"/>
    <id>https://slions.github.io/2021/07/15/k8s-v1.11%E4%BD%BF%E7%94%A8glusterfs/</id>
    <published>2021-07-15T15:18:09.000Z</published>
    <updated>2021-08-02T04:08:07.665Z</updated>
    
    <content type="html"><![CDATA[<p>Glusterfs提供底层存储功能，heketi为glusterfs提供restful风格的api，Heketi要求在每个glusterfs节点上配备<strong>裸磁盘</strong>，目前heketi仅支持使用裸磁盘(未格式化)添加为device，不支持文件系统，因为Heketi要用来创建PV和VG方便管理glusterfs。</p><p>集群托管于heketi后，不能使用命令管理存储卷，以免与Heketi数据库中存储的信息不一致。</p><blockquote><p>glusterfs支持k8s的pv的3种访问模式ReadWriteOnce，ReadOnlyMany ，ReadWriteMany。访问模式只是能力描述，并不是强制执行的，对于没有按pvc声明的方式使用pv，存储提供者应该负责访问时的运行错误。例如：如果设置pvc的访问模式为ReadOnlyMany ，pod挂载后依然可写，如果需要真正的不可写，申请pvc是需要指定 readOnly: true 参数。</p></blockquote><h1 id="环境描述"><a href="#环境描述" class="headerlink" title="环境描述"></a>环境描述</h1><table><thead><tr><th><strong>主机名</strong></th><th>ip地址</th><th><strong>系统环境</strong></th><th><strong>角色</strong></th></tr></thead><tbody><tr><td>k8s-1</td><td>192.168.186.10</td><td>cpu:x2  mem:2GB  disk:/dev/sdb 10GB</td><td>K8s_master,Gluster_master,Heketi_master</td></tr><tr><td>K8s-2</td><td>192.168.186.11</td><td>cpu:x2  mem:2GB  disk:/dev/sdb 10GB</td><td>K8s_node,Gluster_node</td></tr><tr><td>K8s-3</td><td>192.168.186.12</td><td>cpu:x2  mem:2GB  disk:/dev/sdb 10GB</td><td>K8s_node,Gluster_node</td></tr></tbody></table><p>如果存在iptable限制，需执行以下命令开通以下port</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">iptables -N heketi</span><br><span class="line">iptables -A heketi -p tcp -m state --state NEW -m tcp --dport 24007 -j ACCEPT</span><br><span class="line">iptables -A heketi -p tcp -m state --state NEW -m tcp --dport 24008 -j ACCEPT</span><br><span class="line">iptables -A heketi -p tcp -m state --state NEW -m tcp --dport 2222 -j ACCEPT</span><br><span class="line">iptables -A heketi -p tcp -m state --state NEW -m multiport --dports 49152:49251 -j ACCEPT</span><br><span class="line">service iptables save</span><br></pre></td></tr></table></figure><h1 id="安装步骤"><a href="#安装步骤" class="headerlink" title="安装步骤"></a>安装步骤</h1><p>下面的测试是采用容器化方式部署GFS，GFS以Daemonset的方式进行部署，保证每台需要部署GFS管理服务的Node上都运行一个GFS管理服务。</p><h2 id="三台节点执行："><a href="#三台节点执行：" class="headerlink" title="三台节点执行："></a>三台节点执行：</h2><p>要求所有node节点存在主机的解析记录，务必配置好/etc/hosts</p><p><img src="/doc_picture/heketi-1.png" alt="image-20210714161502089"></p><p>安装 glusterfs 每节点需要提前加载 <code>dm_thin_pool</code> 模块：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> modprobe dm_thin_pool</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> modprobe dm_snapshot</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> modprobe dm_mirror</span></span><br></pre></td></tr></table></figure><p>配置开启自加载：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> cat &gt;/etc/modules-load.d/glusterfs.conf&lt;&lt;<span class="string">EOF</span></span></span><br><span class="line">dm_thin_pool</span><br><span class="line">dm_snapshot</span><br><span class="line">dm_mirror</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure><p>安装 glusterfs-fuse：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> yum install -y glusterfs-fuse lvm2</span></span><br></pre></td></tr></table></figure><h2 id="第一台节点执行："><a href="#第一台节点执行：" class="headerlink" title="第一台节点执行："></a>第一台节点执行：</h2><h3 id="安装glusterfs与heketi"><a href="#安装glusterfs与heketi" class="headerlink" title="安装glusterfs与heketi"></a>安装glusterfs与heketi</h3><p>安装 heketi client</p><p><a href="https://github.com/heketi/heketi/releases">https://github.com/heketi/heketi/releases</a></p><p>去github下载相关的版本：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-1 ~]# wget https://github.com/heketi/heketi/releases/download/v8.0.0/heketi-client-v9.0.0.linux.amd64.tar.gz</span><br><span class="line">[root@k8s-1 ~]# tar xf heketi-client-v9.0.0.linux.amd64.tar.gz</span><br><span class="line">[root@k8s-1 ~]# cp heketi-client/bin/heketi-cli /usr/local/bin</span><br></pre></td></tr></table></figure><p> 查看版本：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-1 ~]# heketi-cli -v</span><br></pre></td></tr></table></figure><p>之后部署步骤都在如下目录执行：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-1 ~]#cd heketi-client/share/heketi/kubernetes</span><br></pre></td></tr></table></figure><p>在k8s中部署 glusterfs：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-1 kubernetes]# kubectl create -f glusterfs-daemonset.json</span><br></pre></td></tr></table></figure><blockquote><ol><li>此时采用的为默认的挂载方式，可使用其他磁盘当做GFS的工作目录</li><li>此时创建的namespace为默认的default，按需更改</li></ol></blockquote><p>给提供存储 node 节点打 label:</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-1 kubernetes]# kubectl label node k8s-1 k8s-2 k8s-3 storagenode=glusterfs</span><br></pre></td></tr></table></figure><p>查看 glusterfs 状态:</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-1 kubernetes]# kubectl get pods -o wide</span><br></pre></td></tr></table></figure><p>部署 heketi server #配置 heketi server 的权限:</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-1 kubernetes]# kubectl create -f heketi-service-account.json</span><br><span class="line">[root@k8s-1 kubernetes]# kubectl create clusterrolebinding heketi-gluster-admin --clusterrole=edit --serviceaccount=default:heketi-service-account</span><br></pre></td></tr></table></figure><p> 创建 cofig secret:</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-1 kubernetes]# kubectl create secret generic heketi-config-secret --from-file=./heketi.json</span><br></pre></td></tr></table></figure><p> 初始化部署:</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-1 kubernetes]# kubectl create -f heketi-bootstrap.json</span><br></pre></td></tr></table></figure><p># 查看 heketi bootstrap 状态</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-1 kubernetes]# kubectl get pods -o wide</span><br><span class="line"></span><br><span class="line">[root@k8s-1 kubernetes]# kubectl get svc</span><br></pre></td></tr></table></figure><p># 配置端口转发 heketi server</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-1 kubernetes]# HEKETI_BOOTSTRAP_POD=$(kubectl get pods | grep deploy-heketi | awk &#x27;&#123;print $1&#125;&#x27;)</span><br><span class="line"></span><br><span class="line">[root@k8s-1 kubernetes]# kubectl port-forward $HEKETI_BOOTSTRAP_POD 58080:8080</span><br></pre></td></tr></table></figure><p># 测试访问,另起一终端</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-1 ~]#curl http://localhost:58080/hello</span><br></pre></td></tr></table></figure><h3 id="配置-glusterfs"><a href="#配置-glusterfs" class="headerlink" title="配置 glusterfs"></a>配置 glusterfs</h3><blockquote><ol><li>hostnames/manage 字段里必须和 kubectl get node 一致</li><li>hostnames/storage 指定存储网络 ip 本次实验使用与k8s集群同一个ip</li></ol></blockquote><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s<span class="number">-1</span> kubernetes]# cat &gt;topology.json&lt;&lt;EOF</span><br><span class="line">&#123;</span><br><span class="line">  <span class="attr">&quot;clusters&quot;</span>: [</span><br><span class="line">    &#123;</span><br><span class="line">      <span class="attr">&quot;nodes&quot;</span>: [</span><br><span class="line">        &#123;</span><br><span class="line">          <span class="attr">&quot;node&quot;</span>: &#123;</span><br><span class="line">            <span class="attr">&quot;hostnames&quot;</span>: &#123;</span><br><span class="line">              <span class="attr">&quot;manage&quot;</span>: [</span><br><span class="line">                <span class="string">&quot;k8s-1&quot;</span></span><br><span class="line">              ],</span><br><span class="line">              <span class="attr">&quot;storage&quot;</span>: [</span><br><span class="line">                <span class="string">&quot;192.168.186.10&quot;</span></span><br><span class="line">              ]</span><br><span class="line">            &#125;,</span><br><span class="line">            <span class="attr">&quot;zone&quot;</span>: <span class="number">1</span></span><br><span class="line">          &#125;,</span><br><span class="line">          <span class="attr">&quot;devices&quot;</span>: [</span><br><span class="line">            <span class="string">&quot;/dev/sdb&quot;</span></span><br><span class="line">          ]</span><br><span class="line">        &#125;,</span><br><span class="line">        &#123;</span><br><span class="line">          <span class="attr">&quot;node&quot;</span>: &#123;</span><br><span class="line">            <span class="attr">&quot;hostnames&quot;</span>: &#123;</span><br><span class="line">              <span class="attr">&quot;manage&quot;</span>: [</span><br><span class="line">                <span class="string">&quot;k8s-2&quot;</span></span><br><span class="line">              ],</span><br><span class="line">              <span class="attr">&quot;storage&quot;</span>: [</span><br><span class="line">                <span class="string">&quot;192.168.186.11&quot;</span></span><br><span class="line">              ]</span><br><span class="line">            &#125;,</span><br><span class="line">            <span class="attr">&quot;zone&quot;</span>: <span class="number">1</span></span><br><span class="line">          &#125;,</span><br><span class="line">          <span class="attr">&quot;devices&quot;</span>: [</span><br><span class="line">            <span class="string">&quot;/dev/sdb&quot;</span></span><br><span class="line">          ]</span><br><span class="line">        &#125;,</span><br><span class="line">        &#123;</span><br><span class="line">          <span class="attr">&quot;node&quot;</span>: &#123;</span><br><span class="line">            <span class="attr">&quot;hostnames&quot;</span>: &#123;</span><br><span class="line">              <span class="attr">&quot;manage&quot;</span>: [</span><br><span class="line">                <span class="string">&quot;k8s-3&quot;</span></span><br><span class="line">              ],</span><br><span class="line">              <span class="attr">&quot;storage&quot;</span>: [</span><br><span class="line">                <span class="string">&quot;192.168.186.12&quot;</span></span><br><span class="line">              ]</span><br><span class="line">            &#125;,</span><br><span class="line">            <span class="attr">&quot;zone&quot;</span>: <span class="number">1</span></span><br><span class="line">          &#125;,</span><br><span class="line">          <span class="attr">&quot;devices&quot;</span>: [</span><br><span class="line">            <span class="string">&quot;/dev/sdb&quot;</span></span><br><span class="line">          ]                                                                                       </span><br><span class="line">        &#125;</span><br><span class="line">      ]</span><br><span class="line">    &#125;</span><br><span class="line">  ]</span><br><span class="line">&#125;</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure><p>heketi加载配置:</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-1 kubernetes]# export HEKETI_CLI_SERVER=http://localhost:58080</span><br><span class="line"></span><br><span class="line">[root@k8s-1 kubernetes]# heketi-cli topology load --json=topology.json</span><br></pre></td></tr></table></figure><p>使用 Heketi 创建一个用于存储 Heketi 数据库的 volume：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-1 kubernetes]# heketi-cli setup-openshift-heketi-storage</span><br><span class="line">[root@k8s-1 kubernetes]# kubectl create -f heketi-storage.json</span><br></pre></td></tr></table></figure><blockquote><p>heketi-storage.json中：</p><p>创建了heketi-storage-endpoints，（指明了gfs地址和端口，默认端口为1）创建了heketi-storage-copy-job，此job的作用就是复制heketi中的数据文件到 /heketi，而/heketi目录挂载在了卷heketi-storage中，而heketi-storage volume是前面执行”heketi-cli setup-openshift-heketi-storage”时创建好了的。</p></blockquote><p>查看状态,等所有job完成 即状态为 <code>Completed</code>,才能进行如下的步骤：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-1 kubernetes]# kubectl get pods</span><br><span class="line">[root@k8s-1 kubernetes]# kubectl get job</span><br></pre></td></tr></table></figure><p> 删除部署时产生的相关资源：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-1 kubernetes]# kubectl delete all,service,jobs,deployment,secret --selector=&quot;deploy-heketi&quot;</span><br></pre></td></tr></table></figure><p># 部署 heketi server</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-1 kubernetes]# kubectl create -f heketi-deployment.json</span><br></pre></td></tr></table></figure><p># 查看 heketi server 状态</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-1 kubernetes]# kubectl get pods -o wide</span><br><span class="line">[root@k8s-1 kubernetes]# kubectl get svc</span><br></pre></td></tr></table></figure><p># 查看 heketi 状态信息, 配置端口转发 heketi server</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-1 kubernetes]# HEKETI_BOOTSTRAP_POD=$(kubectl get pods | grep heketi | awk &#x27;&#123;print $1&#125;&#x27;)</span><br><span class="line">[root@k8s-1 kubernetes]# kubectl port-forward $HEKETI_BOOTSTRAP_POD 58080:8080</span><br><span class="line">[root@k8s-1 kubernetes]# export HEKETI_CLI_SERVER=http://localhost:58080</span><br><span class="line">[root@k8s-1 kubernetes]# heketi-cli cluster list</span><br><span class="line">[root@k8s-1 kubernetes]# heketi-cli volume list</span><br></pre></td></tr></table></figure><blockquote><p>可以把heketi的service type换成NodePrort,并给glusterfs的daemonset添加spec. template.spec.hostNetwork: true,之后就不用以端口转发映射本地端口的方式访问heketi，直接heketi-cli -s <a href="srv:port">srv:port</a> 即可</p></blockquote><h3 id="创建-StorageClass"><a href="#创建-StorageClass" class="headerlink" title="创建 StorageClass"></a>创建 StorageClass</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-1 kubernetes]# HEKETI_SERVER=$(kubectl get svc | grep heketi | head -1 | awk &#x27;&#123;print $3&#125;&#x27;)</span><br><span class="line">[root@k8s-1 kubernetes]# echo $HEKETI_SERVER</span><br><span class="line">[root@k8s-1 kubernetes]# cat &gt;storageclass-glusterfs.yaml&lt;&lt;EOF</span><br><span class="line">kind: StorageClass</span><br><span class="line">apiVersion: storage.k8s.io/v1</span><br><span class="line">metadata:</span><br><span class="line">  name: gluster-heketi</span><br><span class="line">provisioner: kubernetes.io/glusterfs</span><br><span class="line"><span class="meta">#</span><span class="bash">reclaimPolicy: Retain</span></span><br><span class="line">parameters:</span><br><span class="line">  resturl: &quot;http://$HEKETI_SERVER:8080&quot;</span><br><span class="line">  gidMin: &quot;40000&quot;</span><br><span class="line">  gidMax: &quot;50000&quot;</span><br><span class="line">  volumetype: &quot;replicate:3&quot;</span><br><span class="line"><span class="meta">#</span><span class="bash">允许对pvc扩容</span></span><br><span class="line">allowVolumeExpansion: true</span><br><span class="line">EOF</span><br><span class="line">[root@k8s-1 kubernetes]# kubectl create -f storageclass-glusterfs.yaml</span><br></pre></td></tr></table></figure><blockquote><ol><li>以上创建了一个含有三个副本的gluster的存储类型（storage-class） </li><li>volumetype中的relicate必须大于1，否则创建pvc的时候会报错</li><li>在这里创建的storageclass显示指定reclaimPolicy为Retain(默认情况下是Delete)，删除pvc后pv以及后端的volume、brick(lvm)不会被删除。</li></ol></blockquote><h3 id="创建pvc"><a href="#创建pvc" class="headerlink" title="创建pvc"></a>创建pvc</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-1 kubernetes]# cat &gt;gluster-pvc-test.yaml&lt;&lt;EOF</span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: PersistentVolumeClaim</span><br><span class="line">metadata:</span><br><span class="line"> name: gluster1</span><br><span class="line"> annotations:</span><br><span class="line">   volume.beta.kubernetes.io/storage-class: gluster-heketi</span><br><span class="line">spec:</span><br><span class="line"> accessModes:</span><br><span class="line">  - ReadWriteMany</span><br><span class="line"> resources:</span><br><span class="line">   requests:</span><br><span class="line">     storage: 1Gi</span><br><span class="line">EOF</span><br><span class="line">[root@k8s-1 kubernetes]# kubectl apply -f gluster-pvc-test.yaml</span><br></pre></td></tr></table></figure><p>查看卷状态</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-1 kubernetes]# kubectl get pvc</span><br><span class="line">[root@k8s-1 kubernetes]# kubectl get pv</span><br></pre></td></tr></table></figure><h3 id="创建服务挂载测试"><a href="#创建服务挂载测试" class="headerlink" title="创建服务挂载测试"></a>创建服务挂载测试</h3><figure class="highlight yml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line">[<span class="string">root@k8s-1</span> <span class="string">kubernetes</span>]<span class="comment"># cat &gt;nginx-pod.yaml&lt;&lt;EOF</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">extensions/v1beta1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Deployment</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">nginx-gfs</span></span><br><span class="line">  <span class="attr">labels:</span></span><br><span class="line">    <span class="attr">name:</span> <span class="string">nginx-gfs</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">replicas:</span> <span class="number">2</span></span><br><span class="line">  <span class="attr">selector:</span></span><br><span class="line">    <span class="attr">matchLabels:</span></span><br><span class="line">      <span class="attr">name:</span> <span class="string">nginx-gfs</span></span><br><span class="line">  <span class="attr">template:</span></span><br><span class="line">    <span class="attr">metadata:</span></span><br><span class="line">      <span class="attr">labels:</span></span><br><span class="line">        <span class="attr">name:</span> <span class="string">nginx-gfs</span></span><br><span class="line">    <span class="attr">spec:</span></span><br><span class="line">      <span class="attr">containers:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">nginx-gfs</span></span><br><span class="line">        <span class="attr">image:</span> <span class="string">nginx</span></span><br><span class="line">        <span class="attr">ports:</span></span><br><span class="line">        <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">web</span></span><br><span class="line">          <span class="attr">containerPort:</span> <span class="number">80</span></span><br><span class="line">        <span class="attr">volumeMounts:</span></span><br><span class="line">        <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">gfs</span></span><br><span class="line">          <span class="attr">mountPath:</span> <span class="string">/usr/share/nginx/html</span></span><br><span class="line">      <span class="attr">volumes:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">gfs</span></span><br><span class="line">        <span class="attr">persistentVolumeClaim:</span></span><br><span class="line">          <span class="attr">claimName:</span> <span class="string">gluster1</span></span><br><span class="line"><span class="string">EOF</span></span><br><span class="line">[<span class="string">root@k8s-1</span> <span class="string">kubernetes</span>]<span class="comment"># kubectl apply -f nginx-pod.yaml</span></span><br></pre></td></tr></table></figure><p>查看服务是否正常启动：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-1 kubernetes]# kubectl get pods -o wide</span><br></pre></td></tr></table></figure><h3 id="测试pvc的扩容"><a href="#测试pvc的扩容" class="headerlink" title="测试pvc的扩容"></a>测试pvc的扩容</h3><p>修改pvc/gluster1容量1G改为2G，过一会儿会自动生效，此时查看pv,pvc,和进入容器都已经成了2G（自己机器上测试发现生效时长大概为1min），把容器停掉继续扩容发现也是ok的。</p><h1 id="分析篇"><a href="#分析篇" class="headerlink" title="分析篇"></a>分析篇</h1><h2 id="heketi是怎么对磁盘进行操作的"><a href="#heketi是怎么对磁盘进行操作的" class="headerlink" title="heketi是怎么对磁盘进行操作的"></a>heketi是怎么对磁盘进行操作的</h2><p>回过头来分析下heketi加载gfs配置时进行了什么操作</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">$ heketi-cli topology load --json=topology-sample.json  </span><br><span class="line">   Creating cluster ... ID: 224a5a6555fa5c0c930691111c63e863</span><br><span class="line">     Allowing file volumes on cluster.</span><br><span class="line">     Allowing block volumes on cluster.</span><br><span class="line">     Creating node 192.168.186.10 ... ID: 7946b917b91a579c619ba51d9129aeb0</span><br><span class="line">            Adding device /dev/sdb ... OK</span><br><span class="line">     Creating node 192.168.186.11 ... ID: 5d10e593e89c7c61f8712964387f959c</span><br><span class="line">            Adding device /dev/sdb ... OK</span><br><span class="line">     Creating node 192.168.186.12 ... ID: de620cb2c313a5461d5e0a6ae234c553</span><br><span class="line">            Adding device /dev/sdb ... OK</span><br></pre></td></tr></table></figure><ul><li>进入任意glusterfs Pod内，执行gluster peer status 发现都已把对端加入到了可信存储池(TSP)中。</li><li>在运行了gluster Pod的节点上，自动创建了一个VG，此VG正是由topology-sample.json 文件中的磁盘裸设备创建而来。</li><li>一块磁盘设备创建出一个VG，以后创建的PVC，即从此VG里划分的LV。</li><li>heketi-cli topology info 查看拓扑结构，显示出每个磁盘设备的ID，对应VG的ID，总空间、已用空间、空余空间等信息。</li></ul><h2 id="heketi创建db-volume的流程"><a href="#heketi创建db-volume的流程" class="headerlink" title="heketi创建db volume的流程"></a>heketi创建db volume的流程</h2><p>执行heketi-cli setup-openshift-heketi-storage并观测heketi后台做了什么，可以通过相应日志查看：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br></pre></td><td class="code"><pre><span class="line">[negroni] 2020-03-06T16:45:13Z | 200 |   61.841µs | 192.168.186.10:30080 | GET /clusters</span><br><span class="line">[negroni] 2020-03-06T16:45:13Z | 200 |   159.901µs | 192.168.186.10:30080 | GET /clusters/6749ed08e37290fbb1cc4c881872054d</span><br><span class="line">[heketi] INFO 2020/03/06 16:45:13 Allocating brick set #0</span><br><span class="line">[negroni] 2020-03-06T16:45:13Z | 202 |   46.293298ms | 192.168.186.10:30080 | POST /volumes</span><br><span class="line">[asynchttp] INFO 2020/03/06 16:45:13 asynchttp.go:288: Started job 5ffdc4ab574897e19511ae43afa7e78c</span><br><span class="line">[heketi] INFO 2020/03/06 16:45:13 Started async operation: Create Volume</span><br><span class="line">[heketi] INFO 2020/03/06 16:45:13 Trying Create Volume (attempt #1/5)</span><br><span class="line">[heketi] INFO 2020/03/06 16:45:13 Creating brick b6411ccff63daf1270bc9f354ca484dd</span><br><span class="line">[heketi] INFO 2020/03/06 16:45:13 Creating brick 98700f7b0bce70eb29279fb275763704</span><br><span class="line">[heketi] INFO 2020/03/06 16:45:13 Creating brick 777c447835963ef4db7cbb2392c85e59</span><br><span class="line">[negroni] 2020-03-06T16:45:13Z | 200 |   41.375µs | 192.168.186.10:30080 | GET /queue/5ffdc4ab574897e19511ae43afa7e78c</span><br><span class="line">[kubeexec] DEBUG 2020/03/06 16:45:13 heketi/pkg/remoteexec/kube/exec.go:81:kube.ExecCommands: Ran command [mkdir -p /var/lib/heketi/mounts/vg_ccc135aa56ab4f89868d9755bd531a22/brick_b6411ccff63daf1270bc9f354ca484dd] on [pod:glusterfs-77ghn c:glusterfs ns:glusterfs (from host:ceph2 selector:glusterfs-node)]: Stdout []: Stderr []</span><br><span class="line">[kubeexec] DEBUG 2020/03/06 16:45:13 heketi/pkg/remoteexec/kube/exec.go:81:kube.ExecCommands: Ran command [mkdir -p /var/lib/heketi/mounts/vg_63f644b972ff7a04259395f67c149cf2/brick_777c447835963ef4db7cbb2392c85e59] on [pod:glusterfs-mhsgb c:glusterfs ns:glusterfs (from host:ceph3 selector:glusterfs-node)]: Stdout []: Stderr []</span><br><span class="line">[kubeexec] DEBUG 2020/03/06 16:45:13 heketi/pkg/remoteexec/kube/exec.go:81:kube.ExecCommands: Ran command [mkdir -p /var/lib/heketi/mounts/vg_6199228451001048c7543f41ce6572cb/brick_98700f7b0bce70eb29279fb275763704] on [pod:glusterfs-fflbn c:glusterfs ns:glusterfs (from host:ceph1 selector:glusterfs-node)]: Stdout []: Stderr []</span><br><span class="line">[kubeexec] DEBUG 2020/03/06 16:45:13 heketi/pkg/remoteexec/kube/exec.go:81:kube.ExecCommands: Ran command [lvcreate -qq --autobackup=n --poolmetadatasize 12288K --chunksize 256K --size 2097152K --thin vg_ccc135aa56ab4f89868d9755bd531a22/tp_76e0f4c09fbd75bcd2bfae166fb8a73d --virtualsize 2097152K --name brick_b6411ccff63daf1270bc9f354ca484dd] on [pod:glusterfs-77ghn c:glusterfs ns:glusterfs (from host:ceph2 selector:glusterfs-node)]: Stdout []: Stderr [  WARNING: This metadata update is NOT backed up.</span><br><span class="line">]</span><br><span class="line">[kubeexec] DEBUG 2020/03/06 16:45:14 heketi/pkg/remoteexec/kube/exec.go:81:kube.ExecCommands: Ran command [lvcreate -qq --autobackup=n --poolmetadatasize 12288K --chunksize 256K --size 2097152K --thin vg_63f644b972ff7a04259395f67c149cf2/tp_777c447835963ef4db7cbb2392c85e59 --virtualsize 2097152K --name brick_777c447835963ef4db7cbb2392c85e59] on [pod:glusterfs-mhsgb c:glusterfs ns:glusterfs (from host:ceph3 selector:glusterfs-node)]: Stdout []: Stderr [  WARNING: This metadata update is NOT backed up.</span><br><span class="line">]</span><br><span class="line">[kubeexec] DEBUG 2020/03/06 16:45:14 heketi/pkg/remoteexec/kube/exec.go:81:kube.ExecCommands: Ran command [lvcreate -qq --autobackup=n --poolmetadatasize 12288K --chunksize 256K --size 2097152K --thin vg_6199228451001048c7543f41ce6572cb/tp_98700f7b0bce70eb29279fb275763704 --virtualsize 2097152K --name brick_98700f7b0bce70eb29279fb275763704] on [pod:glusterfs-fflbn c:glusterfs ns:glusterfs (from host:ceph1 selector:glusterfs-node)]: Stdout []: Stderr [  WARNING: This metadata update is NOT backed up.</span><br><span class="line">]</span><br><span class="line">[kubeexec] DEBUG 2020/03/06 16:45:14 heketi/pkg/remoteexec/kube/exec.go:81:kube.ExecCommands: Ran command [mkfs.xfs -i size=512 -n size=8192 /dev/mapper/vg_ccc135aa56ab4f89868d9755bd531a22-brick_b6411ccff63daf1270bc9f354ca484dd] on [pod:glusterfs-77ghn c:glusterfs ns:glusterfs (from host:ceph2 selector:glusterfs-node)]: Stdout [meta-data=/dev/mapper/vg_ccc135aa56ab4f89868d9755bd531a22-brick_b6411ccff63daf1270bc9f354ca484dd isize=512    agcount=8, agsize=65536 blks</span><br><span class="line">         =                       sectsz=512   attr=2, projid32bit=1</span><br><span class="line">         =                       crc=1        finobt=0, sparse=0</span><br><span class="line">data     =                       bsize=4096   blocks=524288, imaxpct=25</span><br><span class="line">         =                       sunit=64     swidth=64 blks</span><br><span class="line">naming   =version 2              bsize=8192   ascii-ci=0 ftype=1</span><br><span class="line">log      =internal log           bsize=4096   blocks=2560, version=2</span><br><span class="line">         =                       sectsz=512   sunit=64 blks, lazy-count=1</span><br><span class="line">realtime =none                   extsz=4096   blocks=0, rtextents=0</span><br><span class="line">]: Stderr []</span><br><span class="line">[kubeexec] DEBUG 2020/03/06 16:45:14 heketi/pkg/remoteexec/kube/exec.go:81:kube.ExecCommands: Ran command [awk &quot;BEGIN &#123;print \&quot;/dev/mapper/vg_ccc135aa56ab4f89868d9755bd531a22-brick_b6411ccff63daf1270bc9f354ca484dd /var/lib/heketi/mounts/vg_ccc135aa56ab4f89868d9755bd531a22/brick_b6411ccff63daf1270bc9f354ca484dd xfs rw,inode64,noatime,nouuid 1 2\&quot; &gt;&gt; \&quot;/var/lib/heketi/fstab\&quot;&#125;&quot;] on [pod:glusterfs-77ghn c:glusterfs ns:glusterfs (from host:ceph2 selector:glusterfs-node)]: Stdout []: Stderr []</span><br><span class="line">[kubeexec] DEBUG 2020/03/06 16:45:14 heketi/pkg/remoteexec/kube/exec.go:81:kube.ExecCommands: Ran command [mkfs.xfs -i size=512 -n size=8192 /dev/mapper/vg_63f644b972ff7a04259395f67c149cf2-brick_777c447835963ef4db7cbb2392c85e59] on [pod:glusterfs-mhsgb c:glusterfs ns:glusterfs (from host:ceph3 selector:glusterfs-node)]: Stdout [meta-data=/dev/mapper/vg_63f644b972ff7a04259395f67c149cf2-brick_777c447835963ef4db7cbb2392c85e59 isize=512    agcount=8, agsize=65536 blks</span><br><span class="line">         =                       sectsz=512   attr=2, projid32bit=1</span><br><span class="line">         =                       crc=1        finobt=0, sparse=0</span><br><span class="line">data     =                       bsize=4096   blocks=524288, imaxpct=25</span><br><span class="line">         =                       sunit=64     swidth=64 blks</span><br><span class="line">naming   =version 2              bsize=8192   ascii-ci=0 ftype=1</span><br><span class="line">log      =internal log           bsize=4096   blocks=2560, version=2</span><br><span class="line">         =                       sectsz=512   sunit=64 blks, lazy-count=1</span><br><span class="line">realtime =none                   extsz=4096   blocks=0, rtextents=0</span><br><span class="line">]: Stderr []</span><br><span class="line">[negroni] 2020-03-06T16:45:14Z | 200 |   34.302µs | 192.168.186.10:30080 | GET /queue/5ffdc4ab574897e19511ae43afa7e78c</span><br><span class="line">[kubeexec] DEBUG 2020/03/06 16:45:14 heketi/pkg/remoteexec/kube/exec.go:81:kube.ExecCommands: Ran command [awk &quot;BEGIN &#123;print \&quot;/dev/mapper/vg_63f644b972ff7a04259395f67c149cf2-brick_777c447835963ef4db7cbb2392c85e59 /var/lib/heketi/mounts/vg_63f644b972ff7a04259395f67c149cf2/brick_777c447835963ef4db7cbb2392c85e59 xfs rw,inode64,noatime,nouuid 1 2\&quot; &gt;&gt; \&quot;/var/lib/heketi/fstab\&quot;&#125;&quot;] on [pod:glusterfs-mhsgb c:glusterfs ns:glusterfs (from host:ceph3 selector:glusterfs-node)]: Stdout []: Stderr []</span><br><span class="line">[kubeexec] DEBUG 2020/03/06 16:45:14 heketi/pkg/remoteexec/kube/exec.go:81:kube.ExecCommands: Ran command [mkfs.xfs -i size=512 -n size=8192 /dev/mapper/vg_6199228451001048c7543f41ce6572cb-brick_98700f7b0bce70eb29279fb275763704] on [pod:glusterfs-fflbn c:glusterfs ns:glusterfs (from host:ceph1 selector:glusterfs-node)]: Stdout [meta-data=/dev/mapper/vg_6199228451001048c7543f41ce6572cb-brick_98700f7b0bce70eb29279fb275763704 isize=512    agcount=8, agsize=65536 blks</span><br><span class="line">         =                       sectsz=512   attr=2, projid32bit=1</span><br><span class="line">         =                       crc=1        finobt=0, sparse=0</span><br><span class="line">data     =                       bsize=4096   blocks=524288, imaxpct=25</span><br><span class="line">         =                       sunit=64     swidth=64 blks</span><br><span class="line">naming   =version 2              bsize=8192   ascii-ci=0 ftype=1</span><br><span class="line">log      =internal log           bsize=4096   blocks=2560, version=2</span><br><span class="line">         =                       sectsz=512   sunit=64 blks, lazy-count=1</span><br><span class="line">realtime =none                   extsz=4096   blocks=0, rtextents=0</span><br><span class="line">]: Stderr []</span><br><span class="line">[kubeexec] DEBUG 2020/03/06 16:45:14 heketi/pkg/remoteexec/kube/exec.go:81:kube.ExecCommands: Ran command [awk &quot;BEGIN &#123;print \&quot;/dev/mapper/vg_6199228451001048c7543f41ce6572cb-brick_98700f7b0bce70eb29279fb275763704 /var/lib/heketi/mounts/vg_6199228451001048c7543f41ce6572cb/brick_98700f7b0bce70eb29279fb275763704 xfs rw,inode64,noatime,nouuid 1 2\&quot; &gt;&gt; \&quot;/var/lib/heketi/fstab\&quot;&#125;&quot;] on [pod:glusterfs-fflbn c:glusterfs ns:glusterfs (from host:ceph1 selector:glusterfs-node)]: Stdout []: Stderr []</span><br><span class="line">[kubeexec] DEBUG 2020/03/06 16:45:14 heketi/pkg/remoteexec/kube/exec.go:81:kube.ExecCommands: Ran command [mount -o rw,inode64,noatime,nouuid /dev/mapper/vg_ccc135aa56ab4f89868d9755bd531a22-brick_b6411ccff63daf1270bc9f354ca484dd /var/lib/heketi/mounts/vg_ccc135aa56ab4f89868d9755bd531a22/brick_b6411ccff63daf1270bc9f354ca484dd] on [pod:glusterfs-77ghn c:glusterfs ns:glusterfs (from host:ceph2 selector:glusterfs-node)]: Stdout []: Stderr []</span><br><span class="line">[kubeexec] DEBUG 2020/03/06 16:45:14 heketi/pkg/remoteexec/kube/exec.go:81:kube.ExecCommands: Ran command [mount -o rw,inode64,noatime,nouuid /dev/mapper/vg_63f644b972ff7a04259395f67c149cf2-brick_777c447835963ef4db7cbb2392c85e59 /var/lib/heketi/mounts/vg_63f644b972ff7a04259395f67c149cf2/brick_777c447835963ef4db7cbb2392c85e59] on [pod:glusterfs-mhsgb c:glusterfs ns:glusterfs (from host:ceph3 selector:glusterfs-node)]: Stdout []: Stderr []</span><br><span class="line">[kubeexec] DEBUG 2020/03/06 16:45:14 heketi/pkg/remoteexec/kube/exec.go:81:kube.ExecCommands: Ran command [mkdir /var/lib/heketi/mounts/vg_ccc135aa56ab4f89868d9755bd531a22/brick_b6411ccff63daf1270bc9f354ca484dd/brick] on [pod:glusterfs-77ghn c:glusterfs ns:glusterfs (from host:ceph2 selector:glusterfs-node)]: Stdout []: Stderr []</span><br><span class="line">[kubeexec] DEBUG 2020/03/06 16:45:14 heketi/pkg/remoteexec/kube/exec.go:81:kube.ExecCommands: Ran command [mkdir /var/lib/heketi/mounts/vg_63f644b972ff7a04259395f67c149cf2/brick_777c447835963ef4db7cbb2392c85e59/brick] on [pod:glusterfs-mhsgb c:glusterfs ns:glusterfs (from host:ceph3 selector:glusterfs-node)]: Stdout []: Stderr []</span><br><span class="line">[kubeexec] DEBUG 2020/03/06 16:45:14 heketi/pkg/remoteexec/kube/exec.go:81:kube.ExecCommands: Ran command [mount -o rw,inode64,noatime,nouuid /dev/mapper/vg_6199228451001048c7543f41ce6572cb-brick_98700f7b0bce70eb29279fb275763704 /var/lib/heketi/mounts/vg_6199228451001048c7543f41ce6572cb/brick_98700f7b0bce70eb29279fb275763704] on [pod:glusterfs-fflbn c:glusterfs ns:glusterfs (from host:ceph1 selector:glusterfs-node)]: Stdout []: Stderr []</span><br><span class="line">[kubeexec] DEBUG 2020/03/06 16:45:14 heketi/pkg/remoteexec/kube/exec.go:81:kube.ExecCommands: Ran command [mkdir /var/lib/heketi/mounts/vg_6199228451001048c7543f41ce6572cb/brick_98700f7b0bce70eb29279fb275763704/brick] on [pod:glusterfs-fflbn c:glusterfs ns:glusterfs (from host:ceph1 selector:glusterfs-node)]: Stdout []: Stderr []</span><br><span class="line">[cmdexec] INFO 2020/03/06 16:45:14 Creating volume heketidbstorage replica 3</span><br><span class="line">[negroni] 2020-03-06T16:45:15Z | 200 |   42.505µs | 192.168.186.10:30080 | GET /queue/5ffdc4ab574897e19511ae43afa7e78c</span><br><span class="line">[kubeexec] DEBUG 2020/03/06 16:45:15 heketi/pkg/remoteexec/kube/exec.go:81:kube.ExecCommands: Ran command [gluster --mode=script --timeout=600 volume create heketidbstorage replica 3 192.168.186.11:/var/lib/heketi/mounts/vg_ccc135aa56ab4f89868d9755bd531a22/brick_b6411ccff63daf1270bc9f354ca484dd/brick 192.168.186.10:/var/lib/heketi/mounts/vg_6199228451001048c7543f41ce6572cb/brick_98700f7b0bce70eb29279fb275763704/brick 192.168.186.12:/var/lib/heketi/mounts/vg_63f644b972ff7a04259395f67c149cf2/brick_777c447835963ef4db7cbb2392c85e59/brick] on [pod:glusterfs-77ghn c:glusterfs ns:glusterfs (from host:ceph2 selector:glusterfs-node)]: Stdout [volume create: heketidbstorage: success: please start the volume to access data</span><br><span class="line">]: Stderr []</span><br><span class="line">[kubeexec] DEBUG 2020/03/06 16:45:15 heketi/pkg/remoteexec/kube/exec.go:81:kube.ExecCommands: Ran command [gluster --mode=script --timeout=600 volume set heketidbstorage user.heketi.id 17a63c6483a00155fb0b48bb353b9c7a] on [pod:glusterfs-77ghn c:glusterfs ns:glusterfs (from host:ceph2 selector:glusterfs-node)]: Stdout [volume set: success</span><br><span class="line">]: Stderr []</span><br><span class="line">[negroni] 2020-03-06T16:45:16Z | 200 |   33.664µs | 192.168.186.10:30080 | GET /queue/5ffdc4ab574897e19511ae43afa7e78c</span><br><span class="line">[negroni] 2020-03-06T16:45:17Z | 200 |   30.885µs | 192.168.186.10:30080 | GET /queue/5ffdc4ab574897e19511ae43afa7e78c</span><br><span class="line">[negroni] 2020-03-06T16:45:18Z | 200 |   101.832µs | 192.168.186.10:30080 | GET /queue/5ffdc4ab574897e19511ae43afa7e78c</span><br><span class="line">[kubeexec] DEBUG 2020/03/06 16:45:19 heketi/pkg/remoteexec/kube/exec.go:81:kube.ExecCommands: Ran command [gluster --mode=script --timeout=600 volume start heketidbstorage] on [pod:glusterfs-77ghn c:glusterfs ns:glusterfs (from host:ceph2 selector:glusterfs-node)]: Stdout [volume start: heketidbstorage: success</span><br><span class="line">]: Stderr []</span><br><span class="line">[asynchttp] INFO 2020/03/06 16:45:19 asynchttp.go:292: Completed job 5ffdc4ab574897e19511ae43afa7e78c in 5.802125511s</span><br><span class="line">[negroni] 2020-03-06T16:45:19Z | 303 |   42.089µs | 192.168.186.10:30080 | GET /queue/5ffdc4ab574897e19511ae43afa7e78c</span><br><span class="line">[negroni] 2020-03-06T16:45:19Z | 200 |   50.078742ms | 192.168.186.10:30080 | GET /volumes/17a63c6483a00155fb0b48bb353b9c7a</span><br><span class="line">[negroni] 2020-03-06T16:45:19Z | 200 |   484.808µs | 192.168.186.10:30080 | GET /backup/db</span><br><span class="line"></span><br></pre></td></tr></table></figure><h2 id="k8s是怎么通过heketi创建pvc的"><a href="#k8s是怎么通过heketi创建pvc的" class="headerlink" title="k8s是怎么通过heketi创建pvc的"></a>k8s是怎么通过heketi创建pvc的</h2><p>storageclass中会指定heketi server端的地址和卷的类型（replica 3），用户通过pvc创建1G的pv,观查heketi服务后台干了啥：</p><p>首先发现heketi接收到请求后起了一个job，创建了3个bricks，在其中三台gfs节点创建了相应的目录，如下图：</p><p><img src="/doc_picture/gfs-heketi-1.png" alt="image-20210716001252519"></p><p>创建lv,添加自动挂载：</p><p><img src="/doc_picture/gfs-heketi-2.png" alt="image-20210716001304334"></p><p>创建brick，设置权限：</p><p><img src="/doc_picture/gfs-heketi-3.png" alt="image-20210716001320020"></p><p>创建volume：</p><p><img src="/doc_picture/gfs-heketi-4.png" alt="image-20210716001344552"></p><h1 id="参考文档"><a href="#参考文档" class="headerlink" title="参考文档"></a>参考文档</h1><p>heketi: <a href="https://github.com/heketi/heketi">https://github.com/heketi/heketi</a></p><p>glusterfs:  <a href="https://github.com/gluster/gluster-kubernetes">https://github.com/gluster/gluster-kubernetes</a></p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;Glusterfs提供底层存储功能，heketi为glusterfs提供restful风格的api，Heketi要求在每个glusterfs节点上配备&lt;strong&gt;裸磁盘&lt;/strong&gt;，目前heketi仅支持使用裸磁盘(未格式化)添加为device，不支持文件系统，因</summary>
      
    
    
    
    
    <category term="glusterfs" scheme="https://slions.github.io/tags/glusterfs/"/>
    
    <category term="kubernetes storage" scheme="https://slions.github.io/tags/kubernetes-storage/"/>
    
  </entry>
  
  <entry>
    <title>linux添加新硬盘无法识别解决方法</title>
    <link href="https://slions.github.io/2021/07/15/linux%E6%B7%BB%E5%8A%A0%E6%96%B0%E7%A1%AC%E7%9B%98%E6%97%A0%E6%B3%95%E8%AF%86%E5%88%AB%E8%A7%A3%E5%86%B3%E6%96%B9%E6%B3%95/"/>
    <id>https://slions.github.io/2021/07/15/linux%E6%B7%BB%E5%8A%A0%E6%96%B0%E7%A1%AC%E7%9B%98%E6%97%A0%E6%B3%95%E8%AF%86%E5%88%AB%E8%A7%A3%E5%86%B3%E6%96%B9%E6%B3%95/</id>
    <published>2021-07-14T16:18:03.000Z</published>
    <updated>2021-07-14T16:34:35.454Z</updated>
    
    <content type="html"><![CDATA[<p>我们经常会遇到当主机新添加磁盘后，进入宿主机查看并没有显示多出来的那块磁盘，是因为Linux目前缺乏允许动态SCSI通道重配的命令。</p><p>重启主机是检测新添加磁盘设备的可靠方式，但是会造成上面运行的应用服务中断，有没有什么方法是能优雅的解决此问题呢。执行以下脚本就可以了。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> <span class="keyword">for</span> i <span class="keyword">in</span> /sys/class/scsi_host/host*/scan;<span class="keyword">do</span> <span class="built_in">echo</span> <span class="string">&quot;- - -&quot;</span> &gt;<span class="variable">$i</span>;<span class="keyword">done</span></span></span><br></pre></td></tr></table></figure><blockquote><p>其中‘- - -’代表channel，target和LUN编号。以上命令会使系统重新扫描所有channel，target以及可见LUN。</p></blockquote>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;我们经常会遇到当主机新添加磁盘后，进入宿主机查看并没有显示多出来的那块磁盘，是因为Linux目前缺乏允许动态SCSI通道重配的命令。&lt;/p&gt;
&lt;p&gt;重启主机是检测新添加磁盘设备的可靠方式，但是会造成上面运行的应用服务中断，有没有什么方法是能优雅的解决此问题呢。执行以下脚本就</summary>
      
    
    
    
    
    <category term="linux" scheme="https://slions.github.io/tags/linux/"/>
    
  </entry>
  
  <entry>
    <title>手工安装heketi管理glusterfs</title>
    <link href="https://slions.github.io/2021/07/14/%E6%89%8B%E5%B7%A5%E5%AE%89%E8%A3%85heketi%E7%AE%A1%E7%90%86glusterfs/"/>
    <id>https://slions.github.io/2021/07/14/%E6%89%8B%E5%B7%A5%E5%AE%89%E8%A3%85heketi%E7%AE%A1%E7%90%86glusterfs/</id>
    <published>2021-07-14T08:06:32.000Z</published>
    <updated>2021-07-14T08:51:24.816Z</updated>
    
    <content type="html"><![CDATA[<p>GlusterFS（gfs）由此名字也可看出是文件系统存储相关的软件，它是一个开源的分布式文件系统，具有强大的横向扩展能力。Heketi是一个GlusterFs管理软件，可以管理glusterFS集群的卷创建、删除等操作。Glusterfs作为kubernetes支持的多种卷类型之一，可以为上层应用提供多种挂载形式。</p><p>heketi + glusterfs提供两种部署形式：</p><ul><li>容器化</li><li>传统服务</li></ul><p>以下我会先介绍以传统服务的部署形态对接kubernetes。</p><blockquote><ol><li>本文的原稿是我在19年初编写的，以下版本可能过于老旧，同学们进行搭建测试时建议下载较新的版本。</li><li>阅读本文前希望您能对glusterfs,kubernetes,storageclass,pv有所了解。</li></ol></blockquote><h1 id="环境描述"><a href="#环境描述" class="headerlink" title="环境描述"></a>环境描述</h1><table><thead><tr><th><strong>主机名</strong></th><th>ip地址</th><th><strong>系统环境</strong></th><th><strong>角色</strong></th></tr></thead><tbody><tr><td>k8s-1</td><td>IP:192.168.186.10</td><td>cpu:x2  mem:2GB  disk:/dev/sdb 10GB</td><td>K8s_master,Gluster_master,Heketi_master</td></tr><tr><td>K8s-2</td><td>IP:192.168.186.11</td><td>cpu:x2  mem:2GB  disk:/dev/sdb 10GB</td><td>K8s_node,Gluster_node</td></tr><tr><td>K8s-3</td><td>IP:192.168.186.12</td><td>cpu:x2  mem:2GB  disk:/dev/sdb 10GB</td><td>K8s_master,Gluster_node</td></tr></tbody></table><p>如果存在iptable限制，需执行以下命令开通以下port</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">iptables -N heketi</span><br><span class="line">iptables -A heketi -p tcp -m state --state NEW -m tcp --dport 24007 -j ACCEPT</span><br><span class="line">iptables -A heketi -p tcp -m state --state NEW -m tcp --dport 24008 -j ACCEPT</span><br><span class="line">iptables -A heketi -p tcp -m state --state NEW -m tcp --dport 2222 -j ACCEPT</span><br><span class="line">iptables -A heketi -p tcp -m state --state NEW -m multiport --dports 49152:49251 -j ACCEPT</span><br><span class="line">service iptables save</span><br></pre></td></tr></table></figure><h1 id="安装配置gfs"><a href="#安装配置gfs" class="headerlink" title="安装配置gfs"></a>安装配置gfs</h1><p>三台机器都要安装gfs软件并启动服务</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-1 ~]# yum -y install centos-release-gluster</span><br><span class="line">[root@k8s-1 ~]# yum -y install glusterfs-server</span><br><span class="line">[root@k8s-1 ~]# systemctl enable glusterd</span><br><span class="line">[root@k8s-1 ~]# systemctl start glusterd</span><br></pre></td></tr></table></figure><p>配置/etc/hosts,IP和主机名都一一对应</p><p><img src="/doc_picture/heketi-1.png" alt="image-20210714161502089"></p><p>安装glusterfs client客户端命令</p><p><img src="/doc_picture/heketi-2.png" alt="image-20210714161514389"></p><p>为存储池添加节点Node:（k8s-1操作，不用添加自己）</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-1 ~]# gluster peer probe k8s-2</span><br><span class="line">[root@k8s-1 ~]# gluster peer probe k8s-3</span><br></pre></td></tr></table></figure><h1 id="安装配置heketi"><a href="#安装配置heketi" class="headerlink" title="安装配置heketi"></a>安装配置heketi</h1><p>Heketi使用SSH来配置GlusterFS的所有节点。创建SSH密钥对:</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-1 ~]# mkdir /etc/heketi</span><br><span class="line">ssh-keygen -f /etc/heketi/heketi_key -t rsa -N ‘’</span><br><span class="line">[root@k8s-1 ~]# chown heketi:heketi /etc/heketi/heketi_key*</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>ssh公钥传递，这里只以一个节点为例：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-1 ~]# ssh-copy-id -i /etc/heketi/heketi_key.pub root@192.168.186.11</span><br></pre></td></tr></table></figure><p>制作完成后会在当前目录下生成heketi_key、heketi_key.pub，将公钥heketi_key.pub拷贝到所有glusterfs节点上/etc/heketi/keketi_key.pub（包括你登陆的第一个节点）</p><p>安装heketi（在k8s-1操作）：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-1 ~]#  yum install -y https://mirrors.aliyun.com/centos/7.6.1810/storage/x86_64/gluster-5/heketi-8.0.0-1.el7.x86_64.rpm</span><br><span class="line">[root@k8s-1 ~]#  yum install -y https://mirrors.aliyun.com/centos/7.6.1810/storage/x86_64/gluster-5/heketi-client-8.0.0-1.el7.x86_64.rpm</span><br></pre></td></tr></table></figure><p>创建存储db的文件夹：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-1 ~]#  mkdir /dcos/heketi</span><br><span class="line">[root@k8s-1 ~]#  chown -R heketi:heketi /dcos/heketi</span><br></pre></td></tr></table></figure><p>配置 heketi.json：</p><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">  <span class="attr">&quot;_port_comment&quot;</span>: <span class="string">&quot;Heketi Server Port Number&quot;</span>,</span><br><span class="line">  <span class="attr">&quot;port&quot;</span>: <span class="string">&quot;8088&quot;</span>,</span><br><span class="line"></span><br><span class="line">  <span class="attr">&quot;_use_auth&quot;</span>: <span class="string">&quot;Enable JWT authorization. Please enable for deployment&quot;</span>,</span><br><span class="line">  <span class="attr">&quot;use_auth&quot;</span>: <span class="literal">false</span>,</span><br><span class="line"></span><br><span class="line">  <span class="attr">&quot;_jwt&quot;</span>: <span class="string">&quot;Private keys for access&quot;</span>,</span><br><span class="line">  <span class="attr">&quot;jwt&quot;</span>: &#123;</span><br><span class="line">    <span class="attr">&quot;_admin&quot;</span>: <span class="string">&quot;Admin has access to all APIs&quot;</span>,</span><br><span class="line">    <span class="attr">&quot;admin&quot;</span>: &#123;</span><br><span class="line">      <span class="attr">&quot;key&quot;</span>: <span class="string">&quot;123456&quot;</span></span><br><span class="line">    &#125;,</span><br><span class="line">    <span class="attr">&quot;_user&quot;</span>: <span class="string">&quot;User only has access to /volumes endpoint&quot;</span>,</span><br><span class="line">    <span class="attr">&quot;user&quot;</span>: &#123;</span><br><span class="line">      <span class="attr">&quot;key&quot;</span>: <span class="string">&quot;123456&quot;</span></span><br><span class="line">    &#125;</span><br><span class="line">  &#125;,</span><br><span class="line"></span><br><span class="line">  <span class="attr">&quot;_glusterfs_comment&quot;</span>: <span class="string">&quot;GlusterFS Configuration&quot;</span>,</span><br><span class="line">  <span class="attr">&quot;glusterfs&quot;</span>: &#123;</span><br><span class="line">    <span class="attr">&quot;_executor_comment&quot;</span>: [</span><br><span class="line">      <span class="string">&quot;Execute plugin. Possible choices: mock, ssh&quot;</span>,</span><br><span class="line">      <span class="string">&quot;mock: This setting is used for testing and development.&quot;</span>,</span><br><span class="line">      <span class="string">&quot;      It will not send commands to any node.&quot;</span>,</span><br><span class="line">      <span class="string">&quot;ssh:  This setting will notify Heketi to ssh to the nodes.&quot;</span>,</span><br><span class="line">      <span class="string">&quot;      It will need the values in sshexec to be configured.&quot;</span>,</span><br><span class="line">      <span class="string">&quot;kubernetes: Communicate with GlusterFS containers over&quot;</span>,</span><br><span class="line">      <span class="string">&quot;            Kubernetes exec api.&quot;</span></span><br><span class="line">    ],</span><br><span class="line">    <span class="attr">&quot;executor&quot;</span>: <span class="string">&quot;ssh&quot;</span>,</span><br><span class="line"></span><br><span class="line">    <span class="attr">&quot;_sshexec_comment&quot;</span>: <span class="string">&quot;SSH username and private key file information&quot;</span>,</span><br><span class="line">    <span class="attr">&quot;sshexec&quot;</span>: &#123;</span><br><span class="line">      <span class="attr">&quot;keyfile&quot;</span>: <span class="string">&quot;/etc/heketi/heketi_key&quot;</span>,</span><br><span class="line">      <span class="attr">&quot;user&quot;</span>: <span class="string">&quot;root&quot;</span>,</span><br><span class="line">      <span class="attr">&quot;port&quot;</span>: <span class="string">&quot;22&quot;</span>,</span><br><span class="line">      <span class="attr">&quot;fstab&quot;</span>: <span class="string">&quot;/etc/fstab&quot;</span></span><br><span class="line">    &#125;,</span><br><span class="line"></span><br><span class="line">    <span class="attr">&quot;_kubeexec_comment&quot;</span>: <span class="string">&quot;Kubernetes configuration&quot;</span>,</span><br><span class="line">    <span class="attr">&quot;kubeexec&quot;</span>: &#123;</span><br><span class="line">      <span class="attr">&quot;host&quot;</span> :<span class="string">&quot;https://kubernetes.host:8443&quot;</span>,</span><br><span class="line">      <span class="attr">&quot;cert&quot;</span> : <span class="string">&quot;/path/to/crt.file&quot;</span>,</span><br><span class="line">      <span class="attr">&quot;insecure&quot;</span>: <span class="literal">false</span>,</span><br><span class="line">      <span class="attr">&quot;user&quot;</span>: <span class="string">&quot;kubernetes username&quot;</span>,</span><br><span class="line">      <span class="attr">&quot;password&quot;</span>: <span class="string">&quot;password for kubernetes user&quot;</span>,</span><br><span class="line">      <span class="attr">&quot;namespace&quot;</span>: <span class="string">&quot;OpenShift project or Kubernetes namespace&quot;</span>,</span><br><span class="line">      <span class="attr">&quot;fstab&quot;</span>: <span class="string">&quot;Optional: Specify fstab file on node.  Default is /etc/fstab&quot;</span></span><br><span class="line">    &#125;,</span><br><span class="line"></span><br><span class="line">    <span class="attr">&quot;_db_comment&quot;</span>: <span class="string">&quot;Database file name&quot;</span>,</span><br><span class="line">    <span class="attr">&quot;brick_min_size_gb&quot;</span>: <span class="number">1</span>,</span><br><span class="line">    <span class="attr">&quot;db&quot;</span>: <span class="string">&quot;/dcos/heketi/heketi.db&quot;</span>,</span><br><span class="line"></span><br><span class="line">    <span class="attr">&quot;_loglevel_comment&quot;</span>: [</span><br><span class="line">      <span class="string">&quot;Set log level. Choices are:&quot;</span>,</span><br><span class="line">      <span class="string">&quot;  none, critical, error, warning, info, debug&quot;</span>,</span><br><span class="line">      <span class="string">&quot;Default is warning&quot;</span></span><br><span class="line">    ],</span><br><span class="line">    <span class="attr">&quot;loglevel&quot;</span> : <span class="string">&quot;debug&quot;</span></span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><blockquote><p> 注：这里需要注意只是测试的话用mock 授权，standalone模式就 ssh 授权，k8s下就 kubernetes授权。</p></blockquote><p>重启heketi：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-1 ~]#  systemctl enable heketi</span><br><span class="line">[root@k8s-1 ~]#  systemctl restart heketi</span><br></pre></td></tr></table></figure><p>测试heketi是否好用：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-1 ~]#  curl http://localhost:8088/hello</span><br></pre></td></tr></table></figure><p>通过topology文件对接glusterfs集群：</p><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">  <span class="attr">&quot;clusters&quot;</span>: [</span><br><span class="line">    &#123;</span><br><span class="line">      <span class="attr">&quot;nodes&quot;</span>: [</span><br><span class="line">        &#123;</span><br><span class="line">          <span class="attr">&quot;node&quot;</span>: &#123;</span><br><span class="line">            <span class="attr">&quot;hostnames&quot;</span>: &#123;</span><br><span class="line">              <span class="attr">&quot;manage&quot;</span>: [</span><br><span class="line">                <span class="string">&quot;192.168.186.10&quot;</span></span><br><span class="line">              ],</span><br><span class="line">              <span class="attr">&quot;storage&quot;</span>: [</span><br><span class="line">                <span class="string">&quot;192.168.186.10&quot;</span></span><br><span class="line">              ]</span><br><span class="line">            &#125;,</span><br><span class="line">            <span class="attr">&quot;zone&quot;</span>: <span class="number">1</span></span><br><span class="line">          &#125;,</span><br><span class="line">          <span class="attr">&quot;devices&quot;</span>: [</span><br><span class="line">            <span class="string">&quot;/dev/sdb&quot;</span></span><br><span class="line">          ]</span><br><span class="line">        &#125;,</span><br><span class="line">        &#123;</span><br><span class="line">          <span class="attr">&quot;node&quot;</span>: &#123;</span><br><span class="line">            <span class="attr">&quot;hostnames&quot;</span>: &#123;</span><br><span class="line">              <span class="attr">&quot;manage&quot;</span>: [</span><br><span class="line">                <span class="string">&quot;192.168.186.11&quot;</span></span><br><span class="line">              ],</span><br><span class="line">              <span class="attr">&quot;storage&quot;</span>: [</span><br><span class="line">                <span class="string">&quot;192.168.186.11&quot;</span></span><br><span class="line">              ]</span><br><span class="line">            &#125;,</span><br><span class="line">            <span class="attr">&quot;zone&quot;</span>: <span class="number">1</span></span><br><span class="line">          &#125;,</span><br><span class="line">          <span class="attr">&quot;devices&quot;</span>: [</span><br><span class="line">            <span class="string">&quot;/dev/sdb&quot;</span></span><br><span class="line">          ]</span><br><span class="line">        &#125;,</span><br><span class="line">        &#123;</span><br><span class="line">          <span class="attr">&quot;node&quot;</span>: &#123;</span><br><span class="line">            <span class="attr">&quot;hostnames&quot;</span>: &#123;</span><br><span class="line">              <span class="attr">&quot;manage&quot;</span>: [</span><br><span class="line">                <span class="string">&quot;192.168.186.12&quot;</span></span><br><span class="line">              ],</span><br><span class="line">              <span class="attr">&quot;storage&quot;</span>: [</span><br><span class="line">                <span class="string">&quot;192.168.186.12&quot;</span></span><br><span class="line">              ]</span><br><span class="line">            &#125;,</span><br><span class="line">            <span class="attr">&quot;zone&quot;</span>: <span class="number">1</span></span><br><span class="line">          &#125;,</span><br><span class="line">          <span class="attr">&quot;devices&quot;</span>: [</span><br><span class="line">            <span class="string">&quot;/dev/sdb&quot;</span></span><br><span class="line">          ]</span><br><span class="line">        &#125;</span><br><span class="line">      ]</span><br><span class="line">    &#125;</span><br><span class="line">  ]</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>将该文件发送给heketi创建：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-1 ~]# heketi-cli --server http://192.168.186.10:8088 --user admin --secret 123456 topology load --json=/etc/heketi/topology.json</span><br></pre></td></tr></table></figure><p>创建成功后，heketi会在每个gluster节点上创建一个逻辑卷组，通过vgscan或vgdisplay可以看到：</p><p><img src="/doc_picture/heketi-3.png" alt="image-20210714163441286"></p><p>创建卷测试：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-1 ~]# heketi-cli --server http://192.168.186.10:8088 --user admin --secret 123456  volume create --size=1</span><br></pre></td></tr></table></figure><h1 id="配置kubernetes使用glusterfs"><a href="#配置kubernetes使用glusterfs" class="headerlink" title="配置kubernetes使用glusterfs"></a>配置kubernetes使用glusterfs</h1><h2 id="以密文的方式创建heketi-userkey的secret"><a href="#以密文的方式创建heketi-userkey的secret" class="headerlink" title="以密文的方式创建heketi userkey的secret"></a>以密文的方式创建heketi userkey的secret</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-1 k8s_gfs]# echo 123456|base64</span><br><span class="line">MTIzNDU2Cg==</span><br><span class="line">[root@k8s-1 k8s_gfs]# cat glusterfs-secret.yaml </span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: Secret</span><br><span class="line">metadata:</span><br><span class="line">  name: heketi-secret</span><br><span class="line">  namespace: default</span><br><span class="line">data:</span><br><span class="line"><span class="meta">  #</span><span class="bash"> base64 encoded password. E.g.: <span class="built_in">echo</span> -n <span class="string">&quot;mypassword&quot;</span> | base64</span></span><br><span class="line">  key: MTIzNDU2Cg==</span><br><span class="line">type: kubernetes.io/glusterfs</span><br></pre></td></tr></table></figure><h2 id="创建Storageclass"><a href="#创建Storageclass" class="headerlink" title="创建Storageclass"></a>创建Storageclass</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-1 k8s_gfs]# cat storageclass_glusterfs.yaml </span><br><span class="line">apiVersion: storage.k8s.io/v1beta1</span><br><span class="line">kind: StorageClass</span><br><span class="line">metadata:</span><br><span class="line">  name: glusterfs</span><br><span class="line">provisioner: kubernetes.io/glusterfs</span><br><span class="line">parameters:</span><br><span class="line">  resturl: &quot;http://192.168.186.10:8088&quot;</span><br><span class="line">  clusterid: &quot;a06343355a5f3e4240662d3963ec7d90&quot;</span><br><span class="line">  restauthenabled: &quot;true&quot;</span><br><span class="line">  secretNamespace: &quot;default&quot;</span><br><span class="line">  secretName: &quot;heketi-secret&quot;</span><br><span class="line">  restuser: &quot;admin&quot;</span><br><span class="line"><span class="meta">  #</span><span class="bash">restuserkey: <span class="string">&quot;123456&quot;</span></span></span><br><span class="line">  gidMin: &quot;40000&quot;</span><br><span class="line">  gidMax: &quot;50000&quot;</span><br><span class="line">  volumetype: &quot;replicate:3&quot;</span><br><span class="line">allowVolumeExpansion: true</span><br></pre></td></tr></table></figure><h2 id="创建PVC"><a href="#创建PVC" class="headerlink" title="创建PVC"></a>创建PVC</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-1 k8s_gfs]# cat pvc_glusterfs.yaml </span><br><span class="line">kind: PersistentVolumeClaim</span><br><span class="line">apiVersion: v1</span><br><span class="line">metadata:</span><br><span class="line">  name: glusterfs-pvc</span><br><span class="line">  namespace: default</span><br><span class="line">  annotations:</span><br><span class="line">    volume.beta.kubernetes.io/storage-class: &quot;glusterfs&quot;</span><br><span class="line">spec:</span><br><span class="line">  accessModes:</span><br><span class="line">  - ReadWriteMany</span><br><span class="line">  resources:</span><br><span class="line">    requests:</span><br><span class="line">      storage: 2Gi</span><br></pre></td></tr></table></figure><h2 id="创建pods测试"><a href="#创建pods测试" class="headerlink" title="创建pods测试"></a>创建pods测试</h2><figure class="highlight yml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line">[<span class="string">root@k8s-1</span> <span class="string">k8s_gfs</span>]<span class="comment"># cat nginx-pod.yaml </span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">extensions/v1beta1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Deployment</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">nginx-gfs</span></span><br><span class="line">  <span class="attr">labels:</span></span><br><span class="line">    <span class="attr">name:</span> <span class="string">nginx-gfs</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">replicas:</span> <span class="number">2</span></span><br><span class="line">  <span class="attr">selector:</span></span><br><span class="line">    <span class="attr">matchLabels:</span></span><br><span class="line">      <span class="attr">name:</span> <span class="string">nginx-gfs</span></span><br><span class="line">  <span class="attr">template:</span></span><br><span class="line">    <span class="attr">metadata:</span></span><br><span class="line">      <span class="attr">labels:</span></span><br><span class="line">        <span class="attr">name:</span> <span class="string">nginx-gfs</span></span><br><span class="line">    <span class="attr">spec:</span></span><br><span class="line">      <span class="attr">containers:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">nginx-gfs</span></span><br><span class="line">        <span class="attr">image:</span> <span class="string">nginx</span></span><br><span class="line">        <span class="attr">ports:</span></span><br><span class="line">        <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">web</span></span><br><span class="line">          <span class="attr">containerPort:</span> <span class="number">80</span></span><br><span class="line">        <span class="attr">volumeMounts:</span></span><br><span class="line">        <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">gfs</span></span><br><span class="line">          <span class="attr">mountPath:</span> <span class="string">/usr/share/nginx/html</span></span><br><span class="line">      <span class="attr">volumes:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">gfs</span></span><br><span class="line">        <span class="attr">persistentVolumeClaim:</span></span><br><span class="line">          <span class="attr">claimName:</span> <span class="string">glusterfs-pvc</span></span><br></pre></td></tr></table></figure><h1 id="参考文档"><a href="#参考文档" class="headerlink" title="参考文档"></a>参考文档</h1><p><a href="https://kubernetes.io/zh/docs/concepts/storage/volumes/">https://kubernetes.io/zh/docs/concepts/storage/volumes/</a></p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;GlusterFS（gfs）由此名字也可看出是文件系统存储相关的软件，它是一个开源的分布式文件系统，具有强大的横向扩展能力。Heketi是一个GlusterFs管理软件，可以管理glusterFS集群的卷创建、删除等操作。Glusterfs作为kubernetes支持的多种</summary>
      
    
    
    
    
    <category term="heketi" scheme="https://slions.github.io/tags/heketi/"/>
    
    <category term="glusterfs" scheme="https://slions.github.io/tags/glusterfs/"/>
    
  </entry>
  
  <entry>
    <title>自己动手实现network namespace</title>
    <link href="https://slions.github.io/2021/07/13/network-namespace%E9%9A%94%E7%A6%BB%E6%B5%8B%E8%AF%95/"/>
    <id>https://slions.github.io/2021/07/13/network-namespace%E9%9A%94%E7%A6%BB%E6%B5%8B%E8%AF%95/</id>
    <published>2021-07-13T15:20:49.000Z</published>
    <updated>2021-07-13T18:41:22.918Z</updated>
    
    <content type="html"><![CDATA[<a href="/2021/07/09/%E8%81%8A%E8%81%8Anamespace/" title="之前的文章">之前的文章</a>提到了6种Linux的namespace隔离技术，其中network namespace为命名空间内的所有进程提供了全新隔离的网络协议栈。这包括网络接口，路由表和**iptables**规则。通过使用网络命名空间就可以实现网络虚拟环境，实现彼此之间的网络隔离，其实我们通过ip命令就可以模拟出网络命名空间。<h1 id="安装软件"><a href="#安装软件" class="headerlink" title="安装软件"></a>安装软件</h1><p>首先我们需要安装iprouter软件：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@slions_pc1 ~]# rpm -q iproute</span><br><span class="line">iproute-4.11.0-14.el7.x86_64</span><br></pre></td></tr></table></figure><p>我本地已经安装了，查看IP命令：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">[root@slions_pc1 ~]# ip help</span><br><span class="line">Usage: ip [ OPTIONS ] OBJECT &#123; COMMAND | help &#125;</span><br><span class="line">       ip [ -force ] -batch filename</span><br><span class="line">where  OBJECT := &#123; link | address | addrlabel | route | rule | neigh | ntable |</span><br><span class="line">                   tunnel | tuntap | maddress | mroute | mrule | monitor | xfrm |</span><br><span class="line">                   netns | l2tp | fou | macsec | tcp_metrics | token | netconf | ila |</span><br><span class="line">                   vrf &#125;</span><br><span class="line">       OPTIONS := &#123; -V[ersion] | -s[tatistics] | -d[etails] | -r[esolve] |</span><br><span class="line">                    -h[uman-readable] | -iec |</span><br><span class="line">                    -f[amily] &#123; inet | inet6 | ipx | dnet | mpls | bridge | link &#125; |</span><br><span class="line">                    -4 | -6 | -I | -D | -B | -0 |</span><br><span class="line">                    -l[oops] &#123; maximum-addr-flush-attempts &#125; | -br[ief] |</span><br><span class="line">                    -o[neline] | -t[imestamp] | -ts[hort] | -b[atch] [filename] |</span><br><span class="line">                    -rc[vbuf] [size] | -n[etns] name | -a[ll] | -c[olor]&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure><h1 id="创建netns"><a href="#创建netns" class="headerlink" title="创建netns"></a>创建netns</h1><p>查看本地是否存在netns</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@slions_pc1 ~]# ip netns list</span><br><span class="line">[root@slions_pc1 ~]#</span><br></pre></td></tr></table></figure><p>创建两个netns，分别叫slions_ns1,slions_ns2</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[root@slions_pc1 ~]# ip netns add slions_ns1</span><br><span class="line">[root@slions_pc1 ~]# ip netns add slions_ns2</span><br><span class="line">[root@slions_pc1 ~]# ip netns list</span><br><span class="line">slions_ns1</span><br><span class="line">slions_ns2</span><br></pre></td></tr></table></figure><p>我们查看下这个命名空间下的网卡信息,可以看到都只有lo环回网卡</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">[root@slions_pc1 ~]# ip netns exec slions_ns1 ifconfig -a</span><br><span class="line">lo: flags=8&lt;LOOPBACK&gt;  mtu 65536</span><br><span class="line">        loop  txqueuelen 1000  (Local Loopback)</span><br><span class="line">        RX packets 0  bytes 0 (0.0 B)</span><br><span class="line">        RX errors 0  dropped 0  overruns 0  frame 0</span><br><span class="line">        TX packets 0  bytes 0 (0.0 B)</span><br><span class="line">        TX errors 0  dropped 0 overruns 0  carrier 0  collisions 0</span><br><span class="line">[root@slions_pc1 ~]# ip netns exec slions_ns2 ifconfig -a</span><br><span class="line">lo: flags=8&lt;LOOPBACK&gt;  mtu 65536</span><br><span class="line">        loop  txqueuelen 1000  (Local Loopback)</span><br><span class="line">        RX packets 0  bytes 0 (0.0 B)</span><br><span class="line">        RX errors 0  dropped 0  overruns 0  frame 0</span><br><span class="line">        TX packets 0  bytes 0 (0.0 B)</span><br><span class="line">        TX errors 0  dropped 0 overruns 0  carrier 0  collisions 0</span><br><span class="line"></span><br></pre></td></tr></table></figure><h1 id="创建veth网卡设备对"><a href="#创建veth网卡设备对" class="headerlink" title="创建veth网卡设备对"></a>创建veth网卡设备对</h1><p>我们需要创建一对儿veth虚拟网卡</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">[root@slions_pc1 ~]# ip link add veth0 type veth peer name veth1</span><br><span class="line">[root@slions_pc1 ~]# ip link show</span><br><span class="line">1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN mode DEFAULT group default qlen 1000</span><br><span class="line">    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00</span><br><span class="line">2: ens33: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc pfifo_fast state UP mode DEFAULT group default qlen 1000</span><br><span class="line">    link/ether 00:0c:29:97:8c:d0 brd ff:ff:ff:ff:ff:ff</span><br><span class="line">3: docker0: &lt;NO-CARRIER,BROADCAST,MULTICAST,UP&gt; mtu 1500 qdisc noqueue state DOWN mode DEFAULT group default</span><br><span class="line">    link/ether 02:42:8a:8f:d7:d5 brd ff:ff:ff:ff:ff:ff</span><br><span class="line">8: veth1@veth0: &lt;BROADCAST,MULTICAST,M-DOWN&gt; mtu 1500 qdisc noop state DOWN mode DEFAULT group default qlen 1000</span><br><span class="line">    link/ether ae:f9:75:fa:93:4d brd ff:ff:ff:ff:ff:ff</span><br><span class="line">9: veth0@veth1: &lt;BROADCAST,MULTICAST,M-DOWN&gt; mtu 1500 qdisc noop state DOWN mode DEFAULT group default qlen 1000</span><br><span class="line">    link/ether d2:24:de:9b:b5:d9 brd ff:ff:ff:ff:ff:ff</span><br></pre></td></tr></table></figure><p>可以看到，此时本地多了8和9两块没有被激活的网卡，并且从网卡名上可以看出对应关系</p><h1 id="移动veth网卡到netns"><a href="#移动veth网卡到netns" class="headerlink" title="移动veth网卡到netns"></a>移动veth网卡到netns</h1><p>把其中一个网卡移动到命名空间slions_ns1中</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@slions_pc1 ~]# ip link set veth1 netns slions_ns1</span><br></pre></td></tr></table></figure><p>在slions_ns1中验证网卡信息</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[root@slions_pc1 ~]# ip netns exec slions_ns1 ip a</span><br><span class="line">1: lo: &lt;LOOPBACK&gt; mtu 65536 qdisc noop state DOWN group default qlen 1000</span><br><span class="line">    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00</span><br><span class="line">8: veth1@if9: &lt;BROADCAST,MULTICAST&gt; mtu 1500 qdisc noop state DOWN group default qlen 1000</span><br><span class="line">    link/ether ae:f9:75:fa:93:4d brd ff:ff:ff:ff:ff:ff link-netnsid 0</span><br></pre></td></tr></table></figure><p>我们再来看下宿主机的网卡信息，发现之前的8号没了，仔细观察会发现slions_ns中新的网卡正是之前的8号网卡，从序号也可以看出来，并且网卡名中的@ifx就是veth设备对端的网卡序号</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">[root@slions_pc1 ~]# ip link show</span><br><span class="line">1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN mode DEFAULT group default qlen 1000</span><br><span class="line">    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00</span><br><span class="line">2: ens33: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc pfifo_fast state UP mode DEFAULT group default qlen 1000</span><br><span class="line">    link/ether 00:0c:29:97:8c:d0 brd ff:ff:ff:ff:ff:ff</span><br><span class="line">3: docker0: &lt;NO-CARRIER,BROADCAST,MULTICAST,UP&gt; mtu 1500 qdisc noqueue state DOWN mode DEFAULT group default</span><br><span class="line">    link/ether 02:42:8a:8f:d7:d5 brd ff:ff:ff:ff:ff:ff</span><br><span class="line">9: veth0@if8: &lt;BROADCAST,MULTICAST&gt; mtu 1500 qdisc noop state DOWN mode DEFAULT group default qlen 1000</span><br><span class="line">    link/ether d2:24:de:9b:b5:d9 brd ff:ff:ff:ff:ff:ff link-netnsid 0</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>我们还可以给slions_ns1中的veth1改名</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">[root@slions_pc1 ~]# ip netns exec slions_ns1 ip link set dev veth1 name eth0</span><br><span class="line">[root@slions_pc1 ~]# ip netns exec slions_ns1 ifconfig -a</span><br><span class="line">eth0: flags=4098&lt;BROADCAST,MULTICAST&gt;  mtu 1500</span><br><span class="line">        ether ae:f9:75:fa:93:4d  txqueuelen 1000  (Ethernet)</span><br><span class="line">        RX packets 0  bytes 0 (0.0 B)</span><br><span class="line">        RX errors 0  dropped 0  overruns 0  frame 0</span><br><span class="line">        TX packets 0  bytes 0 (0.0 B)</span><br><span class="line">        TX errors 0  dropped 0 overruns 0  carrier 0  collisions 0</span><br><span class="line"></span><br><span class="line">lo: flags=8&lt;LOOPBACK&gt;  mtu 65536</span><br><span class="line">        loop  txqueuelen 1000  (Local Loopback)</span><br><span class="line">        RX packets 0  bytes 0 (0.0 B)</span><br><span class="line">        RX errors 0  dropped 0  overruns 0  frame 0</span><br><span class="line">        TX packets 0  bytes 0 (0.0 B)</span><br><span class="line">        TX errors 0  dropped 0 overruns 0  carrier 0  collisions 0</span><br></pre></td></tr></table></figure><h1 id="激活veth网卡"><a href="#激活veth网卡" class="headerlink" title="激活veth网卡"></a>激活veth网卡</h1><p>给宿主机veth网卡设置ip地址为10.0.0.1，激活网卡</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">[root@slions_pc1 ~]# ip addr add 10.0.0.1/24 dev veth0</span><br><span class="line">[root@slions_pc1 ~]# ip link set veth0 up</span><br><span class="line">[root@slions_pc1 ~]# ip a</span><br><span class="line">1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000</span><br><span class="line">    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00</span><br><span class="line">    inet 127.0.0.1/8 scope host lo</span><br><span class="line">       valid_lft forever preferred_lft forever</span><br><span class="line">    inet6 ::1/128 scope host</span><br><span class="line">       valid_lft forever preferred_lft forever</span><br><span class="line">2: ens33: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc pfifo_fast state UP group default qlen 1000</span><br><span class="line">    link/ether 00:0c:29:97:8c:d0 brd ff:ff:ff:ff:ff:ff</span><br><span class="line">    inet 192.168.100.10/24 brd 192.168.100.255 scope global noprefixroute ens33</span><br><span class="line">       valid_lft forever preferred_lft forever</span><br><span class="line">    inet6 fe80::8199:4454:db1e:911c/64 scope link tentative noprefixroute dadfailed</span><br><span class="line">       valid_lft forever preferred_lft forever</span><br><span class="line">    inet6 fe80::22c9:e9bb:e5a1:9403/64 scope link noprefixroute</span><br><span class="line">       valid_lft forever preferred_lft forever</span><br><span class="line">3: docker0: &lt;NO-CARRIER,BROADCAST,MULTICAST,UP&gt; mtu 1500 qdisc noqueue state DOWN group default</span><br><span class="line">    link/ether 02:42:8a:8f:d7:d5 brd ff:ff:ff:ff:ff:ff</span><br><span class="line">    inet 172.17.0.1/16 brd 172.17.255.255 scope global docker0</span><br><span class="line">       valid_lft forever preferred_lft forever</span><br><span class="line">    inet6 fe80::42:8aff:fe8f:d7d5/64 scope link</span><br><span class="line">       valid_lft forever preferred_lft forever</span><br><span class="line">9: veth0@if8: &lt;NO-CARRIER,BROADCAST,MULTICAST,UP&gt; mtu 1500 qdisc noqueue state LOWERLAYERDOWN group default qlen 1000</span><br><span class="line">    link/ether d2:24:de:9b:b5:d9 brd ff:ff:ff:ff:ff:ff link-netnsid 0</span><br><span class="line">    inet 10.0.0.1/24 scope global veth0</span><br><span class="line">       valid_lft forever preferred_lft forever</span><br></pre></td></tr></table></figure><p>给slions_ns1命名空间的veth网卡设置ip地址为10.0.0.2，激活网卡</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">[root@slions_pc1 ~]# ip netns exec slions_ns1 ifconfig eth0 10.0.0.2/24 up</span><br><span class="line">[root@slions_pc1 ~]# ip netns exec slions_ns1 ifconfig -a</span><br><span class="line">eth0: flags=4163&lt;UP,BROADCAST,RUNNING,MULTICAST&gt;  mtu 1500</span><br><span class="line">        inet 10.0.0.2  netmask 255.255.255.0  broadcast 10.0.0.255</span><br><span class="line">        inet6 fe80::acf9:75ff:fefa:934d  prefixlen 64  scopeid 0x20&lt;link&gt;</span><br><span class="line">        ether ae:f9:75:fa:93:4d  txqueuelen 1000  (Ethernet)</span><br><span class="line">        RX packets 8  bytes 656 (656.0 B)</span><br><span class="line">        RX errors 0  dropped 0  overruns 0  frame 0</span><br><span class="line">        TX packets 8  bytes 656 (656.0 B)</span><br><span class="line">        TX errors 0  dropped 0 overruns 0  carrier 0  collisions 0</span><br><span class="line"></span><br><span class="line">lo: flags=8&lt;LOOPBACK&gt;  mtu 65536</span><br><span class="line">        loop  txqueuelen 1000  (Local Loopback)</span><br><span class="line">        RX packets 0  bytes 0 (0.0 B)</span><br><span class="line">        RX errors 0  dropped 0  overruns 0  frame 0</span><br><span class="line">        TX packets 0  bytes 0 (0.0 B)</span><br><span class="line">        TX errors 0  dropped 0 overruns 0  carrier 0  collisions 0</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>在宿主机ping slions_ns1的网卡地址</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">[root@slions_pc1 ~]# ping 10.0.0.2 -c 3</span><br><span class="line">PING 10.0.0.2 (10.0.0.2) 56(84) bytes of data.</span><br><span class="line">64 bytes from 10.0.0.2: icmp_seq=1 ttl=64 time=0.013 ms</span><br><span class="line">64 bytes from 10.0.0.2: icmp_seq=2 ttl=64 time=0.023 ms</span><br><span class="line">64 bytes from 10.0.0.2: icmp_seq=3 ttl=64 time=0.024 ms</span><br><span class="line"></span><br><span class="line">--- 10.0.0.2 ping statistics ---</span><br><span class="line">3 packets transmitted, 3 received, 0% packet loss, time 1999ms</span><br><span class="line">rtt min/avg/max/mdev = 0.013/0.020/0.024/0.005 ms</span><br></pre></td></tr></table></figure><p>可以ping通，此时证明宿主机和slions_ns1网络命名空间已经通过veth网卡实现了网络联通</p><h1 id="测试两个netns联通性"><a href="#测试两个netns联通性" class="headerlink" title="测试两个netns联通性"></a>测试两个netns联通性</h1><p>还记不记得最开始我们还创建了一个slions_ns2的网络命名空间，我们把宿主机的那块veth0网卡加到其里面去进行测试</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">[root@slions_pc1 ~]# ip link set dev veth0 netns slions_ns2</span><br><span class="line">[root@slions_pc1 ~]# ip netns exec slions_ns2 ifconfig -a</span><br><span class="line">lo: flags=8&lt;LOOPBACK&gt;  mtu 65536</span><br><span class="line">        loop  txqueuelen 1000  (Local Loopback)</span><br><span class="line">        RX packets 0  bytes 0 (0.0 B)</span><br><span class="line">        RX errors 0  dropped 0  overruns 0  frame 0</span><br><span class="line">        TX packets 0  bytes 0 (0.0 B)</span><br><span class="line">        TX errors 0  dropped 0 overruns 0  carrier 0  collisions 0</span><br><span class="line"></span><br><span class="line">veth0: flags=4098&lt;BROADCAST,MULTICAST&gt;  mtu 1500</span><br><span class="line">        ether d2:24:de:9b:b5:d9  txqueuelen 1000  (Ethernet)</span><br><span class="line">        RX packets 24  bytes 2000 (1.9 KiB)</span><br><span class="line">        RX errors 0  dropped 0  overruns 0  frame 0</span><br><span class="line">        TX packets 24  bytes 2000 (1.9 KiB)</span><br><span class="line">        TX errors 0  dropped 0 overruns 0  carrier 0  collisions 0</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>发现网卡没被激活，手动再设置下</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">[root@slions_pc1 ~]# ip netns exec slions_ns2 ifconfig veth0 10.0.0.3/24 up</span><br><span class="line">[root@slions_pc1 ~]# ip netns exec slions_ns2 ifconfig -a</span><br><span class="line">lo: flags=8&lt;LOOPBACK&gt;  mtu 65536</span><br><span class="line">        loop  txqueuelen 1000  (Local Loopback)</span><br><span class="line">        RX packets 0  bytes 0 (0.0 B)</span><br><span class="line">        RX errors 0  dropped 0  overruns 0  frame 0</span><br><span class="line">        TX packets 0  bytes 0 (0.0 B)</span><br><span class="line">        TX errors 0  dropped 0 overruns 0  carrier 0  collisions 0</span><br><span class="line"></span><br><span class="line">veth0: flags=4163&lt;UP,BROADCAST,RUNNING,MULTICAST&gt;  mtu 1500</span><br><span class="line">        inet 10.0.0.3  netmask 255.255.255.0  broadcast 10.0.0.255</span><br><span class="line">        inet6 fe80::d024:deff:fe9b:b5d9  prefixlen 64  scopeid 0x20&lt;link&gt;</span><br><span class="line">        ether d2:24:de:9b:b5:d9  txqueuelen 1000  (Ethernet)</span><br><span class="line">        RX packets 24  bytes 2000 (1.9 KiB)</span><br><span class="line">        RX errors 0  dropped 0  overruns 0  frame 0</span><br><span class="line">        TX packets 30  bytes 2516 (2.4 KiB)</span><br><span class="line">        TX errors 0  dropped 0 overruns 0  carrier 0  collisions 0</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>这时从slions_ns2中测试能否ping通slions_ns1</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">[root@slions_pc1 ~]# ip netns exec slions_ns2 ping 10.0.0.2 -c 3</span><br><span class="line">PING 10.0.0.2 (10.0.0.2) 56(84) bytes of data.</span><br><span class="line">64 bytes from 10.0.0.2: icmp_seq=1 ttl=64 time=0.014 ms</span><br><span class="line">64 bytes from 10.0.0.2: icmp_seq=2 ttl=64 time=0.023 ms</span><br><span class="line">64 bytes from 10.0.0.2: icmp_seq=3 ttl=64 time=0.023 ms</span><br><span class="line"></span><br><span class="line">--- 10.0.0.2 ping statistics ---</span><br><span class="line">3 packets transmitted, 3 received, 0% packet loss, time 1999ms</span><br><span class="line">rtt min/avg/max/mdev = 0.014/0.020/0.023/0.004 ms</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>和料想的一样可以ping通。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;a href=&quot;/2021/07/09/%E8%81%8A%E8%81%8Anamespace/&quot; title=&quot;之前的文章&quot;&gt;之前的文章&lt;/a&gt;提到了6种Linux的namespace隔离技术，其中network namespace为命名空间内的所有进程提供了全新隔离的网络协</summary>
      
    
    
    
    
    <category term="docker" scheme="https://slions.github.io/tags/docker/"/>
    
  </entry>
  
  <entry>
    <title>cadvisor+node_exporter+prometheus+grafana实现docker与主机监控</title>
    <link href="https://slions.github.io/2021/07/12/cadvisor-node-exporter-prometheus-grafana%E5%AE%9E%E7%8E%B0docker%E4%B8%8E%E4%B8%BB%E6%9C%BA%E7%9B%91%E6%8E%A7/"/>
    <id>https://slions.github.io/2021/07/12/cadvisor-node-exporter-prometheus-grafana%E5%AE%9E%E7%8E%B0docker%E4%B8%8E%E4%B8%BB%E6%9C%BA%E7%9B%91%E6%8E%A7/</id>
    <published>2021-07-12T15:49:40.000Z</published>
    <updated>2021-07-13T03:51:30.328Z</updated>
    
    <content type="html"><![CDATA[<p>网上一大堆关于docker监控的工具和案例，这章主要说下如何通过cadvisor+node_exporter+prometheus完成对docker及主机的监控，并且通过grafana来完成监控数据的展示。</p><p>简述下今天用到的几个组件，其中node_exporter是用来监控主机信息的，cadvisor是用来监控容器信息的，这俩组件对于prometheus而言都是采集器的作用，prometheus会收集这些采集器的监控数据，存储到内置的数据库中，后续可以通过Prometheus表达式来进行过滤查看等操作（这篇不作为重点讲），最后Grafana 会对接Prometheus，将监控数据通过Web UI的方式展示出来。</p><h1 id="环境准备"><a href="#环境准备" class="headerlink" title="环境准备"></a>环境准备</h1><p>先说下我本地的环境：</p><table><thead><tr><th align="left">主机名</th><th align="left">IP地址</th><th align="left">已运行docker容器</th></tr></thead><tbody><tr><td align="left">slions_pc1</td><td align="left">192.168.100.10</td><td align="left">slions_nginx1;slions_busybox1</td></tr><tr><td align="left">slions_pc2</td><td align="left">192.168.100.11</td><td align="left">slions_nginx2;slions_busybox2</td></tr></tbody></table><p>准备了2台机器，并且上面已经启动了一些容器。</p><h1 id="安装node-exporter"><a href="#安装node-exporter" class="headerlink" title="安装node_exporter"></a>安装node_exporter</h1><p>2台机器都执行以下命令（–name处根据自己喜好改）：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">[root@slions_pc1 ~]# docker run -d \</span><br><span class="line"><span class="meta">&gt;</span><span class="bash">   --net=<span class="string">&quot;host&quot;</span> \</span></span><br><span class="line"><span class="bash">&gt;   --pid=<span class="string">&quot;host&quot;</span> \</span></span><br><span class="line"><span class="bash">&gt;   -v <span class="string">&quot;/:/host:ro,rslave&quot;</span> \</span></span><br><span class="line"><span class="bash">&gt;   --name slions_node-exporter1 \</span></span><br><span class="line"><span class="bash">&gt;   quay.io/prometheus/node-exporter:latest \</span></span><br><span class="line"><span class="bash">&gt;   --path.rootfs=/host</span></span><br></pre></td></tr></table></figure><p>安装好后可以检查下node_exporter默认的监听端口是否存在，如下说明正常</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@slions_pc1 ~]# ss -lntp|grep 9100</span><br><span class="line">LISTEN     0      128         :::9100                    :::*                   users:((&quot;node_exporter&quot;,pid=12094,fd=3))</span><br></pre></td></tr></table></figure><p>可以使用chrome访问下，出现如下则说明成功，如果你使用浏览器刷不出来数据请先确保防火墙是否关闭。</p><p><img src="/doc_picture/jk-1.png" alt="image-20210713101912265"></p><h1 id="安装cadvisor"><a href="#安装cadvisor" class="headerlink" title="安装cadvisor"></a>安装cadvisor</h1><p>2台机器都执行以下命令（–name处根据自己喜好改）：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">[root@slions_pc1 ~]# docker run \</span><br><span class="line"><span class="meta">&gt;</span><span class="bash">   --volume=/:/rootfs:ro \</span></span><br><span class="line"><span class="bash">&gt;   --volume=/var/run:/var/run:ro \</span></span><br><span class="line"><span class="bash">&gt;   --volume=/sys:/sys:ro \</span></span><br><span class="line"><span class="bash">&gt;   --volume=/var/lib/docker/:/var/lib/docker:ro \</span></span><br><span class="line"><span class="bash">&gt;   --publish=8080:8080 \</span></span><br><span class="line"><span class="bash">&gt;   --detach=<span class="literal">true</span> \</span></span><br><span class="line"><span class="bash">&gt;   --name=slions_cadvisor1 \</span></span><br><span class="line"><span class="bash">&gt;   --privileged \</span></span><br><span class="line"><span class="bash">&gt;   --network host \</span></span><br><span class="line"><span class="bash">&gt;   google/cadvisor</span></span><br></pre></td></tr></table></figure><p>安装好后可以检查下node_exporter默认的监听端口是否存在，如下说明正常</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@slions_pc1 ~]# ss -lntp|grep 8080</span><br><span class="line">LISTEN     0      128         :::8080                    :::*                   users:((&quot;cadvisor&quot;,pid=12344,fd=9))</span><br></pre></td></tr></table></figure><p>同样我们可以使用chrome访问下主机的8080，如图出现了cadvisor的监控数据。</p><p><img src="/doc_picture/jk-2.png" alt="image-20210713104008822"></p><h1 id="安装prometheus"><a href="#安装prometheus" class="headerlink" title="安装prometheus"></a>安装prometheus</h1><p>选择第一台机器作为prometheus服务端，prometheus服务需要通过配置文件来读取监控目标，我们先在本地编写配置文件，后续通过挂载的方式置入服务中：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line">[<span class="string">root@slions_pc1</span> <span class="string">~</span>]<span class="comment"># cat prometheus/prometheus.yml</span></span><br><span class="line"><span class="attr">global:</span></span><br><span class="line">  <span class="attr">scrape_interval:</span>     <span class="string">15s</span> <span class="comment"># By default, scrape targets every 15 seconds.</span></span><br><span class="line">  <span class="attr">evaluation_interval:</span> <span class="string">15s</span> <span class="comment"># Evaluate rules every 15 seconds.</span></span><br><span class="line"></span><br><span class="line">  <span class="comment"># Attach these extra labels to all timeseries collected by this Prometheus instance.</span></span><br><span class="line">  <span class="attr">external_labels:</span></span><br><span class="line">    <span class="attr">monitor:</span> <span class="string">&#x27;slions-monitor&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="attr">rule_files:</span></span><br><span class="line"><span class="comment">#  - &#x27;prometheus.rules.yml&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="attr">scrape_configs:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">job_name:</span> <span class="string">&#x27;prometheus&#x27;</span></span><br><span class="line">    <span class="attr">scrape_interval:</span> <span class="string">5s</span></span><br><span class="line">    <span class="attr">static_configs:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">targets:</span> [<span class="string">&#x27;localhost:9090&#x27;</span>]</span><br><span class="line"></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">job_name:</span> <span class="string">&#x27;node&#x27;</span></span><br><span class="line">    <span class="attr">scrape_interval:</span> <span class="string">5s</span></span><br><span class="line">    <span class="attr">static_configs:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">targets:</span></span><br><span class="line">        <span class="bullet">-</span> <span class="number">192.168</span><span class="number">.100</span><span class="number">.10</span><span class="string">:9100</span></span><br><span class="line">        <span class="bullet">-</span> <span class="number">192.168</span><span class="number">.100</span><span class="number">.11</span><span class="string">:9100</span></span><br><span class="line"></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">job_name:</span> <span class="string">&#x27;docker&#x27;</span></span><br><span class="line">    <span class="attr">scrape_interval:</span> <span class="string">5s</span></span><br><span class="line">    <span class="attr">static_configs:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">targets:</span></span><br><span class="line">        <span class="bullet">-</span> <span class="number">192.168</span><span class="number">.100</span><span class="number">.10</span><span class="string">:8080</span></span><br><span class="line">        <span class="bullet">-</span> <span class="number">192.168</span><span class="number">.100</span><span class="number">.11</span><span class="string">:8080</span></span><br><span class="line"></span><br></pre></td></tr></table></figure><p>启动Prometheus服务：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@slions_pc1 ~]# docker run -d -p 9090:9090 --volume /root/prometheus/prometheus.yml:/etc/prometheus/prometheus.yml --name slions_prometheus --network host prom/prometheus</span><br></pre></td></tr></table></figure><p>验证服务是否正常启动：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@slions_pc1 ~]# ss -lntp|grep 9090</span><br><span class="line">LISTEN     0      128         :::9090                    :::*                   users:((&quot;prometheus&quot;,pid=13113,fd=7))</span><br></pre></td></tr></table></figure><p>chrome访问<code>http://192.168.100.10:9090</code>可以看到已经出现了prometheus的页面</p><p><img src="/doc_picture/jk-3.png" alt="image-20210713110434391"></p><p>点击<code>Status</code>中的<code>Targets</code>按钮，会发现正是我们配置的那几个监控目标</p><p><img src="/doc_picture/jk-4.png" alt="image-20210713110811438"></p><h1 id="安装grafana"><a href="#安装grafana" class="headerlink" title="安装grafana"></a>安装grafana</h1><p>在第一台机器部署grafana，启动时设置登录密码</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@slions_pc1 ~]# docker run -itd --name slions_grafana -p 3000:3000 -e &quot;GF_SERVER_ROOT_URL=http://grafana.server.name&quot; -e &quot;GF_SECURITY_ADMIN_PASSWORD=slions&quot; --network host grafana/grafana</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>查看服务是否正常：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@slions_pc1 ~]# ss -lntp|grep 3000</span><br><span class="line">LISTEN     0      128         :::3000                    :::*                   users:((&quot;grafana-server&quot;,pid=13581,fd=10))</span><br></pre></td></tr></table></figure><p>使用浏览器查看，用户名为admin,密码是我启动服务时设置的slions</p><p><img src="/doc_picture/jk-5.png" alt="image-20210713111849209"></p><p>选择<code>DATA SOURCES</code>来添加数据源:</p><p><img src="/doc_picture/jk-6.png" alt="image-20210713112133355"></p><p>选择Prometheus后，将URL填写上我们自己的服务地址就可以保存了：</p><p><img src="/doc_picture/jk-7.png" alt="image-20210713112328341"></p><p>我们可以直接找一个合适的模板来导入</p><p><img src="/doc_picture/jk-8.png" alt="image-20210713112714427"></p><p>模板在grafana官网找就好：</p><p><img src="/doc_picture/jk-9.png" alt="image-20210713112947279"></p><p>比如我找到一个看起来不错的模板，点击Download JSON下载到本地</p><p><img src="/doc_picture/jk-10.png" alt="image-20210713113715380"></p><p>然后在import页面导入此JSON文件就好了，选择生效于我们刚创建好的slions_Prometheus:</p><p><img src="/doc_picture/jk-11.png" alt="image-20210713113812164"></p><p>最后让我们看一下监控的效果吧。</p><p><img src="/doc_picture/jk-12.png" alt="image-20210713114101676"></p><h1 id="参考文档"><a href="#参考文档" class="headerlink" title="参考文档"></a>参考文档</h1><p>cadvisor: <a href="https://github.com/google/cadvisor">https://github.com/google/cadvisor</a></p><p>node_exporter:  <a href="https://github.com/prometheus/node_exporter">https://github.com/prometheus/node_exporter</a></p><p>prometheus: <a href="https://github.com/prometheus/prometheus">https://github.com/prometheus/prometheus</a>  、<a href="https://prometheus.io/">https://prometheus.io/</a></p><p>grafana:  <a href="https://grafana.com/docs/grafana/latest/installation/docker/">https://grafana.com/docs/grafana/latest/installation/docker/</a></p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;网上一大堆关于docker监控的工具和案例，这章主要说下如何通过cadvisor+node_exporter+prometheus完成对docker及主机的监控，并且通过grafana来完成监控数据的展示。&lt;/p&gt;
&lt;p&gt;简述下今天用到的几个组件，其中node_export</summary>
      
    
    
    
    
    <category term="docker" scheme="https://slions.github.io/tags/docker/"/>
    
    <category term="prometheus" scheme="https://slions.github.io/tags/prometheus/"/>
    
  </entry>
  
  <entry>
    <title>理解存储驱动overlay2</title>
    <link href="https://slions.github.io/2021/07/12/%E7%90%86%E8%A7%A3%E5%AD%98%E5%82%A8%E9%A9%B1%E5%8A%A8overlay2/"/>
    <id>https://slions.github.io/2021/07/12/%E7%90%86%E8%A7%A3%E5%AD%98%E5%82%A8%E9%A9%B1%E5%8A%A8overlay2/</id>
    <published>2021-07-12T04:00:14.000Z</published>
    <updated>2021-07-12T10:20:42.908Z</updated>
    
    <content type="html"><![CDATA[<p>容器中通过使用不同的rootfs来模拟了不同的操作系统及应用，不过，如果使用每个镜像都需要一个独立的根文件系统的话，那想必磁盘早已拥挤不堪了；且一个镜像可以同时运行多个容器，每个容器对文件的改动该怎么办？</p><p>Linux提供了一种叫做联合文件系统的文件系统，它具备如下特性：</p><ul><li>联合挂载：将多个目录按层次组合，一并挂载到一个联合挂载点。</li><li>写时复制：对联合挂载点的修改不会影响到底层的多个目录，而是使用其他目录记录修改的操作。</li></ul><p>目前有多种文件系统可以被当作联合文件系统，实现如上的功能：overlay2，aufs，devicemapper，btrfs，zfs，vfs等等。而overlay2就是其中的佼佼者，也是docker目前推荐的文件系统：<a href="https://docs.docker.com/storage/storagedriver/select-storage-driver/">https://docs.docker.com/storage/storagedriver/select-storage-driver/</a></p><h1 id="Overlay2"><a href="#Overlay2" class="headerlink" title="Overlay2"></a>Overlay2</h1><p>overlay2是一个类似于aufs的现代的联合文件系统，并且更快。overlay2已被收录进linux内核，它需要内核版本不低于4.0，如果是RHEL或Centos的话则不低于3.10.0-514。</p><p>我们可以查看下自己环境docker配置的存储驱动是什么：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line">[root@slions_pc1 ~]# docker info</span><br><span class="line">Containers: 0</span><br><span class="line"> Running: 0</span><br><span class="line"> Paused: 0</span><br><span class="line"> Stopped: 0</span><br><span class="line">Images: 1</span><br><span class="line">Server Version: 18.09.3</span><br><span class="line">Storage Driver: overlay2</span><br><span class="line"> Backing Filesystem: xfs</span><br><span class="line"> Supports d_type: true</span><br><span class="line"> Native Overlay Diff: true</span><br><span class="line">Logging Driver: json-file</span><br><span class="line">Cgroup Driver: cgroupfs</span><br><span class="line">Plugins:</span><br><span class="line"> Volume: local</span><br><span class="line"> Network: bridge host macvlan null overlay</span><br><span class="line"> Log: awslogs fluentd gcplogs gelf journald json-file local logentries splunk syslog</span><br><span class="line">Swarm: inactive</span><br><span class="line">Runtimes: runc</span><br><span class="line">Default Runtime: runc</span><br><span class="line">Init Binary: docker-init</span><br><span class="line">containerd version: d71fcd7d8303cbf684402823e425e9dd2e99285d</span><br><span class="line">runc version: N/A</span><br><span class="line">init version: fec3683</span><br><span class="line">Security Options:</span><br><span class="line"> seccomp</span><br><span class="line">  Profile: default</span><br><span class="line">Kernel Version: 3.10.0-957.el7.x86_64</span><br><span class="line">Operating System: CentOS Linux 7 (Core)</span><br><span class="line">OSType: linux</span><br><span class="line">Architecture: x86_64</span><br><span class="line">CPUs: 2</span><br><span class="line">Total Memory: 3.683GiB</span><br><span class="line">Name: slions_pc1</span><br><span class="line">ID: 7W4N:NDNK:BDXL:ADXV:ONCZ:PUA7:KSK2:73JQ:Q3HS:AEEK:VKAL:H3RC</span><br><span class="line">Docker Root Dir: /var/lib/docker</span><br><span class="line">Debug Mode (client): false</span><br><span class="line">Debug Mode (server): false</span><br><span class="line">Registry: https://index.docker.io/v1/</span><br><span class="line">Labels:</span><br><span class="line">Experimental: false</span><br><span class="line">Insecure Registries:</span><br><span class="line"> 127.0.0.0/8</span><br><span class="line">Live Restore Enabled: false</span><br><span class="line">Product License: Community Engine</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>默认就是overlay2，如果你本地环境不是overlay2，只需编辑<code>/etc/docker/daemon.json</code>, 添加以下内容并重启docker。</p><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">  <span class="attr">&quot;storage-driver&quot;</span>: <span class="string">&quot;overlay2&quot;</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>docker默认的存储目录是<code>/var/lib/docker</code>，如下：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">[root@slions_pc1 ~]# ls -l /var/lib/docker</span><br><span class="line">总用量 0</span><br><span class="line">drwx------. 2 root root  24 7月  10 11:52 builder</span><br><span class="line">drwx------. 4 root root  92 7月  10 11:52 buildkit</span><br><span class="line">drwx------. 3 root root  78 7月  12 13:08 containers</span><br><span class="line">drwx------. 3 root root  22 7月  10 11:52 image</span><br><span class="line">drwxr-x---. 3 root root  19 7月  10 11:52 network</span><br><span class="line">drwx------. 6 root root 261 7月  12 14:52 overlay2</span><br><span class="line">drwx------. 4 root root  32 7月  10 11:52 plugins</span><br><span class="line">drwx------. 2 root root   6 7月  12 12:07 runtimes</span><br><span class="line">drwx------. 2 root root   6 7月  10 11:52 swarm</span><br><span class="line">drwx------. 2 root root   6 7月  12 12:07 tmp</span><br><span class="line">drwx------. 2 root root   6 7月  10 11:52 trust</span><br><span class="line">drwx------. 2 root root  25 7月  10 11:52 volumes</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>在这里，我们只关心<code>image</code>和<code>overlay2</code>就足够了。</p><p>在我本地起了一个叫slions的容器，容器id为<code>47c26762a49a</code>，镜像id 为<code>4cdc5dd7eaad</code>，这两个id作为了容器和镜像的唯一标识符，我们后面会用到</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">[root@slions_pc1 ~]# docker run -itd --name slions nginx:latest</span><br><span class="line">47c26762a49a3f397e6796cf499f97acbc606a68c04d08b9cd169d6e33469c99</span><br><span class="line">[root@slions_pc1 ~]# docker ps</span><br><span class="line">CONTAINER ID        IMAGE               COMMAND                  CREATED             STATUS              PORTS               NAMES</span><br><span class="line">47c26762a49a        nginx:latest        &quot;/docker-entrypoint.…&quot;   3 seconds ago       Up 2 seconds        80/tcp              slions</span><br><span class="line">[root@slions_pc1 ~]# docker images -a</span><br><span class="line">REPOSITORY          TAG                 IMAGE ID            CREATED             SIZE</span><br><span class="line">nginx               latest              4cdc5dd7eaad        5 days ago          133MB</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>docker会在<code>/var/lib/docker/image</code>目录下按每个存储驱动的名字创建一个目录，如这里的<code>overlay2</code>。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@slions_pc1 ~]# ll /var/lib/docker/image/</span><br><span class="line">总用量 0</span><br><span class="line">drwx------. 5 root root 81 7月  12 14:52 overlay2</span><br></pre></td></tr></table></figure><p>查看overlay2下的目录结构：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">[root@slions_pc1 ~]# tree -L 2 /var/lib/docker/image/overlay2/</span><br><span class="line">/var/lib/docker/image/overlay2/</span><br><span class="line">├── distribution</span><br><span class="line">│   ├── diffid-by-digest</span><br><span class="line">│   └── v2metadata-by-diffid</span><br><span class="line">├── imagedb</span><br><span class="line">│   ├── content</span><br><span class="line">│   └── metadata</span><br><span class="line">├── layerdb</span><br><span class="line">│   ├── mounts</span><br><span class="line">│   ├── sha256</span><br><span class="line">│   └── tmp</span><br><span class="line">└── repositories.json</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>看名字就能知道这儿是用来存放元数据的， 其中还细分为了<code>imagedb</code>和<code>layerdb</code>，因为在docker中，image是由多个layer组合而成的，换句话就是layer是一个共享的层，可能有多个image会指向某个layer。<br> 那如何才能确认image包含了哪些layer呢？答案就在<code>imagedb</code>这个目录中去找。</p><p>比如上面启动的slions容器，我们已知image id为<code>4cdc5dd7eaad</code>，接着打印<code>/var/lib/docker/image/overlay2/imagedb/content/sha256</code>这个目录：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@slions_pc1 ~]# ll /var/lib/docker/image/overlay2/imagedb/content/sha256</span><br><span class="line">总用量 8</span><br><span class="line">-rw-------. 1 root root 7729 7月  12 15:59 4cdc5dd7eaadff5080649e8d0014f2f8d36d4ddf2eff2fdf577dd13da85c5d2f</span><br></pre></td></tr></table></figure><p><code>4cdc5dd7eaadff5080649e8d0014f2f8d36d4ddf2eff2fdf577dd13da85c5d2f</code>正是记录slions镜像元数据的文件，接下来cat一下这个文件，得到一个长长的json：</p><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">[root@slions_pc1 ~]# cat /var/lib/docker/image/overlay2/imagedb/content/sha256/<span class="number">4</span>cdc5dd7eaadff5080649e8d0014f2f8d36d4ddf2eff2fdf577dd13da85c5d2f |jq</span><br><span class="line">&#123;</span><br><span class="line">  .......</span><br><span class="line">  <span class="attr">&quot;os&quot;</span>: <span class="string">&quot;linux&quot;</span>,</span><br><span class="line">  <span class="attr">&quot;rootfs&quot;</span>: &#123;</span><br><span class="line">    <span class="attr">&quot;type&quot;</span>: <span class="string">&quot;layers&quot;</span>,</span><br><span class="line">    <span class="attr">&quot;diff_ids&quot;</span>: [</span><br><span class="line">      <span class="string">&quot;sha256:764055ebc9a7a290b64d17cf9ea550f1099c202d83795aa967428ebdf335c9f7&quot;</span>,</span><br><span class="line">      <span class="string">&quot;sha256:ace9ed9bcfafbc909bc3e9451490652f685959db02a4e01e0528a868ee8eab3e&quot;</span>,</span><br><span class="line">      <span class="string">&quot;sha256:48b4a40de3597ec0a28c2d4508dec64ae685ed0da77b128d0fb5c69cada91882&quot;</span>,</span><br><span class="line">      <span class="string">&quot;sha256:c553c6ba5f1354e1980871b413e057950e0c02d2d7a66b39de2e03836048fda9&quot;</span>,</span><br><span class="line">      <span class="string">&quot;sha256:d97733c0a3b64c08bc0dd286926a8eff1b162b4d9fad229eab807c6dc516c172&quot;</span>,</span><br><span class="line">      <span class="string">&quot;sha256:9d1af766c81806211d5453b711169103e4f5c3c2609e1dfb9ea4dee7e96a7968&quot;</span></span><br><span class="line">    ]</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>查看最后的rootfs部分，diff_ids是一个包含了6个元素的数组，这6个元素正是组成<code>nginx</code>镜像的6个layerID，从上往下看，就是底层到顶层，也就是说<code>764055e...</code>是image的最底层。既然得到了组成这个image的所有layerID，那么我们就可以带着这些layerID去寻找对应的layer了。</p><p>接下来，我们返回到上一层的<code>layerdb</code>中，先打印一下这个目录：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[root@slions_pc1 ~]# ll /var/lib/docker/image/overlay2/layerdb/</span><br><span class="line">总用量 4</span><br><span class="line">drwxr-xr-x. 3 root root   78 7月  12 16:00 mounts</span><br><span class="line">drwxr-xr-x. 8 root root 4096 7月  12 15:59 sha256</span><br><span class="line">drwxr-xr-x. 2 root root    6 7月  12 15:59 tmp</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>在这里我们只管<code>mounts</code>和<code>sha256</code>两个目录，再打印一下<code>sha256</code>目录:</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">[root@slions_pc1 ~]# ll /var/lib/docker/image/overlay2/layerdb/sha256/</span><br><span class="line">总用量 0</span><br><span class="line">drwx------. 2 root root 85 7月  12 15:59 2c78bcd3187437a7a5d9d8dbf555b3574ba7d143c1852860f9df0a46d5df056a</span><br><span class="line">drwx------. 2 root root 85 7月  12 15:59 435c6dad68b58885ad437e5f35f53e071213134eb9e4932b445eac7b39170700</span><br><span class="line">drwx------. 2 root root 85 7月  12 15:59 63d268dd303e176ba45c810247966ff8d1cb9a5bce4a404584087ec01c63de15</span><br><span class="line">drwx------. 2 root root 71 7月  12 15:59 764055ebc9a7a290b64d17cf9ea550f1099c202d83795aa967428ebdf335c9f7</span><br><span class="line">drwx------. 2 root root 85 7月  12 15:59 b27eb5bbca70862681631b492735bac31d3c1c558c774aca9c0e36f1b50ba915</span><br><span class="line">drwx------. 2 root root 85 7月  12 15:59 bdf28aff423adfe7c6cb938eced2f19a32efa9fa3922a3c5ddce584b139dc864</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>在这里，我们仅仅发现<code>764055e..</code>这个最底层的layer，那么剩余的layer为什么会没有呢？那是因为docker1.10版本以后layer层之间的关系通过chainId来进行关联。</p><p>公式是：chainID=sha256sum(H(chainID) diffid)，也就是<code>764055e..</code>的上一层的sha256 id是：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@slions_pc1 ~]# echo -n &quot;sha256:764055ebc9a7a290b64d17cf9ea550f1099c202d83795aa967428ebdf335c9f7 sha256:ace9ed9bcfafbc909bc3e9451490652f685959db02a4e01e0528a868ee8eab3e&quot;  | sha256sum -</span><br><span class="line">2c78bcd3187437a7a5d9d8dbf555b3574ba7d143c1852860f9df0a46d5df056a  -</span><br></pre></td></tr></table></figure><p>依次类推，我们就能找出所有的layerID的组合。<br>但是上面我们提到，<code>/var/lib/docker/image/overlay2/layerdb</code>存的只是元数据，那么真实的rootfs到底存在哪里呢？</p><p>答案就在<code>cache-id</code>中。我们打印一下</p><p><code>/var/lib/docker/image/overlay2/layerdb/sha256/764055ebc9a7a290b64d17cf9ea550f1099c202d83795aa967428ebdf335c9f7/cache-id</code>:</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@slions_pc1 ~]# cat /var/lib/docker/image/overlay2/layerdb/sha256/764055ebc9a7a290b64d17cf9ea550f1099c202d83795aa967428ebdf335c9f7/cache-id</span><br><span class="line">45f0a48f47ef308ead10a1b92856a0d3b8a54294518bd178ca6288c7d54a38be</span><br></pre></td></tr></table></figure><p>这个id就是对应<code>/var/lib/docker/overlay2/45f0a48f47ef308ead10a1b92856a0d3b8a54294518bd178ca6288c7d54a38be</code>。因此，以此类推，更高一层的layer对应的cache-id也能找到对应的rootfs，当这些rootfs的diff目录通过联合挂载的方式挂载到某个目录，就能提供整个容器需要的rootfs了。</p><h2 id="overlay存储流程"><a href="#overlay存储流程" class="headerlink" title="overlay存储流程"></a>overlay存储流程</h2><p><img src="/doc_picture/overlay2.png" alt="overlay2"></p><p>最下层是一个 lower 层，也就是镜像层，它是一个只读层。右上层是一个 upper 层，upper 是容器的读写层，upper 层采用了写时复制的机制，也就是说只有对某些文件需要进行修改的时候才会从 lower 层把这个文件拷贝上来，之后所有的修改操作都会对 upper 层的副本进行修改。</p><p>upper 并列的有一个 workdir，它的作用是充当一个中间层的作用。也就是说，当对 upper 层里面的副本进行修改时，会先放到 workdir，然后再从 workdir 移到 upper 里面去，这个是 overlay 的工作机制。</p><p>最上面的是 mergedir，是一个统一视图层。从 mergedir 里面可以看到 upper 和 lower 中所有数据的整合，然后我们 docker exec 到容器里面，看到一个文件系统其实就是 mergedir 统一视图层。</p><p>在运行容器后，可以通过mount命令查看其具体挂载信息：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@slions_pc1 ~]# mount -t overlay</span><br><span class="line">overlay on /var/lib/docker/overlay2/0b319b849790fe43488a0f5aa76ec318640b05d7d1ec4e162ee80ebc16c43232/merged type overlay (rw,relatime,seclabel,lowerdir=/var/lib/docker/overlay2/l/XZ6TYQO6XXCBXKH6E4N3N7QTWU:/var/lib/docker/overlay2/l/U6O63XRKJYL3MKFKZCZ33EEDE6:/var/lib/docker/overlay2/l/Y2XZUZZM42C6HPF6BHHLAMGY2G:/var/lib/docker/overlay2/l/P35XQEQ2NQELR5QWTI3POCZPJG:/var/lib/docker/overlay2/l/KRCQJWNO5RDBHNQOQXFCLT2JLX:/var/lib/docker/overlay2/l/5KKQYPGMGGXBSLAIEHV72IJ2QH:/var/lib/docker/overlay2/l/4PZ2UIG6UG3GF2PLSFP6C6X7AO,upperdir=/var/lib/docker/overlay2/0b319b849790fe43488a0f5aa76ec318640b05d7d1ec4e162ee80ebc16c43232/diff,workdir=/var/lib/docker/overlay2/0b319b849790fe43488a0f5aa76ec318640b05d7d1ec4e162ee80ebc16c43232/work)</span><br></pre></td></tr></table></figure><p>可以看到：</p><p>merged:<br><code>/var/lib/docker/overlay2/0b319b849790fe43488a0f5aa76ec318640b05d7d1ec4e162ee80ebc16c43232/merged</code></p><p>upperdir:<br><code>/var/lib/docker/overlay2/0b319b849790fe43488a0f5aa76ec318640b05d7d1ec4e162ee80ebc16c43232/diff</code></p><p>workdir:<br><code>/var/lib/docker/overlay2/0b319b849790fe43488a0f5aa76ec318640b05d7d1ec4e162ee80ebc16c43232/work</code></p><p>lowerdir:<br><code>/var/lib/docker/overlay2/l/XZ6TYQO6XXCBXKH6E4N3N7QTWU:...:.../overlay2/l/IG6UG3GF2PLSFP6C6X7AO</code><br>冒号分隔多个lowerdir，从左到右层次越低。</p><p>细心的你肯定发现了，lowerdir有7个，但是怎么看起来这么奇怪呢，其实观察下<code>/var/lib/docker/overlay2/l/</code>就会发现，正是之前我们得到的那6个rootfs目录的链接文件外加一个新的容器目录文件。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">[root@slions_pc1 overlay2]# ll /var/lib/docker/overlay2/l/</span><br><span class="line">总用量 0</span><br><span class="line">lrwxrwxrwx. 1 root root 72 7月  12 15:59 4PZ2UIG6UG3GF2PLSFP6C6X7AO -&gt; ../45f0a48f47ef308ead10a1b92856a0d3b8a54294518bd178ca6288c7d54a38be/diff</span><br><span class="line">lrwxrwxrwx. 1 root root 72 7月  12 15:59 5KKQYPGMGGXBSLAIEHV72IJ2QH -&gt; ../de169f3af7d5f47b3d83eeb21408ecb637dd5fd4847d31f7644a72830c16db65/diff</span><br><span class="line">lrwxrwxrwx. 1 root root 72 7月  12 16:00 AD3GPWKO46I5LBSKACROCGTCFN -&gt; ../0b319b849790fe43488a0f5aa76ec318640b05d7d1ec4e162ee80ebc16c43232/diff</span><br><span class="line">lrwxrwxrwx. 1 root root 72 7月  12 15:59 KRCQJWNO5RDBHNQOQXFCLT2JLX -&gt; ../e78f609099df22f4ab410322759923ebab67112a3a88fc323b4bdeb98b00d8d2/diff</span><br><span class="line">lrwxrwxrwx. 1 root root 72 7月  12 15:59 P35XQEQ2NQELR5QWTI3POCZPJG -&gt; ../18998b6cffbcc8eb5984c1077e00d02d6d2f150a889c8c856055364bfa16b0a2/diff</span><br><span class="line">lrwxrwxrwx. 1 root root 72 7月  12 15:59 U6O63XRKJYL3MKFKZCZ33EEDE6 -&gt; ../192d460630fe2fefa59151b3a7b37032346487ac6af719d3be3f2e1dfd373494/diff</span><br><span class="line">lrwxrwxrwx. 1 root root 77 7月  12 16:00 XZ6TYQO6XXCBXKH6E4N3N7QTWU -&gt; ../0b319b849790fe43488a0f5aa76ec318640b05d7d1ec4e162ee80ebc16c43232-init/diff</span><br><span class="line">lrwxrwxrwx. 1 root root 72 7月  12 15:59 Y2XZUZZM42C6HPF6BHHLAMGY2G -&gt; ../aca1597583b8862fae70d6bfc7487a041092a7b9d1cf400802d11225b2fb3f5b/diff</span><br><span class="line"></span><br></pre></td></tr></table></figure><h2 id="overlay实验"><a href="#overlay实验" class="headerlink" title="overlay实验"></a>overlay实验</h2><p><strong>创建并挂载测试</strong></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">[root@slions_pc1 ~]# mkdir tmp</span><br><span class="line">[root@slions_pc1 ~]# cd tmp/</span><br><span class="line">[root@slions_pc1 tmp]# mkdir -p lower&#123;1..2&#125;/dir upper/dir worker merge</span><br><span class="line">[root@slions_pc1 tmp]# ll</span><br><span class="line">总用量 0</span><br><span class="line">drwxr-xr-x. 3 root root 17 7月  12 16:42 lower1</span><br><span class="line">drwxr-xr-x. 3 root root 17 7月  12 16:42 lower2</span><br><span class="line">drwxr-xr-x. 2 root root  6 7月  12 16:42 merge</span><br><span class="line">drwxr-xr-x. 3 root root 17 7月  12 16:42 upper</span><br><span class="line">drwxr-xr-x. 2 root root  6 7月  12 16:42 worker</span><br><span class="line">[root@slions_pc1 tmp]# for i in 1 2 ;do touch lower$i/foo$i;done</span><br><span class="line">[root@slions_pc1 tmp]# tree</span><br><span class="line">.</span><br><span class="line">├── lower1</span><br><span class="line">│   ├── dir</span><br><span class="line">│   └── foo1</span><br><span class="line">├── lower2</span><br><span class="line">│   ├── dir</span><br><span class="line">│   └── foo2</span><br><span class="line">├── merge</span><br><span class="line">├── upper</span><br><span class="line">│   └── dir</span><br><span class="line">└── worker</span><br><span class="line">[root@slions_pc1 tmp]# mount -t overlay overlay -o lowerdir=lower1:lower2,upperdir=upper,workdir=worker merge/</span><br><span class="line">[root@slions_pc1 tmp]# mount |grep overlay</span><br><span class="line">...</span><br><span class="line">overlay on /root/tmp/merge type overlay (rw,relatime,seclabel,lowerdir=lower1:lower2,upperdir=upper,workdir=worker)</span><br></pre></td></tr></table></figure><p>挂载了一个名为overlay的overlay类型的文件系统，挂载点为merged目录。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line">[root@slions_pc1 tmp]# touch upper/foo3</span><br><span class="line">[root@slions_pc1 tmp]# echo &quot;from lower1&quot; &gt;lower1/dir/aa</span><br><span class="line">[root@slions_pc1 tmp]# echo &quot;from lower2&quot; &gt;lower2/dir/aa</span><br><span class="line">[root@slions_pc1 tmp]# echo &quot;from lower1 bb&quot; &gt;lower2/dir/bb</span><br><span class="line">[root@slions_pc1 tmp]# echo &quot;from upper&quot; &gt;upper/dir/bb</span><br><span class="line">[root@slions_pc1 tmp]# tree</span><br><span class="line">.</span><br><span class="line">├── lower1</span><br><span class="line">│   ├── dir</span><br><span class="line">│   │   └── aa</span><br><span class="line">│   └── foo1</span><br><span class="line">├── lower2</span><br><span class="line">│   ├── dir</span><br><span class="line">│   │   ├── aa</span><br><span class="line">│   │   └── bb</span><br><span class="line">│   └── foo2</span><br><span class="line">├── merge</span><br><span class="line">│   ├── dir</span><br><span class="line">│   │   ├── aa</span><br><span class="line">│   │   └── bb</span><br><span class="line">│   ├── foo1</span><br><span class="line">│   ├── foo2</span><br><span class="line">│   └── foo3</span><br><span class="line">├── upper</span><br><span class="line">│   ├── dir</span><br><span class="line">│   │   └── bb</span><br><span class="line">│   └── foo3</span><br><span class="line">└── worker</span><br><span class="line">    └── work</span><br><span class="line">[root@slions_pc1 tmp]# cat merge/dir/aa</span><br><span class="line">from lower1</span><br><span class="line">[root@slions_pc1 tmp]# cat merge/dir/bb</span><br><span class="line">from upper</span><br><span class="line"></span><br></pre></td></tr></table></figure><p><strong>删除文件测试</strong></p><p>删除文件时，会在upper层创建whiteout的特殊文件，overlayfs会自动过过滤掉和whiteout文件自身以及和它同名的lower层文件和目录，达到了隐藏文件的目的，让用户以为文件已经被删除了。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br></pre></td><td class="code"><pre><span class="line">[root@slions_pc1 tmp]# tree</span><br><span class="line">.</span><br><span class="line">├── lower1</span><br><span class="line">│   ├── dir</span><br><span class="line">│   │   └── aa</span><br><span class="line">│   └── foo1</span><br><span class="line">├── lower2</span><br><span class="line">│   ├── dir</span><br><span class="line">│   │   ├── aa</span><br><span class="line">│   │   └── bb</span><br><span class="line">│   └── foo2</span><br><span class="line">├── merge</span><br><span class="line">│   ├── dir</span><br><span class="line">│   │   ├── aa</span><br><span class="line">│   │   └── bb</span><br><span class="line">│   ├── foo1</span><br><span class="line">│   ├── foo2</span><br><span class="line">│   └── foo3</span><br><span class="line">├── upper</span><br><span class="line">│   ├── dir</span><br><span class="line">│   │   └── bb</span><br><span class="line">│   └── foo3</span><br><span class="line">└── worker</span><br><span class="line">    └── work</span><br><span class="line"></span><br><span class="line">10 directories, 12 files</span><br><span class="line">[root@slions_pc1 tmp]# rm merge/foo1</span><br><span class="line">rm：是否删除普通空文件 &quot;merge/foo1&quot;？y</span><br><span class="line">[root@slions_pc1 tmp]# tree</span><br><span class="line">.</span><br><span class="line">├── lower1</span><br><span class="line">│   ├── dir</span><br><span class="line">│   │   └── aa</span><br><span class="line">│   └── foo1</span><br><span class="line">├── lower2</span><br><span class="line">│   ├── dir</span><br><span class="line">│   │   ├── aa</span><br><span class="line">│   │   └── bb</span><br><span class="line">│   └── foo2</span><br><span class="line">├── merge</span><br><span class="line">│   ├── dir</span><br><span class="line">│   │   ├── aa</span><br><span class="line">│   │   └── bb</span><br><span class="line">│   ├── foo2</span><br><span class="line">│   └── foo3</span><br><span class="line">├── upper</span><br><span class="line">│   ├── dir</span><br><span class="line">│   │   └── bb</span><br><span class="line">│   ├── foo1</span><br><span class="line">│   └── foo3</span><br><span class="line">└── worker</span><br><span class="line">    └── work</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure><h1 id="参考文档"><a href="#参考文档" class="headerlink" title="参考文档"></a>参考文档</h1><p>Use the OverlayFS storage driver：<a href="https://docs.docker.com/storage/storagedriver/overlayfs-driver/#how-the-overlay2-driver-works">https://docs.docker.com/storage/storagedriver/overlayfs-driver/#how-the-overlay2-driver-works</a></p><p>Docker storage drivers：<a href="https://docs.docker.com/storage/storagedriver/select-storage-driver/">https://docs.docker.com/storage/storagedriver/select-storage-driver/</a></p><p>Docker存储驱动之–overlay2：<a href="https://www.jianshu.com/p/3826859a6d6e">https://www.jianshu.com/p/3826859a6d6e</a></p><p>深入理解overlayfs（二）：使用与原理分析：<a href="https://blog.csdn.net/linshenyuan1213/article/details/82527712">https://blog.csdn.net/linshenyuan1213/article/details/82527712</a></p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;容器中通过使用不同的rootfs来模拟了不同的操作系统及应用，不过，如果使用每个镜像都需要一个独立的根文件系统的话，那想必磁盘早已拥挤不堪了；且一个镜像可以同时运行多个容器，每个容器对文件的改动该怎么办？&lt;/p&gt;
&lt;p&gt;Linux提供了一种叫做联合文件系统的文件系统，它具备</summary>
      
    
    
    
    
    <category term="docker" scheme="https://slions.github.io/tags/docker/"/>
    
  </entry>
  
</feed>
